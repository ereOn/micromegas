{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Micromegas Documentation","text":"<p>Welcome to Micromegas, a unified observability platform designed for high-performance telemetry collection and analytics.</p> <p>\u2192 Source Code on GitHub \u2192 Get Started</p>"},{"location":"#what-is-micromegas","title":"What is Micromegas?","text":"<p>Micromegas is a comprehensive observability solution that provides:</p> <ul> <li>High-performance instrumentation with 20ns overhead per event</li> <li>Unified data collection for logs, metrics, spans, and traces</li> <li>Cost-efficient storage using object storage for raw data</li> <li>Powerful SQL analytics built on Apache DataFusion</li> <li>Real-time and historical analysis capabilities</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#high-performance","title":"\ud83d\ude80 High Performance","text":"<ul> <li>Ultra-low overhead telemetry collection (20ns per event)</li> <li>Supports up to 100k events/second per process</li> <li>Thread-local storage for minimal performance impact</li> </ul>"},{"location":"#cost-effective","title":"\ud83d\udcb0 Cost Effective","text":"<ul> <li>Raw data stored in cheap object storage (S3/GCS)</li> <li>Metadata in PostgreSQL for fast queries</li> <li>Pay only for what you query with on-demand ETL</li> </ul>"},{"location":"#unified-observability","title":"\ud83d\udd0d Unified Observability","text":"<ul> <li>Logs, metrics, traces, and spans in a single queryable format</li> <li>SQL interface compatible with existing analytics tools</li> <li>Grafana plugin for visualization and dashboards</li> </ul>"},{"location":"#modern-architecture","title":"\ud83c\udfd7\ufe0f Modern Architecture","text":"<ul> <li>Apache Arrow FlightSQL for efficient data transfer</li> <li>DataFusion-powered analytics engine</li> <li>Lakehouse architecture based on Parquet (columnar format optimized for analytics workloads)</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Get started with Micromegas in just a few steps:</p> <ol> <li>Getting Started Guide - Set up your first Micromegas installation</li> <li>Unreal Engine Integration - Add observability to your Unreal Engine games</li> <li>Query Guide - Learn how to query your observability data</li> <li>Architecture Overview - Understand the system design</li> </ol>"},{"location":"#use-cases","title":"Use Cases","text":""},{"location":"#game-development","title":"Game Development","text":"<p>Monitor Unreal Engine games with integrated telemetry for performance, player behavior, and system health.</p>"},{"location":"#application-performance-monitoring","title":"Application Performance Monitoring","text":"<p>Monitor your applications with detailed performance metrics, error tracking, and distributed tracing.</p>"},{"location":"#infrastructure-observability","title":"Infrastructure Observability","text":"<p>Collect and analyze system metrics, logs, and performance data across your entire infrastructure.</p>"},{"location":"#cost-effective-analytics","title":"Cost-Effective Analytics","text":"<p>Store massive amounts of telemetry data cost-effectively while maintaining fast query performance.</p>"},{"location":"#development-debugging","title":"Development &amp; Debugging","text":"<p>Use high-frequency instrumentation to debug performance issues and understand application behavior.</p>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Browse the guides in this documentation</li> <li>GitHub Issues: Report bugs or request features</li> <li>Community: Join discussions and get support</li> </ul>"},{"location":"#license","title":"License","text":"<p>Micromegas is open source software. See the LICENSE file for details.</p>"},{"location":"contributing/","title":"Contributing to Micromegas","text":"<p>We welcome contributions to the Micromegas project! This guide will help you get started with contributing code, documentation, or reporting issues.</p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>By participating in this project, you agree to maintain a respectful and inclusive environment for all contributors.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/madesroches/micromegas.git\ncd micromegas\n</code></pre></p> </li> <li> <p>Set up development environment:    Follow the Getting Started Guide to set up your local environment.</p> </li> <li> <p>Install development dependencies:    <pre><code># Rust development\ncd rust\ncargo build\n\n# Python development\ncd python/micromegas\npoetry install\n</code></pre></p> </li> </ol>"},{"location":"contributing/#contributing-code","title":"Contributing Code","text":""},{"location":"contributing/#before-you-start","title":"Before You Start","text":"<ol> <li>Check existing issues on GitHub Issues</li> <li>Open an issue to discuss your proposed changes if it's a significant feature</li> <li>Fork the repository and create a feature branch</li> </ol>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Create a feature branch:    <pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b bugfix/issue-description\n</code></pre></p> </li> <li> <p>Follow coding standards:</p> </li> <li>Rust: Run <code>cargo fmt</code> before any commit</li> <li>Python: Use <code>black</code> for formatting</li> <li>Dependencies: Keep alphabetical order in Cargo.toml</li> <li> <p>Error handling: Use <code>expect()</code> with descriptive messages instead of <code>unwrap()</code></p> </li> <li> <p>Start development services:</p> </li> </ol> <p>For testing and development, you can start all required services (PostgreSQL, telemetry-ingestion-srv, flight-sql-srv, and telemetry-admin) using the dev.py script:</p> <pre><code># Start all services in a tmux session (debug mode)\npython3 local_test_env/dev.py\n\n# Or in release mode for better performance\npython3 local_test_env/dev.py release\n</code></pre> <p>This will:    - Build the Rust services    - Start PostgreSQL database    - Start telemetry-ingestion-srv on port 9000    - Start flight-sql-srv on port 32010    - Start telemetry-admin service    - Open a tmux session with all services running in separate panes</p> <p>To stop all services:    <pre><code># Use the stop script\npython3 local_test_env/stop-dev.py\n\n# Or manually kill the tmux session\ntmux kill-session -t micromegas\n</code></pre></p> <ol> <li> <p>Write tests:    <pre><code># Rust tests\ncd rust\ncargo test\n\n# Python tests\ncd python/micromegas\npytest\n</code></pre></p> </li> <li> <p>Run CI pipeline locally:    <pre><code>cd rust\npython3 ../build/rust_ci.py\n</code></pre></p> </li> <li> <p>Commit with clear messages:    <pre><code>git commit -m \"Add histogram generation for span duration analysis\"\n</code></pre></p> </li> <li> <p>Create Pull Request:    Once your changes are ready, create a pull request on GitHub.</p> </li> </ol>"},{"location":"contributing/#code-review-process","title":"Code Review Process","text":"<ol> <li>Automated checks: Ensure all CI checks pass</li> <li>Code review: Maintainers will review your changes</li> <li>Address feedback: Make requested changes if needed</li> <li>Merge: Once approved, your PR will be merged</li> </ol>"},{"location":"contributing/#contributing-documentation","title":"Contributing Documentation","text":""},{"location":"contributing/#setup","title":"Setup","text":"<ol> <li>Install documentation dependencies:    <pre><code># Create and activate virtual environment (recommended)\npython3 -m venv docs-venv\nsource docs-venv/bin/activate  # On Windows: docs-venv\\Scripts\\activate\n\n# Install dependencies\npip install -r docs/docs-requirements.txt\n</code></pre></li> </ol>"},{"location":"contributing/#mkdocs-documentation","title":"MkDocs Documentation","text":"<p>The main documentation uses MkDocs with Material theme:</p> <ol> <li>Edit documentation:</li> <li>Files are in <code>mkdocs/docs/</code> directory</li> <li>Configuration in <code>mkdocs/mkdocs.yml</code></li> <li>Use Markdown format</li> <li> <p>Follow existing structure and style</p> </li> <li> <p>Preview changes:    <pre><code>cd mkdocs\n\n# Using the virtual environment\n/home/mad/micromegas/docs-venv/bin/mkdocs serve --dev-addr=0.0.0.0:8000\n\n# Or if mkdocs is in your PATH\nmkdocs serve\n\n# Visit http://localhost:8000\n</code></pre></p> </li> <li> <p>Build documentation:    <pre><code>cd mkdocs\n/home/mad/micromegas/docs-venv/bin/mkdocs build\n\n# Output will be in mkdocs/site/\n</code></pre></p> </li> <li> <p>Deploy documentation:    <pre><code># Documentation is automatically deployed via GitHub Actions\n# when changes are pushed to the main branch\n</code></pre></p> </li> </ol>"},{"location":"contributing/#documentation-guidelines","title":"Documentation Guidelines","text":"<ul> <li>Clear and concise: Write for your audience</li> <li>Code examples: Include working examples with expected output</li> <li>Cross-references: Link to related sections</li> <li>Consistent formatting: Follow existing patterns</li> </ul>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":""},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>Include the following information:</p> <ul> <li>Environment: OS, Rust version, Python version</li> <li>Micromegas version: Git commit or release version</li> <li>Steps to reproduce: Clear, minimal reproduction steps</li> <li>Expected vs actual behavior: What should happen vs what happens</li> <li>Logs/errors: Include relevant error messages or logs</li> <li>Configuration: Relevant environment variables or config</li> </ul>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<ul> <li>Use case: Describe the problem you're trying to solve</li> <li>Proposed solution: Your suggested approach</li> <li>Alternatives: Other approaches you've considered</li> <li>Impact: Who would benefit from this feature</li> </ul>"},{"location":"contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"contributing/#architecture-understanding","title":"Architecture Understanding","text":"<p>Familiarize yourself with the architecture overview:</p> <ul> <li>High-performance instrumentation (20ns overhead)</li> <li>Lakehouse architecture with object storage</li> <li>DataFusion-powered analytics</li> <li>FlightSQL protocol for efficient data transfer</li> </ul>"},{"location":"contributing/#key-areas-for-contribution","title":"Key Areas for Contribution","text":"<ol> <li>Core Rust Libraries:</li> <li>Tracing instrumentation improvements</li> <li>Analytics engine enhancements</li> <li> <p>Performance optimizations</p> </li> <li> <p>Services:</p> </li> <li>Ingestion service features</li> <li>FlightSQL server improvements</li> <li> <p>Admin CLI enhancements</p> </li> <li> <p>Client Libraries:</p> </li> <li>Python API improvements</li> <li> <p>New language bindings</p> </li> <li> <p>Documentation:</p> </li> <li>Query examples and patterns</li> <li>Performance guidance</li> <li> <p>Integration guides</p> </li> <li> <p>Testing:</p> </li> <li>Unit test coverage</li> <li>Integration tests</li> <li>Performance benchmarks</li> </ol>"},{"location":"contributing/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Benchmarking: Include benchmarks for performance-critical changes</li> <li>Memory usage: Consider memory implications of new features</li> <li>Backwards compatibility: Maintain API compatibility when possible</li> </ul>"},{"location":"contributing/#security","title":"Security","text":"<ul> <li>No secrets in code: Never commit API keys, passwords, or tokens</li> <li>Input validation: Validate all external inputs</li> <li>Dependencies: Keep dependencies updated and minimal</li> </ul>"},{"location":"contributing/#community","title":"Community","text":""},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Check the documentation first</li> <li>GitHub Issues: Search existing issues before creating new ones</li> <li>Discussions: Use GitHub Discussions for questions and ideas</li> </ul>"},{"location":"contributing/#stay-updated","title":"Stay Updated","text":"<ul> <li>Watch the repository for updates</li> <li>Follow releases for new features and bug fixes</li> <li>Join discussions about future directions</li> </ul>"},{"location":"contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in: - Git commit history - Release notes for significant contributions - Special thanks in documentation</p> <p>Thank you for contributing to Micromegas! Your contributions help make observability more accessible and cost-effective for everyone.</p>"},{"location":"cost-effectiveness/","title":"Cost Effectiveness","text":"<p>Micromegas is designed to provide enterprise-grade observability at a fraction of the cost of commercial SaaS platforms by leveraging direct infrastructure costs rather than abstracted pricing models.</p>"},{"location":"cost-effectiveness/#cost-philosophy","title":"Cost Philosophy","text":"<p>Unlike traditional observability platforms that charge per GB ingested, per host, or per user, Micromegas runs on your own infrastructure. Your cost is simply the direct cost of the cloud services you consume.</p>"},{"location":"cost-effectiveness/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Full transparency - See every dollar spent on your cloud bill</li> <li>No vendor margins - Pay only for actual infrastructure usage</li> <li>Predictable scaling - Costs scale linearly with resource consumption</li> <li>Data ownership - Your telemetry data never leaves your cloud account</li> </ul>"},{"location":"cost-effectiveness/#primary-cost-drivers","title":"Primary Cost Drivers","text":"<p>The infrastructure cost for Micromegas comes from standard cloud services:</p>"},{"location":"cost-effectiveness/#compute-services","title":"Compute Services","text":"<ul> <li>Ingestion Service (<code>telemetry-ingestion-srv</code>) - Handles incoming telemetry data</li> <li>Analytics Service (<code>flight-sql-srv</code>) - Serves SQL queries and dashboards</li> <li>Maintenance Daemon (<code>telemetry-admin</code>) - Background data processing and rollups</li> </ul>"},{"location":"cost-effectiveness/#storage-services","title":"Storage Services","text":"<ul> <li>Database (PostgreSQL) - Stores metadata about processes, streams, and data blocks</li> <li>Object Storage (S3/GCS) - Stores raw telemetry payloads and materialized Parquet files</li> </ul>"},{"location":"cost-effectiveness/#supporting-infrastructure","title":"Supporting Infrastructure","text":"<ul> <li>Load Balancers - Route traffic to services</li> <li>Networking - Data transfer and connectivity</li> </ul>"},{"location":"cost-effectiveness/#example-deployment-cost","title":"Example Deployment Cost","text":"<p>Here's a real-world cost breakdown for a production Micromegas deployment:</p>"},{"location":"cost-effectiveness/#data-scale","title":"Data Scale","text":"<ul> <li>Retention Period: 90 days</li> <li>Total Storage: 8.5 TB in 118 million objects</li> <li>Log Entries: 9 billion</li> <li>Metric Events: 275 billion  </li> <li>Trace Events: 165 billion</li> </ul>"},{"location":"cost-effectiveness/#monthly-infrastructure-costs","title":"Monthly Infrastructure Costs","text":"Component Specification Monthly Cost Ingestion Services 2 instances \u00d7 (1 vCPU, 2GB RAM) ~$30 Analytics Service 1 instance \u00d7 (4 vCPU, 8GB RAM) ~$120 Maintenance Daemon 1 instance \u00d7 (4 vCPU, 8GB RAM) ~$120 PostgreSQL Database Aurora Serverless (44GB storage) ~$200 Object Storage 8.5TB S3 Standard + requests ~$500 Load Balancer Application Load Balancer ~$30 Total ~$1,000/month"},{"location":"cost-effectiveness/#scale-perspective","title":"Scale Perspective","text":"<p>This deployment handles:</p> <ul> <li>449 billion total events over 90 days</li> <li>~165 million events per day</li> <li>~1,900 events per second average throughput</li> </ul>"},{"location":"cost-effectiveness/#cost-management-features","title":"Cost Management Features","text":""},{"location":"cost-effectiveness/#on-demand-processing-tail-sampling","title":"On-Demand Processing (Tail Sampling)","text":"<p>Micromegas supports storing all raw telemetry data in low-cost object storage and materializing it for analysis only when needed:</p> <ul> <li>Raw data stored cheaply in S3/GCS</li> <li>Processing costs only when querying specific data</li> <li>Selective materialization based on actual analysis needs</li> </ul>"},{"location":"cost-effectiveness/#flexible-retention-policies","title":"Flexible Retention Policies","text":"<p>Configure retention periods independently for:</p> <ul> <li>Raw telemetry data - Keep longer in cheap storage</li> <li>Materialized views - Shorter retention for frequently accessed data</li> <li>Metadata - Configure based on compliance requirements</li> </ul>"},{"location":"cost-effectiveness/#commercial-platform-comparison","title":"Commercial Platform Comparison","text":""},{"location":"cost-effectiveness/#pricing-model-differences","title":"Pricing Model Differences","text":"Aspect Commercial SaaS Micromegas Cost Basis Per-GB, per-host, per-user Direct infrastructure costs Transparency Opaque vendor margins Full cost visibility Control Limited infrastructure control Complete infrastructure control Scalability Vendor-managed, unpredictable costs Self-managed, predictable scaling Data Ownership Third-party hosted Your cloud account only"},{"location":"cost-effectiveness/#when-micromegas-is-cost-effective","title":"When Micromegas is Cost Effective","text":"<p>The Micromegas model is particularly advantageous when:</p> <ul> <li>High data volumes - Direct infrastructure costs scale better than per-GB pricing</li> <li>Cost predictability is critical for budgeting</li> <li>Data governance requirements favor keeping data in your environment</li> <li>Operational maturity exists to manage distributed systems</li> <li>Long-term retention is needed (cheap object storage vs. expensive SaaS retention)</li> </ul>"},{"location":"cost-effectiveness/#detailed-cost-comparisons","title":"Detailed Cost Comparisons","text":"<p>For in-depth, dollar-for-dollar comparisons with specific platforms:</p> <ul> <li>Micromegas vs. Datadog</li> <li>Micromegas vs. Dynatrace</li> <li>Micromegas vs. Elastic Observability</li> <li>Micromegas vs. Grafana Cloud</li> <li>Micromegas vs. New Relic</li> <li>Micromegas vs. Splunk</li> </ul>"},{"location":"cost-effectiveness/#getting-started-with-cost-optimization","title":"Getting Started with Cost Optimization","text":"<ol> <li>Start small - Deploy minimal infrastructure and scale as needed</li> <li>Monitor usage - Use cloud billing dashboards to track costs</li> <li>Optimize retention - Balance storage costs with analysis needs  </li> <li>Leverage tail sampling - Store everything, process selectively</li> <li>Right-size compute - Match instance types to actual workload demands</li> </ol> <p>The goal is predictable, transparent costs that scale efficiently with your observability needs.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will walk you through setting up Micromegas on your local workstation for testing and development purposes.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Docker - For running PostgreSQL</li> <li>Python 3.8+ - For the client API and setup scripts</li> <li>Rust - For building Micromegas services</li> </ul> <p>Optional: - tmux - For managing multiple services in a single terminal (Linux/macOS)</p>"},{"location":"getting-started/#environment-setup","title":"Environment Setup","text":"<p>Set the following environment variables for local development:</p> <pre><code># Database credentials (used by setup scripts)\nexport MICROMEGAS_DB_USERNAME=your_username\nexport MICROMEGAS_DB_PASSWD=your_password\n\n# Service endpoints\nexport MICROMEGAS_TELEMETRY_URL=http://localhost:9000\nexport MICROMEGAS_SQL_CONNECTION_STRING=postgres://your_username:your_password@localhost:5432\n\n# Object storage (replace with your local path)\nexport MICROMEGAS_OBJECT_STORE_URI=file:///path/to/local/storage\n</code></pre> <p>Object Storage Path</p> <p>Choose a local directory for object storage, e.g., <code>/tmp/micromegas-storage</code> or <code>C:\\temp\\micromegas-storage</code> on Windows.</p>"},{"location":"getting-started/#installation-steps","title":"Installation Steps","text":""},{"location":"getting-started/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/madesroches/micromegas.git\ncd micromegas\n</code></pre>"},{"location":"getting-started/#2-start-all-services","title":"2. Start All Services","text":""},{"location":"getting-started/#option-a-using-tmux-linuxmacos","title":"Option A: Using tmux (Linux/macOS)","text":"<p>The easiest way to start all required services is using the development script with tmux:</p> <pre><code># Start all services in a tmux session (debug mode)\npython3 local_test_env/dev.py\n\n# Or in release mode for better performance\npython3 local_test_env/dev.py release\n</code></pre> <p>This will automatically:</p> <ul> <li>Build all Rust services</li> <li>Start PostgreSQL database</li> <li>Start telemetry-ingestion-srv on port 9000</li> <li>Start flight-sql-srv on port 32010</li> <li>Start telemetry-admin service</li> <li>Open a tmux session with all services running in separate panes</li> </ul> <p>Managing Services with tmux</p> <ul> <li>To switch between service panes: <code>Ctrl+B</code> then arrow keys</li> <li>To detach from tmux (leave services running): <code>Ctrl+B</code> then <code>D</code></li> <li>To reattach to tmux: <code>tmux attach -t micromegas</code></li> <li>To stop all services: <code>python3 local_test_env/stop-dev.py</code></li> </ul>"},{"location":"getting-started/#option-b-manual-startup-windows-or-without-tmux","title":"Option B: Manual Startup (Windows or without tmux)","text":"<p>If you're on Windows or prefer not to use tmux, start each service in a separate terminal:</p> <p>Terminal 1: PostgreSQL Database <pre><code>cd local_test_env/db\npython run.py\n</code></pre></p> <p>Terminal 2: Ingestion Server <pre><code>cd rust\ncargo run -p telemetry-ingestion-srv -- --listen-endpoint-http 127.0.0.1:9000\n</code></pre></p> <p>Terminal 3: FlightSQL Server <pre><code>cd rust\ncargo run -p flight-sql-srv -- --disable-auth\n</code></pre></p> <p>Terminal 4: Admin Service <pre><code>cd rust\ncargo run -p telemetry-admin -- crond\n</code></pre></p> <p>Service Roles</p> <ul> <li>PostgreSQL: Stores metadata and service configuration</li> <li>Ingestion Server: Receives telemetry data from applications (port 9000)</li> <li>FlightSQL Server: Provides SQL query interface for analytics (port 32010)</li> <li>Admin Service: Handles background processing and global view materialization</li> </ul>"},{"location":"getting-started/#verify-installation","title":"Verify Installation","text":""},{"location":"getting-started/#install-python-client","title":"Install Python Client","text":"<pre><code>pip install micromegas\n</code></pre>"},{"location":"getting-started/#test-with-sample-query","title":"Test with Sample Query","text":"<p>Create a test script to verify everything is working:</p> <pre><code>import datetime\nimport micromegas\n\n# Connect to local Micromegas instance\nclient = micromegas.connect()\n\n# Set up time range for query\nnow = datetime.datetime.now(datetime.timezone.utc)\nbegin = now - datetime.timedelta(days=1)\nend = now\n\n# Query recent log entries\nsql = \"\"\"\n    SELECT time, process_id, level, target, msg\n    FROM log_entries\n    ORDER BY time DESC\n    LIMIT 10;\n\"\"\"\n\n# Execute query and display results\ndf = client.query(sql, begin, end)\nprint(f\"Found {len(df)} log entries\")\nprint(df.head())\n</code></pre> <p>If you see a DataFrame with log entries (or an empty DataFrame if no data has been ingested yet), your installation is working correctly!</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have Micromegas running locally, you can:</p> <ol> <li>Unreal Engine Integration - Add observability to your Unreal Engine games</li> <li>Learn to Query Data - Explore the SQL interface and available data</li> <li>Understand the Architecture - Learn how Micromegas components work together</li> <li>Instrument Your Application - Start collecting telemetry from your own applications</li> </ol>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#common-issues","title":"Common Issues","text":"<p>Connection refused when querying : Make sure all three services are running and the FlightSQL server is listening on the default port.</p> <p>Database connection errors : Verify your PostgreSQL container is running and the connection string environment variable is correct.</p> <p>Empty query results : This is normal for a fresh installation. You'll need to instrument an application to start collecting telemetry data.</p> <p>Build errors : Ensure you have the latest Rust toolchain installed.</p>"},{"location":"getting-started/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the service logs in each terminal for error messages</li> <li>Verify all environment variables are set correctly</li> <li>Create an issue on GitHub with details about your setup</li> </ol>"},{"location":"admin/functions-reference/","title":"Administrative Functions Reference","text":"<p>This page provides detailed reference documentation for all administrative functions available in the Micromegas admin module.</p>"},{"location":"admin/functions-reference/#table-functions-udtfs","title":"Table Functions (UDTFs)","text":""},{"location":"admin/functions-reference/#list_view_sets","title":"<code>list_view_sets()</code>","text":"<p>Description: Lists all available view sets with their current schema information.</p> <p>Usage: <pre><code>SELECT * FROM list_view_sets();\n</code></pre></p> <p>Returns: Table with columns:</p> Column Type Description <code>view_set_name</code> String Name of the view set (e.g., \"log_entries\", \"measures\") <code>current_schema_hash</code> Binary Version identifier for current schema (e.g., <code>[4]</code>) <code>schema</code> String Full schema as formatted string <code>has_view_maker</code> Boolean Whether view set supports non-global instances <code>global_instance_available</code> Boolean Whether a global instance exists <p>Example: <pre><code>-- List all view sets with schema versions\nSELECT view_set_name, current_schema_hash, has_view_maker \nFROM list_view_sets()\nORDER BY view_set_name;\n\n-- Find view sets with specific schema version\nSELECT * FROM list_view_sets() \nWHERE current_schema_hash = '[4]';\n</code></pre></p> <p>Performance: Fast operation, queries in-memory view registry.</p>"},{"location":"admin/functions-reference/#list_partitions","title":"<code>list_partitions()</code>","text":"<p>Description: Lists all partitions in the lakehouse with metadata.</p> <p>Usage: <pre><code>SELECT * FROM list_partitions();\n</code></pre></p> <p>Returns: Table with columns including:</p> Column Type Description <code>view_set_name</code> String View set name <code>view_instance_id</code> String Instance ID or 'global' <code>file_schema_hash</code> Binary Schema version when partition was created <code>file_path</code> String Object storage file path <code>file_size</code> Integer File size in bytes <code>begin_insert_time</code> Timestamp Earliest event time in partition <code>end_insert_time</code> Timestamp Latest event time in partition <p>Example: <pre><code>-- List partitions for specific view set\nSELECT file_path, file_size, file_schema_hash \nFROM list_partitions() \nWHERE view_set_name = 'log_entries';\n\n-- Find partitions by schema version\nSELECT view_set_name, COUNT(*) as partition_count\nFROM list_partitions()\nWHERE file_schema_hash = '[3]'\nGROUP BY view_set_name;\n</code></pre></p> <p>Performance: Queries database metadata table, indexed by view_set_name.</p>"},{"location":"admin/functions-reference/#retire_partitionsview_set-view_instance-start_time-end_time","title":"<code>retire_partitions(view_set, view_instance, start_time, end_time)</code>","text":"<p>Description: Retires partitions within a specified time range.</p> <p>Parameters: - <code>view_set</code> (String): Target view set name - <code>view_instance</code> (String): Target view instance ID - <code>start_time</code> (Timestamp): Start of time range (inclusive) - <code>end_time</code> (Timestamp): End of time range (inclusive)</p> <p>Usage: <pre><code>SELECT * FROM retire_partitions('log_entries', 'process-123', '2024-01-01T00:00:00Z', '2024-01-02T00:00:00Z');\n</code></pre></p> <p>Returns: Table with retirement operation results.</p> <p>Safety: Uses database transactions for atomicity. All partitions in time range are retired.</p> <p>Time-Based Retirement</p> <p>This function retires ALL partitions in the specified time range, regardless of schema compatibility. Use with caution.</p>"},{"location":"admin/functions-reference/#scalar-functions-udfs","title":"Scalar Functions (UDFs)","text":""},{"location":"admin/functions-reference/#retire_partition_by_filefile_path","title":"<code>retire_partition_by_file(file_path)</code>","text":"<p>Description: Surgically retires a single partition by exact file path match.</p> <p>Parameters: - <code>file_path</code> (String): Exact file path of partition to retire</p> <p>Usage: <pre><code>SELECT retire_partition_by_file('s3://bucket/data/log_entries/process-123/2024/01/01/file.parquet') as result;\n</code></pre></p> <p>Returns: String message indicating success or failure: - Success: <code>\"SUCCESS: Retired partition &lt;file_path&gt;\"</code> - Failure: <code>\"ERROR: Partition not found: &lt;file_path&gt;\"</code></p> <p>Safety: Surgical precision - only targets the exact specified partition. Cannot accidentally retire other partitions.</p> <p>Example: <pre><code>-- Retire specific partition\nSELECT retire_partition_by_file('s3://bucket/data/log_entries/proc1/partition.parquet');\n\n-- Batch retire multiple partitions\nSELECT file_path, retire_partition_by_file(file_path) as result\nFROM (\n    SELECT file_path FROM list_partitions() \n    WHERE view_set_name = 'log_entries' \n    AND file_schema_hash = '[3]'\n    LIMIT 10\n);\n</code></pre></p> <p>Performance: Single partition operation, very fast with appropriate database indexes.</p>"},{"location":"admin/functions-reference/#python-api-functions","title":"Python API Functions","text":""},{"location":"admin/functions-reference/#micromegasadminlist_incompatible_partitionsclient-view_set_namenone","title":"<code>micromegas.admin.list_incompatible_partitions(client, view_set_name=None)</code>","text":"<p>Description: Identifies partitions with schemas incompatible with current schema versions.</p> <p>Parameters: - <code>client</code> (FlightSQLClient): Connected Micromegas client - <code>view_set_name</code> (str, optional): Filter to specific view set</p> <p>Returns: pandas DataFrame with columns:</p> Column Type Description <code>view_set_name</code> str View set name <code>view_instance_id</code> str Instance ID <code>incompatible_schema_hash</code> str Old schema version in partition <code>current_schema_hash</code> str Current schema version <code>partition_count</code> int Number of incompatible partitions <code>total_size_bytes</code> int Total size of incompatible partitions <code>file_paths</code> list Array of file paths for each partition <p>Example: <pre><code>import micromegas\nimport micromegas.admin\n\nclient = micromegas.connect()\n\n# List all incompatible partitions\nincompatible = micromegas.admin.list_incompatible_partitions(client)\nprint(f\"Found {incompatible['partition_count'].sum()} incompatible partitions\")\n\n# List for specific view set\nlog_incompatible = micromegas.admin.list_incompatible_partitions(client, 'log_entries')\nfor _, row in log_incompatible.iterrows():\n    print(f\"{row['view_set_name']}: {row['partition_count']} partitions, {row['total_size_bytes']} bytes\")\n</code></pre></p> <p>Implementation: Uses SQL JOIN between <code>list_partitions()</code> and <code>list_view_sets()</code> with server-side aggregation.</p> <p>Performance: Efficient server-side processing, minimal network overhead.</p>"},{"location":"admin/functions-reference/#micromegasadminretire_incompatible_partitionsclient-view_set_namenone","title":"<code>micromegas.admin.retire_incompatible_partitions(client, view_set_name=None)</code>","text":"<p>Description: Safely retires partitions with incompatible schemas using surgical file-path-based retirement.</p> <p>Parameters: - <code>client</code> (FlightSQLClient): Connected Micromegas client - <code>view_set_name</code> (str, optional): Filter to specific view set</p> <p>Returns: pandas DataFrame with columns:</p> Column Type Description <code>view_set_name</code> str View set processed <code>view_instance_id</code> str Instance ID processed <code>partitions_retired</code> int Count of successfully retired partitions <code>storage_freed_bytes</code> int Total bytes freed from storage <code>retirement_messages</code> list Detailed messages for each retirement attempt <p>Example: <pre><code>import micromegas\nimport micromegas.admin\n\nclient = micromegas.connect()\n\n# Preview what would be retired\npreview = micromegas.admin.list_incompatible_partitions(client, 'log_entries')\nprint(f\"Will retire {preview['partition_count'].sum()} partitions\")\n\n# Retire incompatible partitions\nresult = micromegas.admin.retire_incompatible_partitions(client, 'log_entries')\nfor _, row in result.iterrows():\n    print(f\"Retired {row['partitions_retired']} partitions from {row['view_set_name']}\")\n    print(f\"Freed {row['storage_freed_bytes']} bytes\")\n</code></pre></p> <p>Safety Features: - Uses <code>retire_partition_by_file()</code> for surgical precision - Cannot accidentally retire compatible partitions - Comprehensive error handling with detailed messages - Continues processing even if individual partitions fail - Full transaction safety and rollback protection</p> <p>Implementation:  1. Calls <code>list_incompatible_partitions()</code> to identify targets 2. For each incompatible partition, calls <code>retire_partition_by_file()</code>  3. Aggregates results and provides summary statistics 4. Includes detailed operation logs for auditing</p> <p>Performance: Processes partitions individually for safety, efficient for typical partition counts.</p>"},{"location":"admin/functions-reference/#complex-query-examples","title":"Complex Query Examples","text":""},{"location":"admin/functions-reference/#find-schema-migration-candidates","title":"Find Schema Migration Candidates","text":"<pre><code>-- Identify view sets with the most incompatible partitions\nSELECT \n    vs.view_set_name,\n    vs.current_schema_hash,\n    COUNT(DISTINCT p.file_schema_hash) as schema_versions_count,\n    SUM(CASE WHEN p.file_schema_hash != vs.current_schema_hash THEN 1 ELSE 0 END) as incompatible_count,\n    SUM(CASE WHEN p.file_schema_hash != vs.current_schema_hash THEN p.file_size ELSE 0 END) as incompatible_size_bytes\nFROM list_view_sets() vs\nLEFT JOIN list_partitions() p ON vs.view_set_name = p.view_set_name\nGROUP BY vs.view_set_name, vs.current_schema_hash\nHAVING incompatible_count &gt; 0\nORDER BY incompatible_size_bytes DESC;\n</code></pre>"},{"location":"admin/functions-reference/#analyze-partition-age-distribution","title":"Analyze Partition Age Distribution","text":"<pre><code>-- Find old incompatible partitions that are candidates for retirement\nSELECT \n    p.view_set_name,\n    p.file_schema_hash as old_schema,\n    vs.current_schema_hash,\n    COUNT(*) as partition_count,\n    MIN(p.end_insert_time) as oldest_partition,\n    MAX(p.end_insert_time) as newest_partition,\n    SUM(p.file_size) as total_size_bytes\nFROM list_partitions() p\nJOIN list_view_sets() vs ON p.view_set_name = vs.view_set_name\nWHERE p.file_schema_hash != vs.current_schema_hash\n    AND p.end_insert_time &lt; NOW() - INTERVAL '30 days'\nGROUP BY p.view_set_name, p.file_schema_hash, vs.current_schema_hash\nORDER BY oldest_partition ASC;\n</code></pre>"},{"location":"admin/functions-reference/#storage-impact-analysis","title":"Storage Impact Analysis","text":"<pre><code>-- Calculate storage savings from retiring incompatible partitions\nWITH incompatible_summary AS (\n    SELECT \n        p.view_set_name,\n        COUNT(*) as incompatible_partitions,\n        SUM(p.file_size) as incompatible_size_bytes\n    FROM list_partitions() p\n    JOIN list_view_sets() vs ON p.view_set_name = vs.view_set_name\n    WHERE p.file_schema_hash != vs.current_schema_hash\n    GROUP BY p.view_set_name\n),\ntotal_summary AS (\n    SELECT \n        view_set_name,\n        COUNT(*) as total_partitions,\n        SUM(file_size) as total_size_bytes\n    FROM list_partitions()\n    GROUP BY view_set_name\n)\nSELECT \n    t.view_set_name,\n    COALESCE(i.incompatible_partitions, 0) as incompatible_partitions,\n    t.total_partitions,\n    ROUND(100.0 * COALESCE(i.incompatible_partitions, 0) / t.total_partitions, 2) as incompatible_percentage,\n    COALESCE(i.incompatible_size_bytes, 0) as incompatible_size_bytes,\n    t.total_size_bytes,\n    ROUND(100.0 * COALESCE(i.incompatible_size_bytes, 0) / t.total_size_bytes, 2) as size_percentage\nFROM total_summary t\nLEFT JOIN incompatible_summary i ON t.view_set_name = i.view_set_name\nORDER BY size_percentage DESC;\n</code></pre>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>Micromegas is built on a modern lakehouse architecture designed for high-performance observability data collection and analytics.</p>"},{"location":"architecture/#core-components","title":"Core Components","text":"<pre><code>graph TD\n    subgraph \"Application Layer\"\n        App1[Your Application]\n        App2[Another Service]\n        App3[Third Service]\n    end\n\n    subgraph \"Micromegas Tracing\"\n        Lib1[micromegas-tracing]\n        Lib2[micromegas-tracing]\n        Lib3[micromegas-tracing]\n        Sink1[telemetry-sink]\n        Sink2[telemetry-sink]\n        Sink3[telemetry-sink]\n    end\n\n    subgraph \"Ingestion Layer\"\n        Ingestion[telemetry-ingestion-srv&lt;br/&gt;:9000 HTTP]\n    end\n\n    subgraph \"Storage Layer\"\n        PG[(PostgreSQL&lt;br/&gt;Metadata &amp; Schema)]\n        S3[(Object Storage&lt;br/&gt;S3/GCS/Local&lt;br/&gt;Raw Payloads)]\n    end\n\n    subgraph \"Maintenance\"\n        Admin[telemetry-admin&lt;br/&gt;crond]\n    end\n\n    subgraph \"Analytics Layer\"\n        DataFusion[DataFusion Engine]\n        Parquet[(Parquet Files&lt;br/&gt;Columnar Views)]\n        FlightSQL[flight-sql-srv&lt;br/&gt;:50051 FlightSQL]\n        WebApp[analytics-web-srv&lt;br/&gt;:8000 HTTP]\n    end\n\n    subgraph \"Client Layer\"\n        PyClient[Python Client]\n        Grafana[Grafana Plugin]\n        Custom[Custom Clients]\n        Browser[Web Browser&lt;br/&gt;Analytics UI]\n    end\n\n    App1 --&gt; Lib1\n    App2 --&gt; Lib2 \n    App3 --&gt; Lib3\n    Lib1 --&gt; Sink1\n    Lib2 --&gt; Sink2\n    Lib3 --&gt; Sink3\n\n    Sink1 --&gt; Ingestion\n    Sink2 --&gt; Ingestion\n    Sink3 --&gt; Ingestion\n\n    Ingestion --&gt; PG\n    Ingestion --&gt; S3\n\n    PG --&gt; DataFusion\n    S3 --&gt; DataFusion\n    DataFusion --&gt; Parquet\n    DataFusion --&gt; FlightSQL\n\n    Admin --&gt; PG\n    Admin --&gt; S3\n    Admin --&gt; Parquet\n\n    FlightSQL --&gt; PyClient\n    FlightSQL --&gt; Grafana\n    FlightSQL --&gt; Custom\n    FlightSQL --&gt; WebApp\n    WebApp --&gt; Browser\n\n    classDef app fill:#e8f5e8\n    classDef tracing fill:#fff3e0\n    classDef service fill:#f3e5f5\n    classDef storage fill:#e1f5fe\n    classDef client fill:#fce4ec\n\n    class App1,App2,App3 app\n    class Lib1,Lib2,Lib3,Sink1,Sink2,Sink3 tracing\n    class Ingestion,FlightSQL,DataFusion,Admin,WebApp service\n    class PG,S3,Parquet storage\n    class PyClient,Grafana,Custom,Browser client</code></pre>"},{"location":"architecture/#component-responsibilities","title":"Component Responsibilities","text":""},{"location":"architecture/#data-collection","title":"Data Collection","text":"<ul> <li>Tracing Library: Ultra-low overhead (20ns per event) instrumentation embedded in applications</li> <li>Telemetry Sink: Batches events and handles transmission to ingestion service</li> <li>Ingestion Service: HTTP endpoint for receiving telemetry data from sinks</li> </ul>"},{"location":"architecture/#data-storage","title":"Data Storage","text":"<ul> <li>PostgreSQL: Stores metadata, process information, and stream definitions</li> <li>Object Storage: Stores raw telemetry payloads in efficient binary format (S3, GCS, or local files)</li> <li>Lakehouse: Materialized Parquet views created on-demand for fast analytics</li> </ul>"},{"location":"architecture/#analytics-engine","title":"Analytics Engine","text":"<ul> <li>DataFusion: SQL query engine with vectorized execution optimized for Parquet (columnar format)</li> <li>FlightSQL: High-performance query protocol using Apache Arrow for data transfer</li> <li>HTTP Gateway: REST API gateway for accessing FlightSQL analytics service</li> <li>Analytics Web App: Web interface for exploring data, generating Perfetto traces, and monitoring processes</li> <li>Maintenance Daemon: Background processing for view materialization and data lifecycle</li> </ul>"},{"location":"architecture/#data-flow","title":"Data Flow","text":"<pre><code>flowchart TD\n    App[Application Code] --&gt; Lib[Micromegas Tracing]\n    Lib --&gt; Sink[Telemetry Sink]\n    Sink --&gt; HTTP[HTTP Ingestion Service]\n\n    HTTP --&gt; PG[(PostgreSQL&lt;br/&gt;Metadata)]\n    HTTP --&gt; S3[(Object Storage&lt;br/&gt;Raw Payloads)]\n\n    PG --&gt; Analytics[Analytics Engine]\n    S3 --&gt; Analytics\n    Analytics --&gt; Parquet[(Parquet Files&lt;br/&gt;Lakehouse)]\n    Analytics --&gt; Client[SQL Client]\n\n    Client --&gt; Dashboard[Dashboards &amp; Analysis]\n\n    classDef storage fill:#e1f5fe\n    classDef compute fill:#f3e5f5\n    classDef client fill:#e8f5e8\n\n    class PG,S3,Parquet storage\n    class HTTP,Analytics compute\n    class App,Client,Dashboard client</code></pre>"},{"location":"architecture/#data-flow-steps","title":"Data Flow Steps","text":"<ol> <li>Instrumentation: Applications emit telemetry events using the Micromegas tracing library</li> <li>Collection: Events are batched and sent to the ingestion service via HTTP</li> <li>Storage: Metadata stored in PostgreSQL, raw payloads stored in object storage</li> <li>Materialization: Views created on-demand from raw data using DataFusion</li> <li>Query: SQL interface provides analytics capabilities through FlightSQL</li> </ol>"},{"location":"architecture/#lakehouse-architecture","title":"Lakehouse Architecture","text":"<pre><code>flowchart TD\n    subgraph \"Data Lake Layer\"\n        Events[Live Events&lt;br/&gt;Logs, Metrics, Spans]\n        Binary[(Binary Blocks&lt;br/&gt;LZ4 Compressed&lt;br/&gt;Custom Format)]\n    end\n\n    subgraph \"Processing Layer\"\n        JIT[JIT ETL Engine]\n        Live[Live ETL&lt;br/&gt;Maintenance Daemon]\n    end\n\n    subgraph \"Lakehouse Layer\"\n        Parquet[(Parquet Files&lt;br/&gt;Columnar Format&lt;br/&gt;Optimized for Analytics)]\n        Views[Materialized Views&lt;br/&gt;Global &amp; Process-Scoped]\n    end\n\n    subgraph \"Query Layer\"\n        DataFusion[DataFusion SQL Engine]\n        Client[Query Clients]\n    end\n\n    Events --&gt; Binary\n    Binary --&gt; JIT\n    Binary --&gt; Live\n\n    JIT --&gt; Parquet\n    Live --&gt; Views\n    Views --&gt; Parquet\n\n    Parquet --&gt; DataFusion\n    DataFusion --&gt; Client\n\n    JIT -.-&gt;|\"On-demand&lt;br/&gt;Process-scoped\"| Views\n    Live -.-&gt;|\"Continuous&lt;br/&gt;Global views\"| Views\n\n    classDef datalake fill:#ffebee\n    classDef process fill:#e8f5e8\n    classDef lakehouse fill:#e3f2fd\n    classDef query fill:#f3e5f5\n\n    class Events,Binary datalake\n    class JIT,Live process\n    class Parquet,Views lakehouse\n    class DataFusion,Client query</code></pre>"},{"location":"architecture/#data-transformation-flow","title":"Data Transformation Flow","text":""},{"location":"architecture/#1-data-lake-ingestion","title":"1. Data Lake Ingestion","text":"<ul> <li>Events collected from applications in real-time</li> <li>Stored as compressed binary blocks in object storage</li> <li>Custom binary format optimized for high-throughput writes</li> </ul>"},{"location":"architecture/#2-dual-processing-strategies","title":"2. Dual Processing Strategies","text":"<p>Live ETL (Maintenance Daemon): - Processes recent data continuously (every second/minute/hour) - Creates global materialized views for cross-process analytics - Optimized for dashboards and real-time monitoring</p> <p>JIT ETL (On-Demand): - Triggered when querying process-specific data - Fetches relevant blocks, decompresses, and converts to Parquet - Optimized for deep-dive analysis and debugging</p>"},{"location":"architecture/#3-lakehouse-analytics-optimization","title":"3. Lakehouse Analytics Optimization","text":"<ul> <li>Parquet columnar format enables efficient scanning and filtering</li> <li>Dictionary compression reduces storage and improves query performance  </li> <li>Predicate pushdown leverages Parquet metadata for fast data pruning</li> </ul>"},{"location":"architecture/#analytics-web-application","title":"Analytics Web Application","text":"<p>The analytics web app provides a modern web interface for exploring telemetry data. It consists of:</p> <ul> <li>Backend: Rust-based web server (<code>analytics-web-srv</code>) using Axum framework</li> <li>Frontend: Next.js React application with TypeScript  </li> <li>Integration: Direct FlightSQL connection to analytics service</li> </ul>"},{"location":"architecture/#key-features","title":"Key Features","text":"<ul> <li>Process Explorer: View active processes with real-time metadata</li> <li>Log Viewer: Stream log entries with level filtering and color coding</li> <li>Trace Generation: Generate and download Perfetto traces from process data</li> <li>Process Statistics: Detailed process metrics and monitoring</li> </ul> <p>Development Stage</p> <p>The Analytics Web Application is in early development and only suitable for local testing. Not recommended for production use.</p>"},{"location":"architecture/#design-principles","title":"Design Principles","text":"<ul> <li>High-frequency collection: Support for 100k+ events/second per process</li> <li>Cost-efficient storage: Cheap object storage for raw data with on-demand processing</li> <li>Dual ETL strategy: Live processing for recent data, JIT for historical analysis</li> <li>Unified observability: Logs, metrics, and traces in single queryable format</li> <li>Tail sampling friendly: Store everything cheaply, process selectively</li> </ul>"},{"location":"cost-comparisons/cost-modeling/","title":"Observability Solutions: A Cost Modeling","text":"<p>Disclaimer: This document provides a high-level, qualitative comparison of the Micromegas cost model against common pricing models found in other observability solutions. This document was authored by Gemini, a large language model from Google. Direct cost comparisons are complex and depend heavily on specific usage patterns, data volumes, and negotiated enterprise pricing. This guide is intended to highlight different cost philosophies rather than provide a quantitative analysis.</p>"},{"location":"cost-comparisons/cost-modeling/#common-pricing-models-in-commercial-observability-platforms","title":"Common Pricing Models in Commercial Observability Platforms","text":"<p>Most commercial observability solutions use one or a combination of the following pricing models:</p> <ol> <li> <p>Per-GB Ingested: This is one of the most common models. You are charged based on the volume of log, metric, and trace data you send to the platform each month.</p> <ul> <li>Pros: Simple to understand initially.</li> <li>Cons: Can lead to unpredictable costs. Often encourages teams to sample aggressively or drop data to control costs, potentially losing valuable insights. Costs can scale quickly with application growth or during incidents (when you need data the most).</li> </ul> </li> <li> <p>Per-Host / Per-Node: You are charged a flat rate for each server, container, or agent you monitor.</p> <ul> <li>Pros: Predictable monthly costs.</li> <li>Cons: Can be expensive for highly dynamic or containerized environments (e.g., Kubernetes) where the number of nodes fluctuates. This model is particularly impractical for widely distributed applications (e.g., client-side instrumentation on desktop or mobile) where the node count can be massive and unpredictable. It may not accurately reflect the actual data volume or value derived from each node.</li> </ul> </li> <li> <p>Per-User: You are charged based on the number of users who have access to the platform.</p> <ul> <li>Pros: Predictable and easy to manage for small teams.</li> <li>Cons: Can become a bottleneck, discouraging widespread access to observability data across an organization. It doesn't scale well as more engineers, SREs, and product managers need access.</li> </ul> </li> <li> <p>Feature-Based Tiers: Platforms often bundle features into different tiers (e.g., Basic, Pro, Enterprise). Higher tiers unlock advanced features like longer data retention, more sophisticated analytics, or higher user counts.</p> <ul> <li>Pros: Allows you to pay for only the features you need.</li> <li>Cons: You may be forced into a much more expensive tier to get a single critical feature. The cost jump between tiers can be substantial.</li> </ul> </li> </ol>"},{"location":"cost-comparisons/cost-modeling/#the-micromegas-cost-model-direct-infrastructure-cost","title":"The Micromegas Cost Model: Direct Infrastructure Cost","text":"<p>Micromegas takes a fundamentally different approach. Instead of abstracting away the infrastructure, it is designed to be deployed and run on your own cloud account (e.g., AWS, GCP, Azure).</p> <p>Your cost is the direct cost of the underlying cloud services you consume.</p> <p>As detailed in the <code>README.md</code>, these costs are primarily:</p> <ul> <li>Blob Storage (e.g., S3, GCS): For storing raw telemetry data and materialized views. This is typically the largest portion of the cost.</li> <li>Compute (e.g., EC2, Kubernetes, Fargate): For running the ingestion, analytics, and daemon services.</li> <li>Database (e.g., PostgreSQL, Aurora): For storing metadata.</li> <li>Networking (e.g., Load Balancers, Data Transfer): For routing traffic and moving data.</li> </ul>"},{"location":"cost-comparisons/cost-modeling/#comparison-of-philosophies","title":"Comparison of Philosophies","text":"Aspect Commercial SaaS Platforms Micromegas Cost Basis Abstracted (per-GB, per-host, per-user) Concrete (direct cloud infrastructure spend) Transparency Opaque. The vendor's margin is built into the price. Fully transparent. You see every dollar spent on your cloud bill. Control Limited. You control the data you send, but not the underlying infrastructure or its cost efficiency. Full control. You can fine-tune every component, choose instance types, and optimize storage tiers. Scalability Scales automatically, but costs can become unpredictable and grow non-linearly. Cost scales more directly and predictably with resource consumption. You manage the scaling. Data Ownership Your data is in a third-party system. Your data never leaves your cloud account, enhancing security and data governance. Cost Management Relies on sampling, filtering, and dropping data before ingestion. Relies on on-demand processing (tail sampling). Keep all raw data in cheap storage and only pay to process what you need, when you need it. Management Overhead Low. The vendor manages the platform. Higher. You are responsible for deploying, managing, and scaling the Micromegas services."},{"location":"cost-comparisons/cost-modeling/#when-to-consider-the-micromegas-model","title":"When to Consider the Micromegas Model","text":"<p>The Micromegas cost model is particularly advantageous if:</p> <ul> <li>Cost at scale is a major concern. For large data volumes, paying direct infrastructure costs is almost always cheaper than paying the margins built into a commercial SaaS product.</li> <li>You want maximum control and transparency. You can make granular decisions about cost vs. performance for every component.</li> <li>Data governance and security are paramount. Keeping all telemetry data within your own cloud environment is a significant security benefit.</li> <li>You have the operational maturity to manage a distributed system.</li> </ul> <p>In summary, while commercial platforms offer convenience and managed simplicity, Micromegas provides a path to a more transparent, controllable, and potentially much lower cost structure at scale, by aligning your observability costs directly with your infrastructure spend.</p>"},{"location":"cost-comparisons/cost-modeling/#detailed-comparisons","title":"Detailed Comparisons","text":"<p>For a more in-depth, hypothetical dollar-for-dollar comparison, refer to the following documents:</p> <ul> <li>Micromegas vs. Splunk</li> <li>Micromegas vs. Grafana Cloud Stack</li> <li>Micromegas vs. Datadog</li> <li>Micromegas vs. Elastic Observability</li> <li>Micromegas vs. New Relic</li> <li>Micromegas vs. Dynatrace</li> </ul>"},{"location":"cost-comparisons/datadog/","title":"Cost Comparison: Micromegas vs. Datadog","text":"<p>Author: Gemini, a large language model from Google.</p> <p>Disclaimer: This document presents a hypothetical, dollar-for-dollar cost comparison between a self-hosted Micromegas deployment and the Datadog SaaS platform. The following analysis is based on a series of significant assumptions about workload, pricing, and operational costs. These are estimates, not quotes. Datadog's pricing is complex and modular. Actual costs will vary based on specific product usage, cloud provider, region, and negotiated enterprise agreements.</p> <p>For a broader overview of observability cost models, see the Cost Modeling document.</p>"},{"location":"cost-comparisons/datadog/#core-assumptions-for-this-comparison","title":"Core Assumptions for this Comparison","text":"<ol> <li> <p>Workload Definition (based on Micromegas Example Deployment):</p> <ul> <li>Infrastructure: 20 hosts/nodes to monitor.</li> <li>Total Events (over 90-day retention):<ul> <li>Logs: 9 billion log entries</li> <li>Metrics: 275 billion metric data points</li> <li>Traces: 165 billion trace events</li> </ul> </li> <li>Monthly Ingestion Rate (for pricing comparison):<ul> <li>Logs: 3 billion log entries / month (9 billion / 3 months)</li> <li>Metrics: ~92 billion metric data points / month (275 billion / 3 months)</li> <li>Traces: 55 billion trace events / month (165 billion / 3 months)</li> </ul> </li> <li>Retention: 90 days for logs and traces (requires extended retention plans).</li> </ul> </li> <li> <p>Datadog Pricing Assumption:</p> <ul> <li>We will use the publicly available pricing for the Datadog Pro and Enterprise plans as of mid-2025, as different products are required. Datadog's pricing is highly modular, with costs varying based on specific product usage and consumption. (see references 1, 2)</li> <li>Datadog's pricing is highly modular. We will estimate the cost by combining the necessary components.</li> <li>Assumption on Datadog Data Size: To enable a dollar-for-dollar comparison based on events, we must estimate the billable units for Datadog.<ul> <li>Average log entry size for Datadog (after processing): 500 bytes. This is a common approximation for a typical, well-structured log entry after processing, including the message, timestamp, and various attributes/tags. Datadog's API supports log entries up to 1MB, implying that typical entries are significantly smaller. (see reference 3)</li> <li>Average trace event size for Datadog (after processing): 1 KB. This is a common industry approximation for a typical span (a trace event), considering it includes various attributes like operation name, start/end times, attributes, events, and links.</li> </ul> </li> </ul> </li> <li> <p>Micromegas TCO Assumption:</p> <ul> <li>The Total Cost of Ownership (TCO) for a self-hosted solution must include both direct infrastructure spend and the cost of personnel to manage the system.</li> <li>Infrastructure Costs: Based on the \"Example Deployment\" in the main <code>README.md</code>, the estimated monthly cloud spend for this workload (which results in ~8.5 TB of storage for Micromegas) is ~$1,000 / month.</li> <li>Operational Personnel Costs: We assume managing the self-hosted solution requires 20% of one full-time DevOps/SRE engineer's time. At a fully-loaded annual salary of $150,000, this equates to ~$2,500 / month.</li> </ul> </li> </ol>"},{"location":"cost-comparisons/datadog/#the-challenge-of-traces-why-direct-comparison-is-impractical","title":"The Challenge of Traces: Why Direct Comparison is Impractical","text":"<p>Micromegas is designed to ingest and store a very high volume of raw trace events (165 billion total, or 55 billion per month in our example) and process them on-demand. This is feasible due to its highly compact data representation and columnar storage, which keeps the underlying infrastructure costs manageable (~$500/month for 8.5 TB of total data, including traces).</p> <p>Commercial SaaS tracing solutions like Datadog APM, Grafana Tempo, or Elastic APM are typically priced based on ingested GB or spans, and their underlying architectures are optimized for real-time analysis and high-cardinality indexing. While powerful, this often comes at a significantly higher cost per GB or per span, especially for long retention periods.</p> <p>For the 165 billion trace events (equivalent to 165 TB of raw data at 1KB/event) with 90-day retention, the estimated cost in a typical SaaS tracing solution would be prohibitively expensive (e.g., hundreds of thousands of dollars per month). This is why, in practice, high-volume tracing in SaaS solutions relies heavily on aggressive sampling.</p> <ul> <li>SaaS Tracing Reality: To manage costs, users of SaaS tracing solutions often implement head-based or tail-based sampling, meaning only a small fraction (e.g., 1-10%) of traces are actually ingested and retained. This sacrifices data completeness for cost control.</li> <li>Micromegas Tracing Philosophy: Micromegas is designed to ingest and retain a significantly larger volume of raw trace data compared to typical SaaS solutions. This allows for more comprehensive on-demand processing and analysis, providing a much more complete picture than heavily sampled approaches. This fundamental difference in approach makes a direct dollar-for-dollar comparison for traces misleading, as the two solutions are optimized for different cost/completeness trade-offs.</li> </ul>"},{"location":"cost-comparisons/datadog/#analysis-for-hypothetical-workload","title":"Analysis for Hypothetical Workload","text":""},{"location":"cost-comparisons/datadog/#1-estimated-monthly-cost-micromegas-self-hosted","title":"1. Estimated Monthly Cost: Micromegas (Self-Hosted)","text":"<p>The estimated Total Cost of Ownership (TCO) for a self-hosted Micromegas instance is calculated by combining direct infrastructure costs with the cost of personnel required to manage the system.</p> <ul> <li>Infrastructure Costs: ~$1,000 / month<ul> <li>(Includes blob storage, compute, database, and load balancer based on the example deployment, handling the specified event volume with ~8.5 TB of storage)</li> </ul> </li> <li> <p>Operational &amp; Personnel Costs: ~$2,500 / month</p> <ul> <li>(Assumes 20% of a DevOps/SRE engineer's time)</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (TCO): ~$3,500 / month</p> </li> </ul>"},{"location":"cost-comparisons/datadog/#2-estimated-monthly-cost-datadog-logs-metrics-only","title":"2. Estimated Monthly Cost: Datadog (Logs &amp; Metrics Only)","text":"<p>The Datadog cost is calculated by summing the costs for its individual products based on the defined workload and assumed data sizes, including the cost of extended retention.</p> <ul> <li> <p>Infrastructure Monitoring:</p> <ul> <li><code>20 hosts * ~$23/host/month (Pro Plan)</code></li> <li>Estimated Cost: ~$460 / month</li> </ul> </li> <li> <p>Log Management:</p> <ul> <li>Ingestion Volume: <code>3 billion log entries/month * 500 bytes/entry = 1,500 GB/month</code></li> <li>Ingestion Cost: <code>1,500 GB/month * ~$0.10/GB = ~$150 / month</code></li> <li>Retention Cost (90 days): Datadog charges for log retention beyond 15 days. Assuming <code>~$2.50 per million log events</code> for 60 extra days, this is a rough estimate.</li> <li><code>3,000 million log events/month * ~$2.50/million = ~$7,500 / month</code></li> <li>Estimated Cost (including retention): ~$7,650 / month</li> </ul> </li> <li> <p>Subtotal (Platform): ~$8,110 / month</p> </li> <li> <p>Operational &amp; Personnel Costs:</p> <ul> <li>Datadog is a feature-rich but complex platform that requires significant internal expertise to manage effectively. This cost is not zero but is considered part of the value of the SaaS subscription for this comparison.</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (Logs &amp; Metrics): ~$8,110 / month</p> </li> </ul>"},{"location":"cost-comparisons/datadog/#dollar-for-dollar-comparison-summary","title":"Dollar-for-Dollar Comparison Summary","text":"Category Micromegas (Self-Hosted) Datadog (SaaS) Infrastructure Cost ~$1,000 / month (Included in subscription) Personnel / Ops Cost ~$2,500 / month (Included in subscription) Licensing / Subscription $0 ~$8,110 / month Total Estimated Cost ~$3,500 / month ~$8,110 / month"},{"location":"cost-comparisons/datadog/#qualitative-differences","title":"Qualitative Differences","text":"<p>This comparison highlights the dramatic cost difference for high-volume telemetry between a self-hosted, compact solution and a feature-rich SaaS platform.</p> <ul> <li> <p>Total Cost of Ownership (TCO): For the same volume of events, the estimated TCO for Micromegas is dramatically lower than for Datadog. This difference is primarily driven by the much more compact data representation and storage efficiency of Micromegas, especially for logs and traces, which directly translates to lower infrastructure costs.</p> </li> <li> <p>Platform Philosophy:</p> <ul> <li>Datadog (Federated but Integrated): Datadog's platform consists of distinct products for logs, metrics, and traces. While highly integrated for user experience, the underlying data is stored in specialized systems, leading to separate billing for each.</li> <li>Micromegas (Unified): Micromegas uses a single, unified storage and query layer for all telemetry types. This can offer more powerful and fundamental data correlation capabilities and significantly better storage efficiency.</li> </ul> </li> <li> <p>Cost Complexity: Datadog's pricing model is famously complex and modular. While this offers flexibility, it can also lead to unpredictable and extremely high costs as you enable more features or as usage patterns change, especially for high-volume data. Micromegas's cost is simpler to understand, as it maps directly to your cloud bill.</p> </li> <li> <p>Control &amp; Data Ownership: This remains a primary differentiator. Micromegas offers full data ownership and control within your own cloud environment, which is a critical requirement for many organizations.</p> </li> <li> <p>Cost at Extreme Scale: The cost dynamics are heavily skewed towards Micromegas at extreme scale due to its superior data compactness. In a real-world scenario, to manage these costs, a Datadog deployment handling such high volumes would rely heavily on aggressive sampling of logs and traces, potentially sacrificing data completeness for cost efficiency.</p> </li> </ul>"},{"location":"cost-comparisons/datadog/#references","title":"References","text":"<ol> <li>Datadog Pricing: A Comprehensive Guide - Middleware</li> <li>Datadog Pricing Main Caveats Explained (Updated for 2025) - SigNoz</li> <li>Datadog API Reference - Log Submission</li> </ol>"},{"location":"cost-comparisons/dynatrace/","title":"Cost Comparison: Micromegas vs. Dynatrace","text":"<p>Author: Gemini, a large language model from Google.</p> <p>Disclaimer: This document presents a hypothetical, dollar-for-dollar cost comparison between a self-hosted Micromegas deployment and the Dynatrace SaaS platform. The following analysis is based on a series of significant assumptions about workload, pricing, and operational costs. These are estimates, not quotes. Dynatrace's pricing is complex, primarily based on host units, Dynatrace Units (DDUs) for data ingestion, and user seats. Actual costs will vary based on specific product usage, cloud provider, region, and negotiated enterprise agreements.</p> <p>For a broader overview of observability cost models, see the Cost Modeling document.</p>"},{"location":"cost-comparisons/dynatrace/#core-assumptions-for-this-comparison","title":"Core Assumptions for this Comparison","text":"<ol> <li> <p>Workload Definition (based on Micromegas Example Deployment):</p> <ul> <li>Infrastructure: 20 hosts/nodes to monitor.</li> <li>Total Events (over 90-day retention):<ul> <li>Logs: 9 billion log entries</li> <li>Metrics: 275 billion metric data points</li> <li>Traces: 165 billion trace events</li> </ul> </li> <li>Monthly Ingestion Rate (for pricing comparison):<ul> <li>Logs: 3 billion log entries / month (9 billion / 3 months)</li> <li>Metrics: ~92 billion metric data points / month (275 billion / 3 months)</li> <li>Traces: 55 billion trace events / month (165 billion / 3 months)</li> </ul> </li> <li>Retention: 90 days (3 months).</li> <li>Users: 5 active users.</li> </ul> </li> <li> <p>Dynatrace Pricing Assumption:</p> <ul> <li>Dynatrace's pricing is primarily based on Host Units (for infrastructure and APM monitoring) and Dynatrace Units (DDUs) for data ingestion (logs, custom metrics, traces).</li> <li>We will use publicly available pricing estimates for their standard plans as of mid-2025.</li> <li>Host Unit Cost: We assume ~$69 per Host Unit per month (for a typical cloud instance).</li> <li>DDU Cost: We assume ~$0.001 per DDU (with DDUs consumed by logs, custom metrics, and traces).</li> <li>Assumption on Dynatrace Data Size (DDUs): To enable a dollar-for-dollar comparison based on events, we must estimate the DDU consumption for these events in Dynatrace. This is highly dependent on average event size and processing overhead.<ul> <li>Average log entry DDU consumption: ~0.0005 DDUs per log event (equivalent to ~0.5 KB of raw log data).</li> <li>Average metric data point DDU consumption: ~0.000001 DDUs per metric data point (for custom metrics).</li> <li>Average trace event DDU consumption: ~0.0005 DDUs per trace event (for spans).</li> </ul> </li> </ul> </li> <li> <p>Micromegas TCO Assumption:</p> <ul> <li>The Total Cost of Ownership (TCO) for a self-hosted solution must include both direct infrastructure spend and the cost of personnel to manage the system.</li> <li>Infrastructure Costs: Based on the \"Example Deployment\" in the main <code>README.md</code>, the estimated monthly cloud spend for this workload (which results in ~8.5 TB of storage for Micromegas) is ~$1,000 / month.</li> <li>Operational Personnel Costs: We assume managing the self-hosted solution requires 20% of one full-time DevOps/SRE engineer's time. At a fully-loaded annual salary of $150,000, this equates to ~$2,500 / month.</li> </ul> </li> </ol>"},{"location":"cost-comparisons/dynatrace/#the-challenge-of-traces-why-direct-comparison-is-impractical","title":"The Challenge of Traces: Why Direct Comparison is Impractical","text":"<p>Micromegas is designed to ingest and store a very high volume of raw trace events (165 billion total, or 55 billion per month in our example) and process them on-demand. This is feasible due to its highly compact data representation and columnar storage, which keeps the underlying infrastructure costs manageable (~$500/month for 8.5 TB of total data, including traces).</p> <p>Commercial SaaS tracing solutions like Dynatrace, Grafana Tempo, Datadog APM, or Elastic APM are typically priced based on ingested GB, spans, or DDUs, and their underlying architectures are optimized for real-time analysis and high-cardinality indexing. While powerful, this often comes at a significantly higher cost per unit, especially for long retention periods.</p> <p>For the 165 billion trace events (equivalent to 165 TB of raw data at 1KB/event) with 90-day retention, the estimated cost in a typical SaaS tracing solution would be prohibitively expensive (e.g., hundreds of thousands of dollars per month). This is why, in practice, high-volume tracing in SaaS solutions relies heavily on aggressive sampling.</p> <ul> <li>SaaS Tracing Reality: To manage costs, users of SaaS tracing solutions often implement head-based or tail-based sampling, meaning only a small fraction (e.g., 1-10%) of traces are actually ingested and retained. This sacrifices data completeness for cost control.</li> <li>Micromegas Tracing Philosophy: Micromegas is designed to ingest and retain a significantly larger volume of raw trace data compared to typical SaaS solutions. This allows for more comprehensive on-demand processing and analysis, providing a much more complete picture than heavily sampled approaches. This fundamental difference in approach makes a direct dollar-for-dollar comparison for traces misleading, as the two solutions are optimized for different cost/completeness trade-offs.</li> </ul>"},{"location":"cost-comparisons/dynatrace/#analysis-for-hypothetical-workload","title":"Analysis for Hypothetical Workload","text":""},{"location":"cost-comparisons/dynatrace/#1-estimated-monthly-cost-micromegas-self-hosted","title":"1. Estimated Monthly Cost: Micromegas (Self-Hosted)","text":"<p>The estimated Total Cost of Ownership (TCO) for a self-hosted Micromegas instance is calculated by combining direct infrastructure costs with the cost of personnel required to manage the system.</p> <ul> <li>Infrastructure Costs: ~$1,000 / month<ul> <li>(Includes blob storage, compute, database, and load balancer based on the example deployment, handling the specified event volume with ~8.5 TB of storage)</li> </ul> </li> <li> <p>Operational &amp; Personnel Costs: ~$2,500 / month</p> <ul> <li>(Assumes 20% of a DevOps/SRE engineer's time)</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (TCO): ~$3,500 / month</p> </li> </ul>"},{"location":"cost-comparisons/dynatrace/#2-estimated-monthly-cost-dynatrace-logs-metrics-only","title":"2. Estimated Monthly Cost: Dynatrace (Logs &amp; Metrics Only)","text":"<p>The Dynatrace cost is calculated by summing the costs for Host Units and DDU consumption for logs and metrics.</p> <ul> <li> <p>Host Units:</p> <ul> <li><code>20 hosts * ~$69/host unit/month</code></li> <li>Subtotal (Host Units): ~$1,380 / month</li> </ul> </li> <li> <p>Logs (DDUs):</p> <ul> <li><code>3 billion log entries/month * 0.0005 DDUs/log event = 1,500,000 DDUs/month</code></li> <li><code>1,500,000 DDUs * ~$0.001/DDU = ~$1,500 / month</code></li> <li>Subtotal (Logs DDUs): ~$1,500 / month</li> </ul> </li> <li> <p>Metrics (DDUs - for custom metrics beyond standard host metrics):</p> <ul> <li><code>~92 billion metric data points/month * 0.000001 DDUs/metric = ~92,000 DDUs/month</code></li> <li><code>~92,000 DDUs * ~$0.001/DDU = ~$92 / month</code></li> <li>Subtotal (Metrics DDUs): ~$92 / month</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (Logs &amp; Metrics): ~$2,972 / month</p> </li> <li> <p>Operational &amp; Personnel Costs:</p> <ul> <li>Dynatrace is a highly automated SaaS platform, but it still requires internal expertise for configuration, custom dashboards, and leveraging its advanced AI capabilities. This cost is considered part of the subscription's value.</li> </ul> </li> <li> <p>Total Estimated Monthly Cost: ~$2,972 / month</p> </li> </ul>"},{"location":"cost-comparisons/dynatrace/#dollar-for-dollar-comparison-summary","title":"Dollar-for-Dollar Comparison Summary","text":"Category Micromegas (Self-Hosted) Dynatrace (SaaS) Infrastructure Cost ~$1,000 / month (Included in subscription) Personnel / Ops Cost ~$2,500 / month (Included in subscription) Licensing / Subscription $0 ~$2,972 / month Total Estimated Cost ~$3,500 / month ~$2,972 / month"},{"location":"cost-comparisons/dynatrace/#qualitative-differences","title":"Qualitative Differences","text":"<p>This comparison highlights the impact of host-based and DDU-based pricing models on overall costs.</p> <ul> <li> <p>Total Cost of Ownership (TCO): For the same volume of events (excluding traces), the estimated TCO for Micromegas is lower than for Dynatrace. The primary drivers are the Host Unit and DDU consumption costs in Dynatrace, which can become substantial for large infrastructures and high data volumes.</p> </li> <li> <p>Platform Philosophy:</p> <ul> <li>Dynatrace (AI-Powered Observability): Dynatrace is known for its highly automated, AI-driven approach to observability, offering deep insights into application performance and dependencies with minimal manual configuration. It aims to provide answers, not just data.</li> <li>Micromegas (Unified &amp; Self-Hosted): Micromegas provides a unified data model for all telemetry types within your own cloud environment, offering greater control and cost efficiency for high-volume data, particularly when deep, unsampled data is required.</li> </ul> </li> <li> <p>Pricing Model: Dynatrace's pricing combines Host Units (for infrastructure and APM) with DDUs (for data ingestion). This model can be predictable for infrastructure but can lead to high costs for very high volumes of logs, custom metrics, or traces. Micromegas's cost is directly tied to your cloud infrastructure spend.</p> </li> <li> <p>Operational Burden: Dynatrace, as a SaaS, has a lower operational burden for managing the platform itself due to its high automation. However, leveraging its full capabilities and integrating it into complex environments still requires internal expertise.</p> </li> <li> <p>Control &amp; Data Ownership: Micromegas provides full data ownership and control within your own cloud account, which is a critical requirement for many organizations.</p> </li> <li> <p>Cost at Extreme Scale: For very high data volumes, especially logs and traces, the DDU consumption in Dynatrace can become very high, potentially making a self-hosted solution like Micromegas more cost-effective. The Host Unit pricing also means costs scale with the size of your monitored infrastructure.</p> </li> </ul>"},{"location":"cost-comparisons/elastic/","title":"Cost Comparison: Micromegas vs. Elastic Observability","text":"<p>Author: Gemini, a large language model from Google.</p> <p>Disclaimer: This document presents a hypothetical, dollar-for-dollar cost comparison between a self-hosted Micromegas deployment and the Elastic Observability solution hosted on Elastic Cloud. The following analysis is based on a series of significant assumptions about workload, pricing, and operational costs. These are estimates, not quotes. Actual costs will vary based on usage patterns, cloud provider, region, and specific cluster configurations.</p>"},{"location":"cost-comparisons/elastic/#core-assumptions-for-this-comparison","title":"Core Assumptions for this Comparison","text":"<ol> <li> <p>Workload Definition (based on Micromegas Example Deployment):</p> <ul> <li>Total Events (over 90-day retention):<ul> <li>Logs: 9 billion log entries</li> <li>Metrics: 275 billion metric data points</li> <li>Traces: 165 billion trace events (equivalent to 82.5 billion spans)</li> </ul> </li> <li>Monthly Ingestion Rate (for pricing comparison):<ul> <li>Logs: 3 billion log entries / month (9 billion / 3 months)</li> <li>Metrics: ~92 billion metric data points / month (275 billion / 3 months)</li> <li>Traces: 55 billion trace events / month (165 billion / 3 months)</li> </ul> </li> <li>Retention: 90 days (3 months)</li> </ul> </li> <li> <p>Elastic Cloud Pricing Assumption:</p> <ul> <li>Elastic Cloud uses resource-based pricing (RAM, Storage). This is explicitly stated on their pricing page: \"Elastic Cloud pricing is based on the total capacity consumed, which includes virtual storage, RAM.\" (see reference 1)</li> <li>We need to estimate the cluster size required to handle the event volume and retention.</li> <li>Assumption on Elastic Data Size: To enable a dollar-for-dollar comparison based on events, we must estimate the storage consumed by these events in Elastic. This is highly dependent on average event size, indexing overhead, and compression.<ul> <li>Average log entry size in Elastic (after indexing/overhead): 500 bytes. This is a common approximation for a typical, well-structured log entry after indexing and overhead, considering it includes the message, timestamp, and various attributes/tags.</li> <li>Average metric data point size in Elastic: 100 bytes. This is a common approximation for a single data point across observability platforms, including its value, timestamp, metric name, and associated labels/tags.</li> <li>Average trace span size in Elastic: 1 KB. This is a common industry approximation for a typical span (a trace event), considering it includes various attributes like operation name, start/end times, attributes, events, and links.</li> </ul> </li> <li>Calculated Storage Needed for Elastic:<ul> <li>Logs: 9 billion * 500 bytes = 4.5 TB</li> <li>Metrics: 275 billion * 100 bytes = 27.5 TB</li> <li>Traces: 82.5 billion * 1 KB = 82.5 TB</li> <li>Total Raw Storage Needed: ~114.5 TB</li> </ul> </li> <li>Elastic Cloud Cluster Sizing: To handle ~114.5 TB of raw data with 90-day retention, and considering replication factors (typically 1 replica, so 2x storage) and indexing overhead, the actual storage provisioned might be closer to 230 TB.</li> </ul> </li> <li> <p>Micromegas TCO Assumption:</p> <ul> <li>The Total Cost of Ownership (TCO) for a self-hosted solution must include both direct infrastructure spend and the cost of personnel to manage the system.</li> <li>Infrastructure Costs: Based on the \"Example Deployment\" in the main <code>README.md</code>, the estimated monthly cloud spend for this workload (which results in ~8.5 TB of storage for Micromegas) is ~$1,000 / month.</li> <li>Operational Personnel Costs: We assume managing the self-hosted solution requires 20% of one full-time DevOps/SRE engineer's time. At a fully-loaded annual salary of $150,000, this equates to ~$2,500 / month.</li> </ul> </li> </ol>"},{"location":"cost-comparisons/elastic/#analysis-for-hypothetical-workload","title":"Analysis for Hypothetical Workload","text":""},{"location":"cost-comparisons/elastic/#1-estimated-monthly-cost-micromegas-self-hosted","title":"1. Estimated Monthly Cost: Micromegas (Self-Hosted)","text":"<p>The estimated Total Cost of Ownership (TCO) for a self-hosted Micromegas instance is calculated by combining direct infrastructure costs with the cost of personnel required to manage the system.</p> <ul> <li>Infrastructure Costs: ~$1,000 / month<ul> <li>(Includes blob storage, compute, database, and load balancer based on the example deployment, handling the specified event volume with ~8.5 TB of storage)</li> </ul> </li> <li> <p>Operational &amp; Personnel Costs: ~$2,500 / month</p> <ul> <li>(Assumes 20% of a DevOps/SRE engineer's time)</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (TCO): ~$3,500 / month</p> </li> </ul>"},{"location":"cost-comparisons/elastic/#2-estimated-monthly-cost-elastic-cloud","title":"2. Estimated Monthly Cost: Elastic Cloud","text":"<p>The Elastic Cloud cost is calculated based on the resources required for a cluster capable of handling the specified event volume and retention.</p> <ul> <li> <p>Cluster Configuration: To handle ~38 TB of raw data per month (which translates to ~76 TB provisioned storage with replication and overhead) and the associated query load, a substantial cluster is required.</p> <ul> <li>We will estimate a cluster with 76 TB of storage and corresponding compute (e.g., 2,400 GB RAM).</li> </ul> </li> <li> <p>Estimated Monthly Cost:</p> <ul> <li>Based on public pricing for I/O Optimized instances:<ul> <li>RAM cost: <code>2,400 GB * ~$0.50/GB/month = ~$1,200 / month</code></li> <li>Storage cost: <code>76,000 GB * ~$0.10/GB/month = ~$7,600 / month</code></li> </ul> </li> <li>Subtotal (Platform): ~$8,800 / month</li> </ul> </li> <li> <p>Operational &amp; Personnel Costs:</p> <ul> <li>Elastic Cloud is a managed service, but it still requires significant expertise to manage data schemas (index templates), build visualizations in Kibana, and optimize queries. This cost is considered part of the value of the SaaS subscription for this comparison.</li> </ul> </li> <li> <p>Total Estimated Monthly Cost: ~$8,800 / month</p> </li> </ul>"},{"location":"cost-comparisons/elastic/#dollar-for-dollar-comparison-summary","title":"Dollar-for-Dollar Comparison Summary","text":"Category Micromegas (Self-Hosted) Elastic Cloud (SaaS) Infrastructure Cost ~$1,000 / month (Included in subscription) Personnel / Ops Cost ~$2,500 / month (Included in subscription) Licensing / Subscription $0 ~$8,800 / month Total Estimated Cost ~$3,500 / month ~$8,800 / month"},{"location":"cost-comparisons/elastic/#qualitative-differences","title":"Qualitative Differences","text":"<p>This comparison highlights the significant impact of data compactness on overall cost, especially for high-volume telemetry.</p> <ul> <li> <p>Total Cost of Ownership (TCO): For the same volume of events, the estimated TCO for Micromegas is significantly lower than for Elastic Cloud. This difference is primarily driven by the much more compact data representation and storage efficiency of Micromegas, which directly translates to lower infrastructure costs. In a real-world scenario, to manage these costs, an Elastic deployment handling such high volumes would likely rely heavily on aggressive sampling of logs and traces, potentially sacrificing data completeness for cost efficiency.</p> </li> <li> <p>Architectural Philosophy:</p> <ul> <li>Elastic (Search Index-Centric): The Elastic Stack was built around the Lucene search index. It is exceptionally powerful for log search and text analysis. Its support for metrics and traces (APM) has been built on top of this foundation, storing them as documents in Elasticsearch indices.</li> <li>Micromegas (Unified Telemetry Model): Micromegas was designed from the ground up with a unified data model for logs, metrics, and traces. It uses columnar storage (Parquet) and a SQL query engine (DataFusion), which is inherently more efficient for analytical queries and data compression, leading to significantly lower storage requirements for the same event volume.</li> </ul> </li> <li> <p>Query Language: This is a major differentiator.</p> <ul> <li>Elastic: Uses KQL (Kibana Query Language) and Lucene query syntax, which are powerful for text search but can be a learning curve for teams primarily familiar with SQL.</li> <li>Micromegas: Uses SQL, the standard language for data analysis. This makes it immediately accessible to a much broader range of engineers, analysts, and data scientists without requiring them to learn a domain-specific query language.</li> </ul> </li> <li> <p>Operational Burden: Elastic Cloud has a lower operational burden, as Elastic manages the cluster's uptime, security, and patching. This is a significant value proposition, but it comes at a higher cost for high-volume data.</p> </li> <li> <p>Control &amp; Data Ownership: Micromegas provides full data ownership within your own cloud account, offering a higher degree of control and simplifying data governance.</p> </li> <li> <p>Cost Model: The cost models are fundamentally different. Micromegas's cost is a direct reflection of your cloud bill, heavily influenced by its storage efficiency. Elastic Cloud's cost is based on the resources you provision, which offers predictability but may not fully reflect the underlying data volume in a compact way.</p> </li> </ul>"},{"location":"cost-comparisons/elastic/#references","title":"References","text":"<ol> <li>Official Elastic Cloud pricing - compare serverless and hosted</li> </ol>"},{"location":"cost-comparisons/grafana/","title":"vs. Grafana Cloud","text":""},{"location":"cost-comparisons/grafana/#core-assumptions-for-this-comparison","title":"Core Assumptions for this Comparison","text":"<ol> <li> <p>Workload Definition (based on Micromegas Example Deployment):</p> <ul> <li>Total Events (over 90-day retention):<ul> <li>Logs: 9 billion log entries</li> <li>Metrics: 275 billion metric data points (equivalent to 1,000,000 active series for pricing)</li> <li>Traces: 165 billion trace events</li> </ul> </li> <li>Monthly Ingestion Rate (for pricing comparison):<ul> <li>Logs: 3 billion log entries / month (9 billion / 3 months)</li> <li>Metrics: ~92 billion metric data points / month (275 billion / 3 months)</li> <li>Traces: 55 billion trace events / month (165 billion / 3 months)</li> </ul> </li> <li>Retention: 90 days (3 months)</li> <li>Users: 5 active users.</li> </ul> </li> <li> <p>Grafana Cloud Pricing Assumption:</p> <ul> <li>We will use the publicly available pricing for the Grafana Cloud Pro plan as of mid-2025.</li> <li>Pricing is calculated based on logs ingested (GB), metric series, traces ingested (GB), and users.</li> <li>Assumption on Grafana Cloud Data Size: To enable a dollar-for-dollar comparison based on events, we must estimate the storage consumed by these events in Grafana Cloud. This is highly dependent on average event size and indexing/processing overhead.<ul> <li>Average log entry size for Loki (after processing): 500 bytes</li> <li>Average trace event size for Tempo (after processing): 1 KB</li> </ul> </li> </ul> </li> <li> <p>Micromegas TCO Assumption:</p> <ul> <li>The Total Cost of Ownership (TCO) for a self-hosted solution must include both direct infrastructure spend and the cost of personnel to manage the system.</li> <li>Infrastructure Costs: Based on the \"Example Deployment\" in the main <code>README.md</code>, the estimated monthly cloud spend for this workload (which results in ~8.5 TB of storage for Micromegas) is ~$1,000 / month.</li> <li>Operational Personnel Costs: We assume managing the self-hosted solution requires 20% of one full-time DevOps/SRE engineer's time. At a fully-loaded annual salary of $150,000, this equates to ~$2,500 / month.</li> </ul> </li> </ol>"},{"location":"cost-comparisons/grafana/#the-challenge-of-traces-why-direct-comparison-is-impractical","title":"The Challenge of Traces: Why Direct Comparison is Impractical","text":"<p>Micromegas is designed to ingest and store a very high volume of raw trace events (165 billion total, or 55 billion per month in our example) and process them on-demand. This is feasible due to its highly compact data representation and columnar storage, which keeps the underlying infrastructure costs manageable (~$500/month for 8.5 TB of total data, including traces).</p> <p>Commercial SaaS tracing solutions like Grafana Tempo, Datadog APM, or Elastic APM are typically priced based on ingested GB or spans, and their underlying architectures are optimized for real-time analysis and high-cardinality indexing. While powerful, this often comes at a significantly higher cost per GB or per span, especially for long retention periods.</p> <p>For the 165 billion trace events (equivalent to 165 TB of raw data at 1KB/event) with 90-day retention, the estimated cost in a typical SaaS tracing solution would be prohibitively expensive (e.g., hundreds of thousands of dollars per month, as seen in previous calculations). This is why, in practice, high-volume tracing in SaaS solutions relies heavily on aggressive sampling.</p> <ul> <li>SaaS Tracing Reality: To manage costs, users of SaaS tracing solutions often implement head-based or tail-based sampling, meaning only a small fraction (e.g., 1-10%) of traces are actually ingested and retained. This sacrifices data completeness for cost control.</li> <li>Micromegas Tracing Philosophy: Micromegas is designed to ingest and retain a significantly larger volume of raw trace data compared to typical SaaS solutions. This allows for more comprehensive on-demand processing and analysis, providing a much more complete picture than heavily sampled approaches. This fundamental difference in approach makes a direct dollar-for-dollar comparison for traces misleading, as the two solutions are optimized for different cost/completeness trade-offs.</li> </ul>"},{"location":"cost-comparisons/grafana/#analysis-for-hypothetical-workload","title":"Analysis for Hypothetical Workload","text":""},{"location":"cost-comparisons/grafana/#1-estimated-monthly-cost-micromegas-self-hosted","title":"1. Estimated Monthly Cost: Micromegas (Self-Hosted)","text":"<p>The estimated Total Cost of Ownership (TCO) for a self-hosted Micromegas instance is calculated by combining direct infrastructure costs with the cost of personnel required to manage the system.</p> <ul> <li>Infrastructure Costs: ~$1,000 / month<ul> <li>(Includes blob storage, compute, database, and load balancer based on the example deployment, handling the specified event volume with ~8.5 TB of storage)</li> </ul> </li> <li> <p>Operational &amp; Personnel Costs: ~$2,500 / month</p> <ul> <li>(Assumes 20% of a DevOps/SRE engineer's time)</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (TCO): ~$3,500 / month</p> </li> </ul>"},{"location":"cost-comparisons/grafana/#2-estimated-monthly-cost-grafana-cloud-pro-logs-metrics-only","title":"2. Estimated Monthly Cost: Grafana Cloud Pro (Logs &amp; Metrics Only)","text":"<p>The Grafana Cloud cost for logs and metrics is calculated by summing the costs for its individual components based on the defined workload and assumed data sizes, including the cost of extended retention.</p> <ul> <li> <p>Logs (Loki):</p> <ul> <li>Ingestion Volume: <code>3 billion log entries/month * 500 bytes/entry = 1,500 GB/month</code></li> <li>Ingestion Cost: <code>1,500 GB/month * ~$0.50/GB = ~$750 / month</code></li> <li>Retention Cost (90 days): Storing 1,500 GB for an additional 60 days is estimated to cost <code>~$0.30/GB</code>. <code>1,500 GB * $0.30/GB * 2 months</code> = <code>~$900 / month</code></li> <li>Subtotal (Logs): ~$1,650 / month</li> </ul> </li> <li> <p>Metrics (Mimir):</p> <ul> <li><code>1,000,000 active series</code> (retention is typically longer for metrics and included)</li> <li>Subtotal (Metrics): ~$1,000 / month</li> </ul> </li> <li> <p>Users:</p> <ul> <li><code>5 users</code></li> <li>Subtotal (Users): ~$100 / month</li> </ul> </li> <li> <p>Subtotal (Platform): ~$2,750 / month</p> </li> <li> <p>Operational &amp; Personnel Costs:</p> <ul> <li>Grafana Cloud is a managed service, but it still requires internal expertise to build dashboards, run searches, and manage data onboarding. This cost is considered part of the subscription's value.</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (Logs &amp; Metrics): ~$2,750 / month</p> </li> </ul> <p>Therefore, for this comparison, we will focus on the costs of logs and metrics, acknowledging that the trace handling philosophies and associated costs are fundamentally different and not directly comparable on a per-event basis without considering sampling strategies.</p>"},{"location":"cost-comparisons/grafana/#dollar-for-dollar-comparison-summary","title":"Dollar-for-Dollar Comparison Summary","text":"Category Micromegas (Self-Hosted) Grafana Cloud (SaaS) Infrastructure Cost ~$1,000 / month (Included in subscription) Personnel / Ops Cost ~$2,500 / month (Included in subscription) Licensing / Subscription $0 ~$2,750 / month Total Estimated Cost ~$3,500 / month ~$2,750 / month"},{"location":"cost-comparisons/newrelic/","title":"Cost Comparison: Micromegas vs. New Relic","text":"<p>Author: Gemini, a large language model from Google.</p> <p>Disclaimer: This document presents a hypothetical, dollar-for-dollar cost comparison between a self-hosted Micromegas deployment and the New Relic SaaS platform. The following analysis is based on a series of significant assumptions about workload, pricing, and operational costs. These are estimates, not quotes. New Relic's pricing is based on data ingested and user seats. Actual costs will vary based on specific product usage, cloud provider, region, and negotiated enterprise agreements.</p> <p>For a broader overview of observability cost models, see the Cost Modeling document.</p>"},{"location":"cost-comparisons/newrelic/#core-assumptions-for-this-comparison","title":"Core Assumptions for this Comparison","text":"<ol> <li> <p>Workload Definition (based on Micromegas Example Deployment):</p> <ul> <li>Total Events (over 90-day retention):<ul> <li>Logs: 9 billion log entries</li> <li>Metrics: 275 billion metric data points</li> <li>Traces: 165 billion trace events</li> </ul> </li> <li>Monthly Ingestion Rate (for pricing comparison):<ul> <li>Logs: 3 billion log entries / month (9 billion / 3 months)</li> <li>Metrics: ~92 billion metric data points / month (275 billion / 3 months)</li> <li>Traces: 55 billion trace events / month (165 billion / 3 months)</li> </ul> </li> <li>Retention: 90 days (3 months).</li> <li>Users: 5 Full Users (access to all data and features).</li> </ul> </li> <li> <p>New Relic Pricing Assumption:</p> <ul> <li>New Relic's pricing is primarily based on data ingested (GB) and user seats.</li> <li>We will use publicly available pricing estimates for their standard plans as of mid-2025.</li> <li>Data Ingest Cost: We assume an average cost of ~$0.30 per GB of ingested data per month.</li> <li>User Seat Cost: We assume ~$99 per Full User per month.</li> <li>Assumption on New Relic Data Size: To enable a dollar-for-dollar comparison based on events, we must estimate the ingested GB for these events in New Relic. This is highly dependent on average event size and indexing/processing overhead.<ul> <li>Average log entry size for New Relic (after processing): 500 bytes</li> <li>Average metric data point size for New Relic: 100 bytes</li> <li>Average trace event size for New Relic: 1 KB</li> </ul> </li> </ul> </li> <li> <p>Micromegas TCO Assumption:</p> <ul> <li>The Total Cost of Ownership (TCO) for a self-hosted solution must include both direct infrastructure spend and the cost of personnel to manage the system.</li> <li>Infrastructure Costs: Based on the \"Example Deployment\" in the main <code>README.md</code>, the estimated monthly cloud spend for this workload (which results in ~8.5 TB of storage for Micromegas) is ~$1,000 / month.</li> <li>Operational Personnel Costs: We assume managing the self-hosted solution requires 20% of one full-time DevOps/SRE engineer's time. At a fully-loaded annual salary of $150,000, this equates to ~$2,500 / month.</li> </ul> </li> </ol>"},{"location":"cost-comparisons/newrelic/#the-challenge-of-traces-why-direct-comparison-is-impractical","title":"The Challenge of Traces: Why Direct Comparison is Impractical","text":"<p>Micromegas is designed to ingest and store a very high volume of raw trace events (165 billion total, or 55 billion per month in our example) and process them on-demand. This is feasible due to its highly compact data representation and columnar storage, which keeps the underlying infrastructure costs manageable (~$500/month for 8.5 TB of total data, including traces).</p> <p>Commercial SaaS tracing solutions like New Relic APM, Grafana Tempo, Datadog APM, or Elastic APM are typically priced based on ingested GB or spans, and their underlying architectures are optimized for real-time analysis and high-cardinality indexing. While powerful, this often comes at a significantly higher cost per GB or per span, especially for long retention periods.</p> <p>For the 165 billion trace events (equivalent to 165 TB of raw data at 1KB/event) with 90-day retention, the estimated cost in a typical SaaS tracing solution would be prohibitively expensive (e.g., hundreds of thousands of dollars per month). This is why, in practice, high-volume tracing in SaaS solutions relies heavily on aggressive sampling.</p> <ul> <li>SaaS Tracing Reality: To manage costs, users of SaaS tracing solutions often implement head-based or tail-based sampling, meaning only a small fraction (e.g., 1-10%) of traces are actually ingested and retained. This sacrifices data completeness for cost control.</li> <li>Micromegas Tracing Philosophy: Micromegas is designed to ingest and retain a significantly larger volume of raw trace data compared to typical SaaS solutions. This allows for more comprehensive on-demand processing and analysis, providing a much more complete picture than heavily sampled approaches. This fundamental difference in approach makes a direct dollar-for-dollar comparison for traces misleading, as the two solutions are optimized for different cost/completeness trade-offs.</li> </ul>"},{"location":"cost-comparisons/newrelic/#analysis-for-hypothetical-workload","title":"Analysis for Hypothetical Workload","text":""},{"location":"cost-comparisons/newrelic/#1-estimated-monthly-cost-micromegas-self-hosted","title":"1. Estimated Monthly Cost: Micromegas (Self-Hosted)","text":"<p>The estimated Total Cost of Ownership (TCO) for a self-hosted Micromegas instance is calculated by combining direct infrastructure costs with the cost of personnel required to manage the system.</p> <ul> <li>Infrastructure Costs: ~$1,000 / month<ul> <li>(Includes blob storage, compute, database, and load balancer based on the example deployment, handling the specified event volume with ~8.5 TB of storage)</li> </ul> </li> <li> <p>Operational &amp; Personnel Costs: ~$2,500 / month</p> <ul> <li>(Assumes 20% of a DevOps/SRE engineer's time)</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (TCO): ~$3,500 / month</p> </li> </ul>"},{"location":"cost-comparisons/newrelic/#2-estimated-monthly-cost-new-relic-logs-metrics-only","title":"2. Estimated Monthly Cost: New Relic (Logs &amp; Metrics Only)","text":"<p>The New Relic cost is calculated by summing the costs for data ingested and user seats.</p> <ul> <li> <p>Calculated Ingested GB for New Relic (Logs &amp; Metrics):</p> <ul> <li>Logs: <code>3 billion log entries/month * 500 bytes/entry = 1,500 GB/month</code></li> <li>Metrics: <code>~92 billion metric data points/month * 100 bytes/point = ~9,200 GB/month</code></li> <li>Total Ingested GB (Logs &amp; Metrics): <code>1,500 + 9,200 = 10,700 GB/month</code></li> </ul> </li> <li> <p>Data Ingest Cost:</p> <ul> <li><code>10,700 GB/month * $0.30/GB</code></li> <li>Subtotal (Data Ingest): ~$3,210 / month</li> </ul> </li> <li> <p>User Seats:</p> <ul> <li><code>5 Full Users * ~$99/user/month</code></li> <li>Subtotal (Users): ~$495 / month</li> </ul> </li> <li> <p>Operational &amp; Personnel Costs:</p> <ul> <li>New Relic is a managed SaaS, but it still requires internal expertise to configure agents, build dashboards, and optimize queries. This cost is considered part of the subscription's value.</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (Logs &amp; Metrics): ~$3,705 / month</p> </li> </ul>"},{"location":"cost-comparisons/newrelic/#dollar-for-dollar-comparison-summary","title":"Dollar-for-Dollar Comparison Summary","text":"Category Micromegas (Self-Hosted) New Relic (SaaS) Infrastructure Cost ~$1,000 / month (Included in subscription) Personnel / Ops Cost ~$2,500 / month (Included in subscription) Licensing / Subscription $0 ~$3,705 / month Total Estimated Cost ~$3,500 / month ~$3,705 / month"},{"location":"cost-comparisons/newrelic/#qualitative-differences","title":"Qualitative Differences","text":"<p>This comparison highlights the impact of data volume and user-based pricing on overall costs.</p> <ul> <li> <p>Total Cost of Ownership (TCO): For the same volume of events (excluding traces), the estimated TCO for Micromegas is lower than for New Relic. The primary driver is the cost of data ingestion in New Relic, which can become substantial for high volumes.</p> </li> <li> <p>Platform Philosophy:</p> <ul> <li>New Relic (Integrated SaaS): New Relic offers a broad, integrated platform covering APM, infrastructure, logs, and more. It aims to provide a single pane of glass for observability, with a strong focus on application performance.</li> <li>Micromegas (Unified &amp; Self-Hosted): Micromegas provides a unified data model for all telemetry types within your own cloud environment, offering greater control and cost efficiency for high-volume data.</li> </ul> </li> <li> <p>Pricing Model: New Relic's pricing is a combination of data ingested and user seats. While data ingest is common, the per-user pricing can become a significant factor for larger teams. Micromegas's cost is directly tied to your cloud infrastructure spend.</p> </li> <li> <p>Operational Burden: New Relic, as a SaaS, has a lower operational burden for managing the platform itself. However, configuring agents and optimizing data ingestion still requires internal effort.</p> </li> <li> <p>Control &amp; Data Ownership: Micromegas provides full data ownership and control within your own cloud account, which is a critical requirement for many organizations.</p> </li> <li> <p>Cost at Extreme Scale: For very high data volumes, the cost of data ingestion in New Relic can become very high, potentially making a self-hosted solution like Micromegas more cost-effective. The user-based pricing also means costs scale with team size, regardless of data volume.</p> </li> </ul>"},{"location":"cost-comparisons/splunk/","title":"Cost Comparison: Micromegas vs. Splunk","text":"<p>Author: Gemini, a large language model from Google.</p> <p>Disclaimer: This document presents a hypothetical, dollar-for-dollar cost comparison between a self-hosted Micromegas deployment and a Splunk Cloud subscription. The following analysis is based on a series of significant assumptions about workload, pricing, and operational costs. These are estimates, not quotes. Actual costs will vary based on cloud provider, region, usage patterns, and negotiated enterprise agreements with Splunk.</p> <p>For a broader overview of observability cost models, see the Cost Modeling document.</p>"},{"location":"cost-comparisons/splunk/#core-assumptions-for-this-comparison","title":"Core Assumptions for this Comparison","text":"<ol> <li> <p>Workload Definition (based on Micromegas Example Deployment):</p> <ul> <li>Total Events (over 90-day retention):<ul> <li>Logs: 9 billion log entries</li> <li>Metrics: 275 billion metric data points</li> <li>Traces: 165 billion trace events</li> </ul> </li> <li>Monthly Ingestion Rate (for pricing comparison):<ul> <li>Logs: 3 billion log entries / month (9 billion / 3 months)</li> <li>Metrics: ~92 billion metric data points / month (275 billion / 3 months)</li> <li>Traces: 55 billion trace events / month (165 billion / 3 months)</li> </ul> </li> <li>Retention: 90 days (3 months).</li> </ul> </li> <li> <p>Splunk Pricing Assumption:</p> <ul> <li>Splunk's pricing is complex and not fully public. For this analysis, we assume an ingest-based pricing model for Splunk Cloud. This is supported by Splunk's official pricing page, which states: \"Pay based on the amount of data you bring into the Splunk Platform.\" (see reference 1)</li> <li>Based on publicly available industry analysis, which indicates significant volume discounts, we will use an estimated cost of $2.25 per GB of ingested data per month for this high-volume workload. This is a critical assumption, as actual costs can vary significantly based on negotiated enterprise rates. This estimate is a conservative adjustment to low-volume pricing examples (such as the one cited in reference 2) to account for expected discounts at scale. (see references 1, 2)</li> <li>Assumption on Splunk Data Size: To enable a dollar-for-dollar comparison based on events, we must estimate the ingested GB for these events in Splunk. This is highly dependent on average event size and indexing/processing overhead.<ul> <li>Average log entry size for Splunk (after indexing/overhead): 500 bytes</li> <li>Average metric data point size for Splunk: 100 bytes. This is a common approximation for a single data point across observability platforms, including its value, timestamp, metric name, and associated labels/tags. For example, Datadog's API documentation suggests that a metric data point, including its timestamp (8 bytes), value (8 bytes), metric name (approx. 20 bytes), and typical labels/tags (approx. 50 bytes of overhead per data point for unique identification), sums up to around 100 bytes when considering additional overhead. (see reference 3)</li> <li>Average trace event size for Splunk: 1 KB. While Splunk does not provide an exact average, this is a common industry approximation for a typical span (a trace event), considering it includes various attributes like operation name, start/end times, attributes, events, and links. Splunk APM has a maximum span size limit of 64KB, implying that typical spans are significantly smaller.</li> </ul> </li> </ul> </li> <li> <p>Micromegas Operational Cost Assumption:</p> <ul> <li>Self-hosting requires engineering time for setup, maintenance, and upgrades. This is a real cost.</li> <li>We assume this requires 20% of one full-time DevOps/SRE engineer's time.</li> <li>We assume a fully-loaded annual salary of $150,000 for this engineer, which translates to a monthly cost of $12,500.</li> </ul> </li> </ol>"},{"location":"cost-comparisons/splunk/#the-challenge-of-traces-why-direct-comparison-is-impractical","title":"The Challenge of Traces: Why Direct Comparison is Impractical","text":"<p>Micromegas is designed to ingest and store a very high volume of raw trace events (165 billion total, or 55 billion per month in our example) and process them on-demand. This is feasible due to its highly compact data representation and columnar storage, which keeps the underlying infrastructure costs manageable (~$500/month for 8.5 TB of total data, including traces).</p> <p>Commercial SaaS tracing solutions like Splunk APM, Grafana Tempo, Datadog APM, or Elastic APM are typically priced based on ingested GB or spans, and their underlying architectures are optimized for real-time analysis and high-cardinality indexing. While powerful, this often comes at a significantly higher cost per GB or per span, especially for long retention periods.</p> <p>For the 165 billion trace events (equivalent to 165 TB of raw data at 1KB/event) with 90-day retention, the estimated cost in a typical SaaS tracing solution would be prohibitively expensive (e.g., hundreds of thousands of dollars per month). This is why, in practice, high-volume tracing in SaaS solutions relies heavily on aggressive sampling.</p> <ul> <li>SaaS Tracing Reality: To manage costs, users of SaaS tracing solutions often implement head-based or tail-based sampling, meaning only a small fraction (e.g., 1-10%) of traces are actually ingested and retained. This sacrifices data completeness for cost control.</li> <li>Micromegas Tracing Philosophy: Micromegas is designed to ingest and retain a significantly larger volume of raw trace data compared to typical SaaS solutions. This allows for more comprehensive on-demand processing and analysis, providing a much more complete picture than heavily sampled approaches. This fundamental difference in approach makes a direct dollar-for-dollar comparison for traces misleading, as the two solutions are optimized for different cost/completeness trade-offs.</li> </ul>"},{"location":"cost-comparisons/splunk/#analysis-for-hypothetical-workload","title":"Analysis for Hypothetical Workload","text":""},{"location":"cost-comparisons/splunk/#1-estimated-monthly-cost-micromegas-self-hosted","title":"1. Estimated Monthly Cost: Micromegas (Self-Hosted)","text":"<p>The Micromegas cost is broken down into direct infrastructure spend and the operational personnel cost.</p> <ul> <li> <p>Infrastructure Costs:</p> <ul> <li>Based on the \"Example Deployment\" in the main <code>README.md</code>, the estimated monthly cloud spend for this workload is ~$1,000 / month.<ul> <li>(Includes blob storage, compute, database, and load balancer)</li> </ul> </li> </ul> </li> <li> <p>Operational &amp; Personnel Costs:</p> <ul> <li>Based on the assumption of 20% of an engineer's time.</li> <li><code>$12,500/month * 0.20</code></li> <li>Subtotal (Personnel): ~$2,500 / month</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (TCO): ~$3,500 / month</p> </li> </ul>"},{"location":"cost-comparisons/splunk/#2-estimated-monthly-cost-splunk-cloud-logs-metrics-only","title":"2. Estimated Monthly Cost: Splunk Cloud (Logs &amp; Metrics Only)","text":"<p>The Splunk Cloud cost for logs and metrics is calculated based on the assumed ingest-based pricing model and the estimated ingested GB from the event volume.</p> <ul> <li> <p>Calculated Ingested GB for Splunk (Logs &amp; Metrics):</p> <ul> <li>Logs: <code>3 billion log entries/month * 500 bytes/entry = 1,500 GB/month</code></li> <li>Metrics: <code>~92 billion metric data points/month * 100 bytes/point = ~9,200 GB/month</code></li> <li>Total Ingested GB (Logs &amp; Metrics): <code>1,500 + 9,200 = 10,700 GB/month</code></li> </ul> </li> <li> <p>Ingestion Cost:</p> <ul> <li><code>10,700 GB/month * $2.25/GB</code></li> <li>Subtotal (Ingestion): ~$24,075 / month</li> </ul> </li> <li> <p>Operational &amp; Personnel Costs:</p> <ul> <li>While Splunk is a managed SaaS, it still requires internal expertise to build dashboards, run searches, and manage data onboarding. This cost is highly variable but generally lower than managing a full self-hosted solution. For this comparison, we will consider it part of the subscription's value.</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (Logs &amp; Metrics): ~$24,075 / month</p> </li> </ul>"},{"location":"cost-comparisons/splunk/#dollar-for-dollar-comparison-summary","title":"Dollar-for-Dollar Comparison Summary","text":"Category Micromegas (Self-Hosted) Splunk Cloud (SaaS) Infrastructure Cost ~$1,000 / month (Included in subscription) Personnel / Ops Cost ~$2,500 / month (Included in subscription) Licensing / Subscription $0 ~$24,075 / month Total Estimated Cost ~$3,500 / month ~$24,075 / month"},{"location":"cost-comparisons/splunk/#qualitative-differences","title":"Qualitative Differences","text":"<p>Beyond the direct cost estimates, the two solutions represent different philosophies:</p> <ul> <li>Total Cost of Ownership (TCO): For the specified workload, the estimated TCO for Micromegas is significantly lower than Splunk Cloud. The primary driver is paying direct infrastructure costs versus a bundled SaaS price that includes the vendor's margin.</li> <li>Operational Burden: Micromegas carries a higher operational burden. You are responsible for the uptime, scaling, and maintenance of the system. Splunk, as a SaaS, handles this for you.</li> <li>Control &amp; Transparency: With Micromegas, you have full control over the infrastructure and complete transparency into the cost of every component. You can fine-tune instance types and storage classes to optimize costs. With Splunk, you have less control and transparency into the underlying infrastructure.</li> <li>Data Ownership &amp; Security: The Micromegas model means all telemetry data remains within your own cloud environment, which can be a major advantage for security and data governance.</li> <li>Scalability: Both solutions are designed to scale. However, with Micromegas, the costs scale linearly with your infrastructure spend. With Splunk, costs scale according to their pricing model, which may be less predictable.</li> </ul>"},{"location":"cost-comparisons/splunk/#references","title":"References","text":"<ol> <li>Splunk Pricing - Splunk</li> <li>Guide to Splunk Pricing and Costs in 2025 - Uptrace</li> <li>Datadog API Reference - Metric Submission</li> </ol>"},{"location":"development/build/","title":"Build Guide","text":"<p>This guide covers building Micromegas from source and setting up a development environment.</p>"},{"location":"development/build/#prerequisites","title":"Prerequisites","text":"<ul> <li>Rust - Latest stable version</li> <li>Python 3.8+</li> <li>Docker - For running PostgreSQL</li> <li>Git</li> </ul>"},{"location":"development/build/#building-from-source","title":"Building from Source","text":""},{"location":"development/build/#1-clone-repository","title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/madesroches/micromegas.git\ncd micromegas\n</code></pre>"},{"location":"development/build/#2-build-rust-components","title":"2. Build Rust Components","text":"<pre><code>cd rust\n\n# Build all components\ncargo build\n\n# Build with optimizations\ncargo build --release\n\n# Build specific component\ncargo build -p telemetry-ingestion-srv\n</code></pre>"},{"location":"development/build/#3-run-tests","title":"3. Run Tests","text":"<pre><code># Run all tests\ncargo test\n\n# Run tests with output\ncargo test -- --nocapture\n\n# Run specific test\ncargo test -p micromegas-tracing\n</code></pre>"},{"location":"development/build/#4-format-and-lint","title":"4. Format and Lint","text":"<pre><code># Format code (required before commits)\ncargo fmt\n\n# Run linter\ncargo clippy --workspace -- -D warnings\n\n# Run full CI pipeline\npython3 ../build/rust_ci.py\n</code></pre>"},{"location":"development/build/#python-client-development","title":"Python Client Development","text":""},{"location":"development/build/#1-set-up-environment","title":"1. Set Up Environment","text":"<pre><code>cd python/micromegas\n\n# Install Poetry (if not already installed)\ncurl -sSL https://install.python-poetry.org | python3 -\n\n# Install dependencies\npoetry install\n\n# Activate virtual environment\npoetry shell\n</code></pre>"},{"location":"development/build/#2-run-tests","title":"2. Run Tests","text":"<pre><code># Run tests\npytest\n\n# Run with coverage\npytest --cov=micromegas\n\n# Run specific test file\npytest tests/test_client.py\n</code></pre>"},{"location":"development/build/#3-format-code","title":"3. Format Code","text":"<pre><code># Format with black (required before commits)\nblack .\n\n# Check formatting\nblack --check .\n</code></pre>"},{"location":"development/build/#documentation-development","title":"Documentation Development","text":""},{"location":"development/build/#1-install-dependencies","title":"1. Install Dependencies","text":"<pre><code># Install MkDocs and theme\npip install -r docs/docs-requirements.txt\n\n# Or use the build script\npython docs/build-docs.py\n</code></pre>"},{"location":"development/build/#2-development-server","title":"2. Development Server","text":"<pre><code># Start development server\nmkdocs serve\n\n# Visit http://localhost:8000\n# Changes automatically reload\n</code></pre>"},{"location":"development/build/#3-build-static-site","title":"3. Build Static Site","text":"<pre><code># Build documentation\nmkdocs build\n\n# Output in site/ directory\npython -m http.server 8000 --directory site\n</code></pre>"},{"location":"development/build/#ide-setup","title":"IDE Setup","text":""},{"location":"development/build/#vs-code","title":"VS Code","text":"<p>Recommended extensions: - rust-analyzer - Rust language support - Python - Python language support - Even Better TOML - TOML file support - Error Lens - Inline error display</p>"},{"location":"development/build/#settings","title":"Settings","text":"<p>Add to <code>.vscode/settings.json</code>: <pre><code>{\n    \"rust-analyzer.cargo.features\": \"all\",\n    \"python.defaultInterpreterPath\": \"./python/micromegas/.venv/bin/python\",\n    \"python.formatting.provider\": \"black\"\n}\n</code></pre></p>"},{"location":"development/build/#common-build-tasks","title":"Common Build Tasks","text":""},{"location":"development/build/#full-clean-build","title":"Full Clean Build","text":"<pre><code># Clean Rust build artifacts\ncd rust\ncargo clean\n\n# Clean Python artifacts\ncd ../python/micromegas\npoetry env remove --all\npoetry install\n</code></pre>"},{"location":"development/build/#release-build","title":"Release Build","text":"<pre><code>cd rust\n\n# Build optimized release\ncargo build --release\n\n# Run optimized tests\ncargo test --release\n</code></pre>"},{"location":"development/build/#cross-platform-build","title":"Cross-Platform Build","text":"<pre><code># Add target\nrustup target add x86_64-pc-windows-gnu\n\n# Build for target\ncargo build --target x86_64-pc-windows-gnu\n</code></pre>"},{"location":"development/build/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/build/#common-issues","title":"Common Issues","text":"<p>Rust compilation errors: - Ensure you have the latest stable Rust: <code>rustup update</code> - Clear build cache: <code>cargo clean</code></p> <p>Python dependency conflicts: - Remove and recreate environment: <code>poetry env remove --all &amp;&amp; poetry install</code></p> <p>Database connection issues: - Ensure PostgreSQL container is running - Check environment variables are set correctly</p> <p>Permission errors on Windows: - Run PowerShell as Administrator - Use Windows Subsystem for Linux (WSL)</p>"},{"location":"development/build/#build-environment","title":"Build Environment","text":"<p>Check your build environment:</p> <pre><code># Rust version\nrustc --version\n\n# Cargo version  \ncargo --version\n\n# Python version\npython --version\n\n# Docker version\ndocker --version\n</code></pre>"},{"location":"development/build/#performance-builds","title":"Performance Builds","text":""},{"location":"development/build/#optimized-release","title":"Optimized Release","text":"<pre><code>cd rust\n\n# Maximum optimization\ncargo build --release\n\n# With debug symbols for profiling\ncargo build --profile release-debug\n</code></pre>"},{"location":"development/build/#profiling-build","title":"Profiling Build","text":"<pre><code># Build with profiling\ncargo build --profile profiling\n\n# Enable specific features\ncargo build --features \"profiling,metrics\"\n</code></pre>"},{"location":"development/build/#continuous-integration","title":"Continuous Integration","text":"<p>The CI pipeline runs:</p> <ol> <li>Format check: <code>cargo fmt --check</code></li> <li>Linting: <code>cargo clippy --workspace -- -D warnings</code> </li> <li>Tests: <code>cargo test --workspace</code></li> <li>Documentation: <code>cargo doc --workspace</code></li> </ol> <p>Run locally with: <pre><code>cd rust\npython3 ../build/rust_ci.py\n</code></pre></p>"},{"location":"development/build/#next-steps","title":"Next Steps","text":"<ul> <li>Contributing Guide - How to contribute to the project</li> <li>Getting Started - Set up a development instance</li> <li>Architecture Overview - Understand the system design</li> </ul>"},{"location":"query-guide/","title":"Query Guide Overview","text":"<p>Micromegas provides a powerful SQL interface for querying observability data including logs, metrics, spans, and traces. Micromegas SQL is an extension of Apache DataFusion SQL - you can use all standard DataFusion SQL features plus Micromegas-specific functions and views optimized for observability workloads.</p>"},{"location":"query-guide/#key-concepts","title":"Key Concepts","text":""},{"location":"query-guide/#sql-engine","title":"SQL Engine","text":"<p>Micromegas uses Apache DataFusion as its SQL engine, which means you get:</p> <ul> <li>Full SQL standard compliance</li> <li>Advanced query optimization</li> <li>Vectorized execution engine</li> <li>Columnar data processing with Apache Arrow</li> </ul>"},{"location":"query-guide/#data-architecture","title":"Data Architecture","text":"<ul> <li>Raw data stored in object storage (S3/GCS) in Parquet format</li> <li>Metadata stored in PostgreSQL for fast lookups</li> <li>Views provide logical organization of telemetry data</li> <li>On-demand ETL processes data only when queried</li> </ul>"},{"location":"query-guide/#available-interfaces","title":"Available Interfaces","text":""},{"location":"query-guide/#python-api","title":"Python API","text":"<p>The primary interface for querying Micromegas data programmatically. All queries return pandas DataFrames, making it easy to work with results using the pandas ecosystem:</p> <pre><code>import micromegas\nclient = micromegas.connect()\ndf = client.query(\"SELECT * FROM log_entries LIMIT 10;\")\n</code></pre>"},{"location":"query-guide/#grafana-plugin","title":"Grafana Plugin","text":"<p>Use the same SQL capabilities in Grafana dashboards through the Micromegas Grafana plugin.</p>"},{"location":"query-guide/#data-views","title":"Data Views","text":"<p>Micromegas organizes telemetry data into several queryable views:</p> View Description <code>processes</code> Process metadata and system information <code>streams</code> Data stream information within processes <code>log_entries</code> Application log messages with levels and context <code>measures</code> Numeric metrics and performance measurements <code>thread_spans</code> Synchronous execution spans and timing <code>async_events</code> Asynchronous event lifecycle tracking"},{"location":"query-guide/#query-capabilities","title":"Query Capabilities","text":""},{"location":"query-guide/#standard-sql-features","title":"Standard SQL Features","text":"<ul> <li>SELECT, FROM, WHERE, ORDER BY, GROUP BY</li> <li>JOINs between views</li> <li>Aggregation functions (COUNT, SUM, AVG, etc.)</li> <li>Window functions and CTEs</li> <li>Complex filtering and sorting</li> </ul>"},{"location":"query-guide/#observability-extensions","title":"Observability Extensions","text":"<ul> <li>Time-range filtering for performance</li> <li>Process-scoped view instances</li> <li>Histogram generation functions</li> <li>Log level filtering and analysis</li> <li>Span relationship queries</li> </ul>"},{"location":"query-guide/#performance-features","title":"Performance Features","text":"<ul> <li>Query streaming for large datasets</li> <li>Predicate pushdown to storage layer</li> <li>Automatic view materialization</li> <li>Memory-efficient processing</li> </ul>"},{"location":"query-guide/#getting-started","title":"Getting Started","text":"<ol> <li>Quick Start - Basic queries to get you started</li> <li>Python API - Complete API reference and examples</li> <li>Schema Reference - Detailed view and field documentation</li> <li>Functions Reference - Available SQL functions</li> <li>Query Patterns - Common observability query patterns</li> <li>Async Performance Analysis - Comprehensive async operation analysis with depth tracking</li> <li>Performance Guide - Optimize your queries for best performance</li> <li>Advanced Features - View materialization and custom views</li> </ol>"},{"location":"query-guide/#best-practices","title":"Best Practices","text":""},{"location":"query-guide/#always-use-time-ranges","title":"Always Use Time Ranges","text":"<p>For performance and memory efficiency, always specify time ranges in your queries:</p> <pre><code># Good - uses time range\ndf = client.query(sql, begin_time, end_time)\n\n# Avoid - queries all data\ndf = client.query(sql)  # Can be slow and memory-intensive\n</code></pre>"},{"location":"query-guide/#start-simple","title":"Start Simple","text":"<p>Begin with basic queries and add complexity incrementally:</p> <pre><code>-- Start with this\nSELECT * FROM log_entries LIMIT 10;\n\n-- Then add filtering\nSELECT * FROM log_entries WHERE level &lt;= 3 LIMIT 10;\n\n-- Then add time range\nSELECT * FROM log_entries\nWHERE level &lt;= 3 AND time &gt;= NOW() - INTERVAL '1 hour'\nLIMIT 10;\n</code></pre>"},{"location":"query-guide/#use-process-scoped-views","title":"Use Process-Scoped Views","text":"<p>For better performance when analyzing specific processes:</p> <pre><code>-- Instead of filtering the global view\nSELECT * FROM log_entries WHERE process_id = 'my_process';\n\n-- Use a process-scoped view instance\nSELECT * FROM view_instance('log_entries', 'my_process');\n</code></pre> <p>Ready to start querying? Head to the Quick Start guide!</p>"},{"location":"query-guide/advanced-features/","title":"Advanced Features","text":"<p>Advanced Micromegas features including view materialization, custom views, and system administration.</p>"},{"location":"query-guide/advanced-features/#view-materialization","title":"View Materialization","text":"<p>Micromegas uses a lakehouse architecture with on-demand view materialization for optimal performance.</p>"},{"location":"query-guide/advanced-features/#jit-view-processing","title":"JIT View Processing","text":"<ul> <li>Raw data stored in object storage (S3/GCS)</li> <li>Views materialized on-demand when queried</li> <li>Automatic caching for frequently accessed data</li> </ul>"},{"location":"query-guide/advanced-features/#global-views-vs-view-instances","title":"Global Views vs View Instances","text":"<p>Micromegas provides two ways to access telemetry data:</p>"},{"location":"query-guide/advanced-features/#global-views-implicit","title":"Global Views (Implicit)","text":"<p>When you query views directly by name, you're using global views that span all processes:</p> <pre><code>-- Global view - queries data from ALL processes\nSELECT * FROM log_entries WHERE level &lt;= 2;\nSELECT * FROM measures WHERE name = 'cpu_usage';\n</code></pre> <p>Global views are convenient for: - Exploring data across the entire system - Cross-process analysis and correlation - Getting started without knowing specific process IDs</p>"},{"location":"query-guide/advanced-features/#view-instances-explicit","title":"View Instances (Explicit)","text":"<p>Use the <code>view_instance()</code> function to create process-scoped views for better performance:</p> <pre><code>-- View instance - queries data from ONE specific process\nSELECT * FROM view_instance('log_entries', 'my_process_123') WHERE level &lt;= 2;\nSELECT * FROM view_instance('measures', 'my_process_123') WHERE name = 'cpu_usage';\n</code></pre> <p>View instances are optimal for: - Analyzing specific processes or streams - Better query performance (fewer partitions to scan) - Production systems with large amounts of data</p> <p>Performance Impact: - Global views: May scan many partitions across all processes - View instances: Only scan partitions for the specified process/stream</p>"},{"location":"query-guide/advanced-features/#architecture-benefits","title":"Architecture Benefits","text":""},{"location":"query-guide/advanced-features/#datalake-lakehouse-query","title":"Datalake \u2192 Lakehouse \u2192 Query","text":"<ul> <li>Datalake (S3): Custom binary format, cheap storage, fast writes</li> <li>Lakehouse (Parquet): Columnar format, fast analytics, industry standard</li> <li>Query Engine (DataFusion): SQL engine optimized for analytical workloads</li> </ul>"},{"location":"query-guide/advanced-features/#tail-sampling-support","title":"Tail Sampling Support","text":"<ul> <li>Heavy data streams remain unprocessed until queried</li> <li>Cheap to store in S3, cheap to delete unused data</li> <li>Use low-frequency streams (logs, metrics) to decide sampling of high-frequency streams (spans)</li> </ul>"},{"location":"query-guide/async-performance-analysis/","title":"Async Performance Analysis Guide","text":"<p>This guide provides comprehensive patterns and examples for analyzing asynchronous operation performance using the <code>async_events</code> view with depth tracking.</p>"},{"location":"query-guide/async-performance-analysis/#understanding-async-event-depth","title":"Understanding Async Event Depth","text":"<p>The <code>depth</code> field in <code>async_events</code> represents the nesting level in the async call hierarchy:</p> <ul> <li>Depth 0: Top-level async operations (entry points)</li> <li>Depth 1: First-level nested async operations</li> <li>Depth 2+: Deeper nested async operations</li> </ul> <p>This enables hierarchical performance analysis similar to synchronous call stack profiling.</p>"},{"location":"query-guide/async-performance-analysis/#core-analysis-patterns","title":"Core Analysis Patterns","text":""},{"location":"query-guide/async-performance-analysis/#1-top-level-performance-overview","title":"1. Top-Level Performance Overview","text":"<p>Start with top-level operations (depth = 0) to identify primary performance bottlenecks:</p> <pre><code>-- Top-level async operations with performance metrics\nSELECT\n    name,\n    COUNT(*) as operation_count,\n    AVG(duration_ms) as avg_duration,\n    MIN(duration_ms) as min_duration,\n    MAX(duration_ms) as max_duration,\n    STDDEV(duration_ms) as duration_stddev\nFROM (\n    SELECT\n        begin_events.name,\n        CAST((end_events.time - begin_events.time) AS BIGINT) / 1000000 as duration_ms\n    FROM\n        (SELECT * FROM view_instance('async_events', 'process_id')\n         WHERE event_type = 'begin' AND depth = 0) begin_events\n    LEFT JOIN\n        (SELECT * FROM view_instance('async_events', 'process_id')\n         WHERE event_type = 'end') end_events\n        ON begin_events.span_id = end_events.span_id\n    WHERE end_events.span_id IS NOT NULL\n)\nGROUP BY name\nORDER BY avg_duration DESC;\n</code></pre>"},{"location":"query-guide/async-performance-analysis/#2-depth-based-performance-comparison","title":"2. Depth-Based Performance Comparison","text":"<p>Compare performance characteristics across different call depths:</p> <pre><code>-- Performance metrics by async call depth\nSELECT\n    depth,\n    COUNT(*) as span_count,\n    AVG(duration_ms) as avg_duration,\n    PERCENTILE(duration_ms, 0.5) as median_duration,\n    PERCENTILE(duration_ms, 0.95) as p95_duration,\n    PERCENTILE(duration_ms, 0.99) as p99_duration\nFROM (\n    SELECT\n        begin_events.depth,\n        CAST((end_events.time - begin_events.time) AS BIGINT) / 1000000 as duration_ms\n    FROM\n        (SELECT * FROM view_instance('async_events', 'process_id') WHERE event_type = 'begin') begin_events\n    LEFT JOIN\n        (SELECT * FROM view_instance('async_events', 'process_id') WHERE event_type = 'end') end_events\n        ON begin_events.span_id = end_events.span_id\n    WHERE end_events.span_id IS NOT NULL\n)\nGROUP BY depth\nORDER BY depth;\n</code></pre>"},{"location":"query-guide/async-performance-analysis/#3-parent-child-performance-analysis","title":"3. Parent-Child Performance Analysis","text":"<p>Analyze how async operations delegate work to nested operations:</p> <pre><code>-- Parent-child async operation performance relationships\nSELECT\n    parent.name as parent_operation,\n    parent.depth as parent_depth,\n    child.name as child_operation,\n    child.depth as child_depth,\n    COUNT(*) as relationship_count,\n    AVG(parent_duration_ms) as avg_parent_duration,\n    AVG(child_duration_ms) as avg_child_duration,\n    AVG(parent_duration_ms) - AVG(child_duration_ms) as avg_overhead_ms\nFROM (\n    SELECT\n        p.name, p.depth, p.span_id,\n        c.name as child_name, c.depth as child_depth, c.span_id as child_span_id,\n        CAST((p_end.time - p_begin.time) AS BIGINT) / 1000000 as parent_duration_ms,\n        CAST((c_end.time - c_begin.time) AS BIGINT) / 1000000 as child_duration_ms\n    FROM view_instance('async_events', 'process_id') p\n    JOIN view_instance('async_events', 'process_id') c ON p.span_id = c.parent_span_id\n    JOIN view_instance('async_events', 'process_id') p_begin ON p.span_id = p_begin.span_id AND p_begin.event_type = 'begin'\n    JOIN view_instance('async_events', 'process_id') p_end ON p.span_id = p_end.span_id AND p_end.event_type = 'end'\n    JOIN view_instance('async_events', 'process_id') c_begin ON c.span_id = c_begin.span_id AND c_begin.event_type = 'begin'\n    JOIN view_instance('async_events', 'process_id') c_end ON c.span_id = c_end.span_id AND c_end.event_type = 'end'\n    WHERE p.event_type = 'begin' AND c.event_type = 'begin'\n) as relationships(name, depth, span_id, child_name, child_depth, child_span_id, parent_duration_ms, child_duration_ms)\nGROUP BY parent_operation, parent_depth, child_operation, child_depth\nHAVING COUNT(*) &gt; 5  -- Focus on significant relationships\nORDER BY relationship_count DESC, avg_overhead_ms DESC;\n</code></pre>"},{"location":"query-guide/async-performance-analysis/#advanced-analysis-techniques","title":"Advanced Analysis Techniques","text":""},{"location":"query-guide/async-performance-analysis/#4-async-concurrency-analysis","title":"4. Async Concurrency Analysis","text":"<p>Identify periods of high async concurrency:</p> <pre><code>-- Concurrent async operations over time\nSELECT\n    time_bucket,\n    MAX(concurrent_operations) as peak_concurrency,\n    AVG(concurrent_operations) as avg_concurrency\nFROM (\n    SELECT\n        date_trunc('minute', time) as time_bucket,\n        COUNT(*) as concurrent_operations\n    FROM view_instance('async_events', 'process_id')\n    WHERE event_type = 'begin'\n    GROUP BY date_trunc('minute', time)\n)\nGROUP BY time_bucket\nORDER BY time_bucket;\n</code></pre>"},{"location":"query-guide/async-performance-analysis/#5-deep-nesting-detection","title":"5. Deep Nesting Detection","text":"<p>Find problematic deep async call chains:</p> <pre><code>-- Operations with excessive async nesting depth\nSELECT\n    name,\n    depth,\n    COUNT(*) as occurrence_count,\n    AVG(duration_ms) as avg_duration\nFROM (\n    SELECT\n        begin_events.name,\n        begin_events.depth,\n        CAST((end_events.time - begin_events.time) AS BIGINT) / 1000000 as duration_ms\n    FROM\n        (SELECT * FROM view_instance('async_events', 'process_id') WHERE event_type = 'begin') begin_events\n    LEFT JOIN\n        (SELECT * FROM view_instance('async_events', 'process_id') WHERE event_type = 'end') end_events\n        ON begin_events.span_id = end_events.span_id\n    WHERE end_events.span_id IS NOT NULL\n)\nWHERE depth &gt;= 3  -- Focus on deep nesting\nGROUP BY name, depth\nORDER BY depth DESC, occurrence_count DESC;\n</code></pre>"},{"location":"query-guide/async-performance-analysis/#6-async-operation-hotspots","title":"6. Async Operation Hotspots","text":"<p>Identify the most frequently called async operations by depth:</p> <pre><code>-- Async operation frequency by depth level\nSELECT\n    depth,\n    name,\n    COUNT(*) as call_count,\n    AVG(duration_ms) as avg_duration,\n    COUNT(*) * AVG(duration_ms) as total_time_spent\nFROM (\n    SELECT\n        begin_events.name,\n        begin_events.depth,\n        CAST((end_events.time - begin_events.time) AS BIGINT) / 1000000 as duration_ms\n    FROM\n        (SELECT * FROM view_instance('async_events', 'process_id') WHERE event_type = 'begin') begin_events\n    LEFT JOIN\n        (SELECT * FROM view_instance('async_events', 'process_id') WHERE event_type = 'end') end_events\n        ON begin_events.span_id = end_events.span_id\n    WHERE end_events.span_id IS NOT NULL\n)\nGROUP BY depth, name\nORDER BY total_time_spent DESC;\n</code></pre>"},{"location":"query-guide/async-performance-analysis/#performance-optimization-strategies","title":"Performance Optimization Strategies","text":""},{"location":"query-guide/async-performance-analysis/#focus-areas-based-on-depth-analysis","title":"Focus Areas Based on Depth Analysis","text":"<ol> <li>Depth 0 Optimization: Target top-level operations for maximum impact</li> <li>High-Frequency Operations: Optimize operations with high call counts</li> <li>Deep Nesting Reduction: Flatten async call hierarchies where possible</li> <li>Concurrency Tuning: Balance async concurrency with resource usage</li> </ol>"},{"location":"query-guide/async-performance-analysis/#query-performance-tips","title":"Query Performance Tips","text":"<ol> <li>Always use time ranges through Python API parameters</li> <li>Filter by depth early to reduce data processing</li> <li>Use process-scoped views (<code>view_instance</code>) for efficiency</li> <li>Combine with other views (logs, measures) for context</li> </ol>"},{"location":"query-guide/async-performance-analysis/#example-comprehensive-async-performance-dashboard","title":"Example: Comprehensive Async Performance Dashboard","text":"<pre><code>-- Multi-dimensional async performance summary\nWITH async_durations AS (\n    SELECT\n        begin_events.name,\n        begin_events.depth,\n        begin_events.time as start_time,\n        CAST((end_events.time - begin_events.time) AS BIGINT) / 1000000 as duration_ms\n    FROM\n        (SELECT * FROM view_instance('async_events', 'process_id') WHERE event_type = 'begin') begin_events\n    LEFT JOIN\n        (SELECT * FROM view_instance('async_events', 'process_id') WHERE event_type = 'end') end_events\n        ON begin_events.span_id = end_events.span_id\n    WHERE end_events.span_id IS NOT NULL\n),\ndepth_summary AS (\n    SELECT\n        depth,\n        COUNT(*) as operation_count,\n        AVG(duration_ms) as avg_duration,\n        PERCENTILE(duration_ms, 0.95) as p95_duration\n    FROM async_durations\n    GROUP BY depth\n),\ntop_operations AS (\n    SELECT\n        name,\n        COUNT(*) as call_count,\n        AVG(duration_ms) as avg_duration\n    FROM async_durations\n    WHERE depth = 0  -- Top-level only\n    GROUP BY name\n    ORDER BY avg_duration DESC\n    LIMIT 5\n)\nSELECT\n    'Depth Summary' as analysis_type,\n    CAST(depth AS VARCHAR) as name,\n    operation_count as count,\n    avg_duration,\n    p95_duration as p95\nFROM depth_summary\nUNION ALL\nSELECT\n    'Top Operations' as analysis_type,\n    name,\n    call_count as count,\n    avg_duration,\n    NULL as p95\nFROM top_operations\nORDER BY analysis_type, avg_duration DESC;\n</code></pre> <p>This comprehensive approach enables effective async performance analysis and optimization based on call hierarchy depth information.</p>"},{"location":"query-guide/functions-reference/","title":"Functions Reference","text":"<p>This page provides a complete reference to all SQL functions available in Micromegas queries, including both standard DataFusion functions and Micromegas-specific extensions.</p>"},{"location":"query-guide/functions-reference/#micromegas-extensions","title":"Micromegas Extensions","text":""},{"location":"query-guide/functions-reference/#table-functions","title":"Table Functions","text":"<p>Table functions return tables that can be used in FROM clauses.</p>"},{"location":"query-guide/functions-reference/#view_instanceview_name-identifier","title":"<code>view_instance(view_name, identifier)</code>","text":"<p>Creates a process or stream-scoped view instance for better performance.</p> <p>Syntax: <pre><code>view_instance(view_name, identifier)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>view_name</code> (<code>Utf8</code>): Name of the view ('log_entries', 'measures', 'thread_spans', 'async_events')</p> </li> <li> <p><code>identifier</code> (<code>Utf8</code>): Process ID (for most views) or Stream ID (for thread_spans)</p> </li> </ul> <p>Returns: Schema depends on the view type (see Schema Reference)</p> <p>Examples: <pre><code>-- Get logs for a specific process\nSELECT time, level, msg\nFROM view_instance('log_entries', 'my_process_123')\nWHERE level &lt;= 3;\n\n-- Get spans for a specific stream\nSELECT name, duration\nFROM view_instance('thread_spans', 'stream_456')\nWHERE duration &gt; 1000000;  -- &gt; 1ms\n</code></pre></p>"},{"location":"query-guide/functions-reference/#list_partitions","title":"<code>list_partitions()</code> \ud83d\udd27","text":"<p>Administrative Function - Lists available data partitions in the lakehouse.</p> <p>Syntax: <pre><code>SELECT * FROM list_partitions()\n</code></pre></p> <p>Returns:</p> Column Type Description view_set_name Utf8 Name of the view set view_instance_id Utf8 Instance identifier begin_insert_time Timestamp(Nanosecond) Partition start time end_insert_time Timestamp(Nanosecond) Partition end time min_event_time Timestamp(Nanosecond) Earliest event time max_event_time Timestamp(Nanosecond) Latest event time updated Timestamp(Nanosecond) Last update time file_path Utf8 Partition file path file_size Int64 File size in bytes file_schema_hash Binary Hash of the file schema source_data_hash Binary Hash of the source data <p>Example: <pre><code>-- View partition information\nSELECT view_set_name, view_instance_id, file_size\nFROM list_partitions()\nORDER BY updated DESC;\n</code></pre></p> <p>\u2139\ufe0f Administrative Use: This function provides system-level partition metadata primarily useful for administrators monitoring lakehouse storage and partition management. Regular users querying data typically don't need this information.</p>"},{"location":"query-guide/functions-reference/#retire_partitionsview_set_name-view_instance_id-begin_insert_time-end_insert_time","title":"<code>retire_partitions(view_set_name, view_instance_id, begin_insert_time, end_insert_time)</code> \ud83d\udd27","text":"<p>Administrative Function - Retires (removes) data partitions from the lakehouse for a specified time range. Returns a log stream of the operation.</p> <p>Syntax: <pre><code>SELECT * FROM retire_partitions(view_set_name, view_instance_id, begin_insert_time, end_insert_time)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>view_set_name</code> (<code>Utf8</code>): Name of the view set</p> </li> <li> <p><code>view_instance_id</code> (<code>Utf8</code>): Instance identifier</p> </li> <li> <p><code>begin_insert_time</code> (<code>Timestamp(Nanosecond)</code>): Start time for partition retirement</p> </li> <li> <p><code>end_insert_time</code> (<code>Timestamp(Nanosecond)</code>): End time for partition retirement</p> </li> </ul> <p>Returns: Log stream table with operation progress and messages</p> <p>Example: <pre><code>-- Retire old partitions for a specific view\nSELECT * FROM retire_partitions(\n    'log_entries', \n    'global',\n    NOW() - INTERVAL '30 days',\n    NOW() - INTERVAL '7 days'\n);\n</code></pre></p> <p>\u26a0\ufe0f DESTRUCTIVE OPERATION: This function permanently removes data partitions from the lakehouse, making the contained data inaccessible. Use only for data retention management and with extreme caution in production environments. Ensure proper backups exist before retiring partitions.</p>"},{"location":"query-guide/functions-reference/#materialize_partitionsview_name-begin_insert_time-end_insert_time-partition_delta_seconds","title":"<code>materialize_partitions(view_name, begin_insert_time, end_insert_time, partition_delta_seconds)</code> \ud83d\udd27","text":"<p>Administrative Function - Materializes data partitions for a view over a specified time range. Returns a log stream of the operation.</p> <p>Syntax: <pre><code>SELECT * FROM materialize_partitions(view_name, begin_insert_time, end_insert_time, partition_delta_seconds)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>view_name</code> (<code>Utf8</code>): Name of the view to materialize</p> </li> <li> <p><code>begin_insert_time</code> (<code>Timestamp(Nanosecond)</code>): Start time for materialization</p> </li> <li> <p><code>end_insert_time</code> (<code>Timestamp(Nanosecond)</code>): End time for materialization</p> </li> <li> <p><code>partition_delta_seconds</code> (<code>Int64</code>): Partition time delta in seconds</p> </li> </ul> <p>Returns: Log stream table with operation progress and messages</p> <p>Example: <pre><code>-- Materialize partitions for CPU usage view\nSELECT * FROM materialize_partitions(\n    'cpu_usage_per_process_per_minute',\n    NOW() - INTERVAL '1 day',\n    NOW(),\n    3600  -- 1 hour partitions\n);\n</code></pre></p> <p>\u26a0\ufe0f Administrative Use Only: This function is intended for system administrators and data engineers managing the lakehouse infrastructure. Regular users querying data should not need to call this function. It triggers background processing to create materialized partitions and can impact system performance.</p>"},{"location":"query-guide/functions-reference/#list_view_sets","title":"<code>list_view_sets()</code> \ud83d\udd27","text":"<p>Administrative Function - Lists all available view sets with their current schema information. Useful for schema discovery and management.</p> <p>Syntax: <pre><code>SELECT * FROM list_view_sets()\n</code></pre></p> <p>Returns:</p> Column Type Description view_set_name Utf8 Name of the view set (e.g., 'log_entries', 'measures') current_schema_hash Binary Current schema version identifier schema Utf8 Full schema as formatted string has_view_maker Boolean Whether view set supports process-specific instances global_instance_available Boolean Whether a global instance exists <p>Example: <pre><code>-- View all available view sets and their schemas\nSELECT view_set_name, current_schema_hash, has_view_maker\nFROM list_view_sets()\nORDER BY view_set_name;\n\n-- Check schema for specific view set\nSELECT schema\nFROM list_view_sets()\nWHERE view_set_name = 'log_entries';\n</code></pre></p> <p>\u2139\ufe0f Administrative Use: This function provides schema discovery for administrators managing view compatibility and schema evolution. It shows the current schema versions and capabilities of each view set in the lakehouse.</p>"},{"location":"query-guide/functions-reference/#retire_partition_by_filefile_path","title":"<code>retire_partition_by_file(file_path)</code> \ud83d\udd27","text":"<p>Administrative Function - Retires a single partition by its exact file path. Provides targeted partition removal for schema evolution and maintenance.</p> <p>Syntax: <pre><code>SELECT retire_partition_by_file(file_path) as result\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>file_path</code> (<code>Utf8</code>): Exact file path of the partition to retire</li> </ul> <p>Returns: <code>Utf8</code> - Result message indicating success or failure</p> <p>Example: <pre><code>-- Retire a specific partition\nSELECT retire_partition_by_file('/lakehouse/log_entries/process-123/2024/01/01/partition.parquet') as result;\n\n-- Retire multiple partitions (use with list_partitions())\nSELECT retire_partition_by_file(file_path) as result\nFROM list_partitions()\nWHERE view_set_name = 'log_entries' \n  AND file_schema_hash != '[4]'  -- Retire old schema versions\nLIMIT 10;\n</code></pre></p> <p>\u26a0\ufe0f DESTRUCTIVE OPERATION: This function permanently removes a single data partition from the lakehouse, making the contained data inaccessible. Unlike <code>retire_partitions()</code> which operates on time ranges, this function targets exact file paths for precise partition management. Ensure proper backups exist before retiring partitions.</p> <p>\u2705 Safety Note: This function only affects the specified partition file. It cannot accidentally retire other partitions, making it safer than time-range-based retirement for schema evolution tasks.</p>"},{"location":"query-guide/functions-reference/#scalar-functions","title":"Scalar Functions","text":""},{"location":"query-guide/functions-reference/#jsonjsonb-functions","title":"JSON/JSONB Functions","text":"<p>Micromegas provides functions for working with JSON data stored in binary JSONB format for efficient storage and querying.</p>"},{"location":"query-guide/functions-reference/#jsonb_parsejson_string","title":"<code>jsonb_parse(json_string)</code>","text":"<p>Parses a JSON string into binary JSONB format.</p> <p>Syntax: <pre><code>jsonb_parse(json_string)\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>json_string</code> (<code>Utf8</code>): JSON string to parse</li> </ul> <p>Returns: <code>Binary</code> - Parsed JSONB data</p> <p>Example: <pre><code>-- Parse JSON string into JSONB\nSELECT jsonb_parse('{\"name\": \"web_server\", \"port\": 8080}') as parsed_json\nFROM processes;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#jsonb_getjsonb-key","title":"<code>jsonb_get(jsonb, key)</code>","text":"<p>Extracts a value from a JSONB object by key name.</p> <p>Syntax: <pre><code>jsonb_get(jsonb, key)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>jsonb</code> (<code>Binary</code>): JSONB object</p> </li> <li> <p><code>key</code> (<code>Utf8</code>): Key name to extract</p> </li> </ul> <p>Returns: <code>Binary</code> - JSONB value or NULL if key not found</p> <p>Example: <pre><code>-- Extract name field from JSON data\nSELECT jsonb_get(jsonb_parse('{\"name\": \"web_server\", \"port\": 8080}'), 'name') as name_value\nFROM processes;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#jsonb_format_jsonjsonb","title":"<code>jsonb_format_json(jsonb)</code>","text":"<p>Converts a JSONB value back to a human-readable JSON string.</p> <p>Syntax: <pre><code>jsonb_format_json(jsonb)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>jsonb</code> (Multiple formats supported): JSONB value in any of these formats:</p> </li> <li> <p><code>Dictionary&lt;Int32, Binary&gt;</code> - Dictionary-encoded JSONB (default)</p> </li> <li><code>Binary</code> - Non-dictionary JSONB</li> </ul> <p>Returns: <code>Utf8</code> - JSON string representation</p> <p>Examples: <pre><code>-- Format JSONB back to JSON string\nSELECT jsonb_format_json(jsonb_parse('{\"name\": \"web_server\"}')) as json_string\nFROM processes;\n\n-- Works directly with dictionary-encoded properties\nSELECT jsonb_format_json(properties_to_jsonb(properties)) as json_props\nFROM log_entries;\n\n-- Format property values as JSON\nSELECT jsonb_format_json(properties) as json_string\nFROM processes\nWHERE properties IS NOT NULL;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#jsonb_as_stringjsonb","title":"<code>jsonb_as_string(jsonb)</code>","text":"<p>Casts a JSONB value to a string.</p> <p>Syntax: <pre><code>jsonb_as_string(jsonb)\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>jsonb</code> (<code>Binary</code>): JSONB value to convert</li> </ul> <p>Returns: <code>Utf8</code> - String value or NULL if not a string</p> <p>Example: <pre><code>-- Extract string value from JSONB\nSELECT jsonb_as_string(jsonb_get(jsonb_parse('{\"service\": \"web_server\"}'), 'service')) as service_name\nFROM processes;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#jsonb_as_f64jsonb","title":"<code>jsonb_as_f64(jsonb)</code>","text":"<p>Casts a JSONB value to a 64-bit float.</p> <p>Syntax: <pre><code>jsonb_as_f64(jsonb)\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>jsonb</code> (<code>Binary</code>): JSONB value to convert</li> </ul> <p>Returns: <code>Float64</code> - Numeric value or NULL if not a number</p> <p>Example: <pre><code>-- Extract numeric value from JSONB\nSELECT jsonb_as_f64(jsonb_get(jsonb_parse('{\"cpu_usage\": 75.5}'), 'cpu_usage')) as cpu_usage\nFROM processes;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#jsonb_as_i64jsonb","title":"<code>jsonb_as_i64(jsonb)</code>","text":"<p>Casts a JSONB value to a 64-bit integer.</p> <p>Syntax: <pre><code>jsonb_as_i64(jsonb)\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>jsonb</code> (<code>Binary</code>): JSONB value to convert</li> </ul> <p>Returns: <code>Int64</code> - Integer value or NULL if not an integer</p> <p>Example: <pre><code>-- Extract integer value from JSONB\nSELECT jsonb_as_i64(jsonb_get(jsonb_parse('{\"port\": 8080}'), 'port')) as port_number\nFROM processes;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#data-access-functions","title":"Data Access Functions","text":""},{"location":"query-guide/functions-reference/#get_payloadprocess_id-stream_id-block_id","title":"<code>get_payload(process_id, stream_id, block_id)</code>","text":"<p>Retrieves the raw binary payload of a telemetry block from data lake storage.</p> <p>Syntax: <pre><code>get_payload(process_id, stream_id, block_id)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>process_id</code> (<code>Utf8</code>): Process identifier</p> </li> <li> <p><code>stream_id</code> (<code>Utf8</code>): Stream identifier</p> </li> <li> <p><code>block_id</code> (<code>Utf8</code>): Block identifier</p> </li> </ul> <p>Returns: <code>Binary</code> - Raw block payload data</p> <p>Example: <pre><code>-- Get raw payload data for specific blocks\nSELECT process_id, stream_id, block_id, get_payload(process_id, stream_id, block_id) as payload\nFROM blocks\nWHERE insert_time &gt;= NOW() - INTERVAL '1 hour'\nLIMIT 10;\n</code></pre></p> <p>Note: This is an async function that fetches data from object storage. Use sparingly in queries as it can impact performance.</p>"},{"location":"query-guide/functions-reference/#property-functions","title":"Property Functions","text":"<p>Micromegas provides specialized functions for working with property data, including efficient dictionary encoding for memory optimization.</p>"},{"location":"query-guide/functions-reference/#property_getproperties-key","title":"<code>property_get(properties, key)</code>","text":"<p>Extracts a value from a properties map with automatic format detection and optimized performance for JSONB data.</p> <p>Syntax: <pre><code>property_get(properties, key)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>properties</code> (Multiple formats supported): Properties data in any of these formats:</p> <ul> <li><code>Dictionary&lt;Int32, Binary&gt;</code> - JSONB format (default, optimized)</li> <li><code>List&lt;Struct&lt;key, value&gt;&gt;</code> - Legacy format (automatic conversion)</li> <li><code>Dictionary&lt;Int32, List&lt;Struct&gt;&gt;</code> - Dictionary-encoded legacy</li> <li><code>Binary</code> - Non-dictionary JSONB</li> </ul> </li> <li> <p><code>key</code> (<code>Utf8</code>): Property key to extract</p> </li> </ul> <p>Returns: <code>Dictionary&lt;Int32, Utf8&gt;</code> - Property value or NULL if not found</p> <p>Performance: Optimized for the new JSONB format. Legacy formats are automatically converted for backward compatibility.</p> <p>Examples: <pre><code>-- Get thread name from process properties (works with all formats)\nSELECT time, msg, property_get(process_properties, 'thread-name') as thread\nFROM log_entries\nWHERE property_get(process_properties, 'thread-name') IS NOT NULL;\n\n-- Filter by custom property\nSELECT time, name, value\nFROM measures\nWHERE property_get(properties, 'source') = 'system_monitor';\n\n-- Direct JSONB property access (post-migration default)\nSELECT time, msg, property_get(properties, 'service') as service\nFROM log_entries\nWHERE property_get(properties, 'env') = 'production';\n</code></pre></p>"},{"location":"query-guide/functions-reference/#properties_lengthproperties","title":"<code>properties_length(properties)</code>","text":"<p>Returns the number of properties in a properties map with support for multiple storage formats.</p> <p>Syntax: <pre><code>properties_length(properties)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>properties</code> (Multiple formats supported): Properties data in any of these formats:</p> <ul> <li><code>List&lt;Struct&lt;key, value&gt;&gt;</code> - Legacy format</li> <li><code>Dictionary&lt;Int32, Binary&gt;</code> - JSONB format (optimized)</li> <li><code>Dictionary&lt;Int32, List&lt;Struct&gt;&gt;</code> - Dictionary-encoded legacy</li> <li><code>Binary</code> - Non-dictionary JSONB</li> </ul> </li> </ul> <p>Returns: <code>Int32</code> - Number of properties</p> <p>Examples: <pre><code>-- Works with regular properties\nSELECT properties_length(properties) as prop_count\nFROM measures;\n\n-- Works with dictionary-encoded properties\nSELECT properties_length(properties_to_dict(properties)) as prop_count\nFROM measures;\n\n-- JSONB property counting\nSELECT properties_length(properties_to_jsonb(properties)) as prop_count\nFROM measures;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#properties_to_dictproperties","title":"<code>properties_to_dict(properties)</code>","text":"<p>Converts a properties list to a dictionary-encoded array for memory efficiency.</p> <p>Syntax: <pre><code>properties_to_dict(properties)\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>properties</code> (<code>List&lt;Struct&lt;key: Utf8, value: Utf8&gt;&gt;</code>): Properties list to encode</li> </ul> <p>Returns: <code>Dictionary&lt;Int32, List&lt;Struct&lt;key: Utf8, value: Utf8&gt;&gt;&gt;</code> - Dictionary-encoded properties</p> <p>Examples: <pre><code>-- Convert properties to dictionary encoding for memory efficiency\nSELECT properties_to_dict(properties) as dict_props\nFROM measures;\n\n-- Use with other functions via properties_to_array\nSELECT array_length(properties_to_array(properties_to_dict(properties))) as prop_count\nFROM measures;\n</code></pre></p> <p>Note: Dictionary encoding can reduce memory usage by 50-80% for datasets with repeated property patterns.</p>"},{"location":"query-guide/functions-reference/#properties_to_jsonbproperties","title":"<code>properties_to_jsonb(properties)</code>","text":"<p>Converts a properties list to binary JSONB format with dictionary encoding for efficient storage and querying.</p> <p>Syntax: <pre><code>properties_to_jsonb(properties)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>properties</code> (Multiple formats supported): Properties in any of these formats:</p> <ul> <li><code>List&lt;Struct&lt;key: Utf8, value: Utf8&gt;&gt;</code> - Regular properties list</li> <li><code>Dictionary&lt;Int32, List&lt;Struct&gt;&gt;</code> - Dictionary-encoded properties</li> <li><code>Binary</code> - Non-dictionary JSONB</li> <li><code>Dictionary&lt;Int32, Binary&gt;</code> - JSONB format</li> </ul> </li> </ul> <p>Returns: <code>Dictionary&lt;Int32, Binary&gt;</code> - Dictionary-encoded JSONB object containing the properties as key-value pairs</p> <p>Examples: <pre><code>-- Convert properties to JSONB format\nSELECT properties_to_jsonb(properties) as jsonb_props\nFROM log_entries;\n\n-- Use with other JSONB functions\nSELECT jsonb_get(properties_to_jsonb(properties), 'hostname') as hostname\nFROM log_entries;\n\n-- Convert dictionary-encoded properties to JSONB\nSELECT properties_to_jsonb(properties_to_dict(properties)) as jsonb_props\nFROM measures;\n</code></pre></p> <p>Note: This function returns <code>Dictionary&lt;Int32, Binary&gt;</code> format for optimal memory usage with Arrow's built-in dictionary encoding.</p>"},{"location":"query-guide/functions-reference/#properties_to_arraydict_properties","title":"<code>properties_to_array(dict_properties)</code>","text":"<p>Converts dictionary-encoded properties back to a regular array for compatibility with standard functions.</p> <p>Syntax: <pre><code>properties_to_array(dict_properties)\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>dict_properties</code> (<code>Dictionary&lt;Int32, List&lt;Struct&gt;&gt;</code>): Dictionary-encoded properties</li> </ul> <p>Returns: <code>List&lt;Struct&lt;key: Utf8, value: Utf8&gt;&gt;</code> - Regular properties array</p> <p>Examples: <pre><code>-- Convert dictionary-encoded properties back to array\nSELECT properties_to_array(properties_to_dict(properties)) as props\nFROM measures;\n\n-- Use with array functions\nSELECT array_length(properties_to_array(properties_to_dict(properties))) as count\nFROM measures;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#histogram-functions","title":"Histogram Functions","text":"<p>Micromegas provides a comprehensive set of functions for creating and analyzing histograms, enabling efficient statistical analysis of large datasets.</p>"},{"location":"query-guide/functions-reference/#make_histogramstart-end-bins-values","title":"<code>make_histogram(start, end, bins, values)</code>","text":"<p>Creates histogram data from numeric values with specified range and bin count.</p> <p>Syntax: <pre><code>make_histogram(start, end, bins, values)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>start</code> (<code>Float64</code>): Histogram minimum value</p> </li> <li> <p><code>end</code> (<code>Float64</code>): Histogram maximum value</p> </li> <li> <p><code>bins</code> (<code>Int64</code>): Number of histogram bins</p> </li> <li> <p><code>values</code> (<code>Float64</code>): Column of numeric values to histogram</p> </li> </ul> <p>Returns: Histogram structure with buckets and counts</p> <p>Example: <pre><code>-- Create histogram of response times (0-50ms, 20 bins)\nSELECT make_histogram(0.0, 50.0, 20, CAST(duration AS FLOAT64) / 1000000.0) as duration_histogram\nFROM view_instance('thread_spans', 'web_server_123')\nWHERE name = 'handle_request';\n</code></pre></p>"},{"location":"query-guide/functions-reference/#sum_histogramshistogram_column","title":"<code>sum_histograms(histogram_column)</code>","text":"<p>Aggregates multiple histograms by summing their bins.</p> <p>Syntax: <pre><code>sum_histograms(histogram_column)\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>histogram_column</code> (Histogram): Column containing histogram values</li> </ul> <p>Returns: Combined histogram with summed bins</p> <p>Example: <pre><code>-- Combine histograms across processes\nSELECT sum_histograms(duration_histogram) as combined_histogram\nFROM cpu_usage_per_process_per_minute\nWHERE time_bin &gt;= NOW() - INTERVAL '1 hour';\n</code></pre></p>"},{"location":"query-guide/functions-reference/#quantile_from_histogramhistogram-quantile","title":"<code>quantile_from_histogram(histogram, quantile)</code>","text":"<p>Estimates a quantile value from a histogram.</p> <p>Syntax: <pre><code>quantile_from_histogram(histogram, quantile)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>histogram</code> (Histogram): Histogram to analyze</p> </li> <li> <p><code>quantile</code> (<code>Float64</code>): Quantile to estimate (0.0 to 1.0)</p> </li> </ul> <p>Returns: <code>Float64</code> - Estimated quantile value</p> <p>Examples: <pre><code>-- Get median (50th percentile) response time\nSELECT quantile_from_histogram(duration_histogram, 0.5) as median_duration\nFROM performance_histograms;\n\n-- Get 95th percentile response time\nSELECT quantile_from_histogram(duration_histogram, 0.95) as p95_duration\nFROM performance_histograms;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#variance_from_histogramhistogram","title":"<code>variance_from_histogram(histogram)</code>","text":"<p>Calculates variance from histogram data.</p> <p>Syntax: <pre><code>variance_from_histogram(histogram)\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>histogram</code> (Histogram): Histogram to analyze</li> </ul> <p>Returns: <code>Float64</code> - Variance of the histogram data</p> <p>Example: <pre><code>-- Calculate response time variance\nSELECT variance_from_histogram(duration_histogram) as duration_variance\nFROM performance_histograms;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#count_from_histogramhistogram","title":"<code>count_from_histogram(histogram)</code>","text":"<p>Extracts the total count of values from a histogram.</p> <p>Syntax: <pre><code>count_from_histogram(histogram)\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>histogram</code> (Histogram): Histogram to analyze</li> </ul> <p>Returns: <code>UInt64</code> - Total number of values in the histogram</p> <p>Example: <pre><code>-- Get total sample count from histogram\nSELECT count_from_histogram(duration_histogram) as total_samples\nFROM performance_histograms;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#sum_from_histogramhistogram","title":"<code>sum_from_histogram(histogram)</code>","text":"<p>Extracts the sum of all values from a histogram.</p> <p>Syntax: <pre><code>sum_from_histogram(histogram)\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>histogram</code> (Histogram): Histogram to analyze</li> </ul> <p>Returns: <code>Float64</code> - Sum of all values in the histogram</p> <p>Example: <pre><code>-- Get total duration from histogram\nSELECT sum_from_histogram(duration_histogram) as total_duration\nFROM performance_histograms;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#standard-sql-functions","title":"Standard SQL Functions","text":"<p>Micromegas supports all standard DataFusion SQL functions including math, string, date/time, conditional, and array functions. For a complete list with examples, see the DataFusion Scalar Functions documentation.</p>"},{"location":"query-guide/functions-reference/#advanced-query-patterns","title":"Advanced Query Patterns","text":""},{"location":"query-guide/functions-reference/#histogram-analysis","title":"Histogram Analysis","text":"<pre><code>-- Create performance histogram (0-100ms, 10 bins)\nSELECT make_histogram(0.0, 100.0, 10, duration / 1000000.0) as response_time_ms_histogram\nFROM view_instance('thread_spans', 'web_server')\nWHERE name = 'handle_request'\n  AND duration &gt; 1000000;  -- &gt; 1ms\n</code></pre> <pre><code>-- Analyze histogram statistics\nSELECT \n    quantile_from_histogram(response_time_histogram, 0.5) as median_ms,\n    quantile_from_histogram(response_time_histogram, 0.95) as p95_ms,\n    quantile_from_histogram(response_time_histogram, 0.99) as p99_ms,\n    variance_from_histogram(response_time_histogram) as variance,\n    count_from_histogram(response_time_histogram) as sample_count,\n    sum_from_histogram(response_time_histogram) as total_time_ms\nFROM performance_histograms\nWHERE time_bin &gt;= NOW() - INTERVAL '1 hour';\n</code></pre> <pre><code>-- Aggregate histograms across multiple processes\nSELECT \n    time_bin,\n    sum_histograms(cpu_usage_histo) as combined_cpu_histogram,\n    quantile_from_histogram(sum_histograms(cpu_usage_histo), 0.95) as p95_cpu\nFROM cpu_usage_per_process_per_minute\nWHERE time_bin &gt;= NOW() - INTERVAL '1 day'\nGROUP BY time_bin\nORDER BY time_bin;\n</code></pre>"},{"location":"query-guide/functions-reference/#property-extraction-and-filtering","title":"Property Extraction and Filtering","text":"<pre><code>-- Find logs with specific thread names\nSELECT time, level, msg, property_get(process_properties, 'thread-name') as thread\nFROM log_entries\nWHERE property_get(process_properties, 'thread-name') LIKE '%worker%'\nORDER BY time DESC;\n</code></pre>"},{"location":"query-guide/functions-reference/#high-performance-jsonb-property-access","title":"High-Performance JSONB Property Access","text":"<pre><code>-- Convert properties to JSONB for better performance\nSELECT\n    time,\n    msg,\n    property_get(properties_to_jsonb(properties), 'service') as service,\n    property_get(properties_to_jsonb(properties), 'version') as version\nFROM log_entries\nWHERE property_get(properties_to_jsonb(properties), 'env') = 'production'\n  AND time &gt;= NOW() - INTERVAL '1 hour'\nORDER BY time DESC;\n</code></pre> <pre><code>-- Efficient property filtering with JSONB\nWITH jsonb_logs AS (\n    SELECT\n        time,\n        level,\n        msg,\n        properties_to_jsonb(properties) as jsonb_props\n    FROM log_entries\n    WHERE time &gt;= NOW() - INTERVAL '1 day'\n)\nSELECT\n    time,\n    level,\n    msg,\n    property_get(jsonb_props, 'service') as service,\n    property_get(jsonb_props, 'request_id') as request_id\nFROM jsonb_logs\nWHERE property_get(jsonb_props, 'error_code') IS NOT NULL\nORDER BY time DESC;\n</code></pre> <pre><code>-- Property aggregation with optimal performance\nSELECT\n    property_get(properties_to_jsonb(properties), 'service') as service,\n    property_get(properties_to_jsonb(properties), 'env') as environment,\n    COUNT(*) as event_count,\n    COUNT(CASE WHEN level &lt;= 2 THEN 1 END) as error_count\nFROM log_entries\nWHERE time &gt;= NOW() - INTERVAL '1 hour'\n  AND property_get(properties_to_jsonb(properties), 'service') IS NOT NULL\nGROUP BY service, environment\nORDER BY error_count DESC;\n</code></pre>"},{"location":"query-guide/functions-reference/#json-data-processing","title":"JSON Data Processing","text":"<pre><code>-- Parse and extract configuration from JSON logs\nSELECT \n    time,\n    msg,\n    jsonb_as_string(jsonb_get(jsonb_parse(msg), 'service')) as service_name,\n    jsonb_as_i64(jsonb_get(jsonb_parse(msg), 'port')) as port,\n    jsonb_as_f64(jsonb_get(jsonb_parse(msg), 'cpu_limit')) as cpu_limit\nFROM log_entries\nWHERE msg LIKE '%{%'  -- Contains JSON\n  AND jsonb_parse(msg) IS NOT NULL\nORDER BY time DESC;\n</code></pre> <pre><code>-- Aggregate metrics from JSON payloads\nSELECT \n    jsonb_as_string(jsonb_get(jsonb_parse(msg), 'service')) as service,\n    COUNT(*) as event_count,\n    AVG(jsonb_as_f64(jsonb_get(jsonb_parse(msg), 'response_time'))) as avg_response_ms\nFROM log_entries\nWHERE msg LIKE '%response_time%'\n  AND jsonb_parse(msg) IS NOT NULL\nGROUP BY service\nORDER BY avg_response_ms DESC;\n</code></pre>"},{"location":"query-guide/functions-reference/#time-based-aggregation","title":"Time-based Aggregation","text":"<pre><code>-- Hourly error counts\nSELECT \n    date_trunc('hour', time) as hour,\n    COUNT(*) as error_count\nFROM log_entries\nWHERE level &lt;= 2  -- Fatal and Error\n  AND time &gt;= NOW() - INTERVAL '24 hours'\nGROUP BY date_trunc('hour', time)\nORDER BY hour;\n</code></pre>"},{"location":"query-guide/functions-reference/#performance-trace-analysis","title":"Performance Trace Analysis","text":"<pre><code>-- Top 10 slowest functions with statistics\nSELECT \n    name,\n    COUNT(*) as call_count,\n    AVG(duration) / 1000000.0 as avg_ms,\n    MAX(duration) / 1000000.0 as max_ms,\n    STDDEV(duration) / 1000000.0 as stddev_ms\nFROM view_instance('thread_spans', 'my_process')\nWHERE duration &gt; 100000  -- &gt; 0.1ms\nGROUP BY name\nORDER BY avg_ms DESC\nLIMIT 10;\n</code></pre>"},{"location":"query-guide/functions-reference/#datafusion-reference","title":"DataFusion Reference","text":"<p>Micromegas supports all standard DataFusion SQL syntax, functions, and operators. For complete documentation including functions, operators, data types, and SQL syntax, see the Apache DataFusion SQL Reference.</p>"},{"location":"query-guide/functions-reference/#next-steps","title":"Next Steps","text":"<ul> <li>Query Patterns - Common observability query patterns</li> <li>Performance Guide - Optimize your queries for best performance</li> <li>Schema Reference - Complete view and field reference</li> </ul>"},{"location":"query-guide/performance/","title":"Performance Guide","text":"<p>Guidelines for writing efficient Micromegas SQL queries and avoiding common performance pitfalls.</p>"},{"location":"query-guide/performance/#critical-performance-rules","title":"Critical Performance Rules","text":""},{"location":"query-guide/performance/#1-always-use-time-ranges-via-python-api","title":"1. Always Use Time Ranges (via Python API)","text":"<p>\u26a1 Performance Tip: Always specify time ranges through the Python API parameters, not in SQL WHERE clauses.</p> <p>\u274c Inefficient - SQL time filter: <pre><code># Analytics server scans ALL partitions, then filters in SQL\nsql = \"\"\"\n    SELECT COUNT(*) FROM log_entries \n    WHERE time &gt;= NOW() - INTERVAL '1 hour';\n\"\"\"\nresult = client.query(sql)  # No time range parameters!\n</code></pre></p> <p>\u2705 Efficient - API time range: <pre><code>import datetime\n\n# Analytics server eliminates irrelevant partitions BEFORE query execution\nnow = datetime.datetime.now(datetime.timezone.utc)\nbegin = now - datetime.timedelta(hours=1)\nend = now\n\nsql = \"SELECT COUNT(*) FROM log_entries;\"\nresult = client.query(sql, begin, end)  # \u2b50 Time range in API\n</code></pre></p> <p>Why API time ranges are faster:</p> <ul> <li>Partition Elimination: Analytics server removes entire partitions from consideration before SQL execution</li> <li>Metadata Optimization: Uses partition metadata to skip irrelevant data files  </li> <li>Memory Efficiency: Only loads relevant data into query engine memory</li> <li>Network Efficiency: Transfers only relevant data over FlightSQL</li> </ul> <p>Performance Impact:</p> <ul> <li>API time range: Query considers only 1-2 partitions</li> <li>SQL time filter: Query scans all partitions, then filters millions of rows</li> </ul>"},{"location":"query-guide/performance/#2-use-process-scoped-views","title":"2. Use Process-Scoped Views","text":"<p>\u274c Less Efficient: <pre><code>-- Scans all data then filters\nSELECT * FROM log_entries WHERE process_id = 'my_process';\n</code></pre></p> <p>\u2705 More Efficient: <pre><code>-- Uses optimized process partition\nSELECT * FROM view_instance('log_entries', 'my_process');\n</code></pre></p>"},{"location":"query-guide/performance/#query-optimization","title":"Query Optimization","text":""},{"location":"query-guide/performance/#predicate-pushdown","title":"Predicate Pushdown","text":"<p>Micromegas automatically pushes filters down to the storage layer when possible:</p> <pre><code>-- These filters are pushed to Parquet reader for efficiency\nWHERE time &gt;= NOW() - INTERVAL '1 day'\n  AND level &lt;= 3\n  AND process_id = 'my_process'\n</code></pre>"},{"location":"query-guide/performance/#memory-considerations","title":"Memory Considerations","text":"<p>Use LIMIT for exploration: <pre><code>-- Good for testing queries\nSELECT * FROM log_entries LIMIT 1000;\n</code></pre></p> <p>Use streaming for large results: <pre><code># Python API for large datasets\nfor batch in client.query_stream(sql, begin, end):\n    process_batch(batch.to_pandas())\n</code></pre></p>"},{"location":"query-guide/properties-jsonb-migration/","title":"Properties to JSONB Migration","text":"<p>This guide explains the migration from legacy properties format to the new dictionary-encoded JSONB format in Micromegas, providing improved storage efficiency and query performance.</p>"},{"location":"query-guide/properties-jsonb-migration/#overview","title":"Overview","text":"<p>Micromegas has migrated properties storage from the original <code>List&lt;Struct&lt;key: String, value: String&gt;&gt;</code> format to an optimized <code>Dictionary&lt;Int32, Binary&gt;</code> JSONB format. This migration provides:</p> <ul> <li>70-80% storage reduction through dictionary compression</li> <li>Improved query performance with native JSONB operations</li> <li>Full backward compatibility - all existing queries work unchanged</li> <li>Zero downtime deployment via automatic schema versioning</li> </ul>"},{"location":"query-guide/properties-jsonb-migration/#migration-status","title":"Migration Status","text":"<p>\u2705 Migration Complete - All view sets now use the new JSONB format:</p> View Set Status Schema Version Properties Fields blocks \u2705 Complete v2 <code>processes.properties</code>, <code>streams.properties</code> processes \u2705 Complete Inherited <code>properties</code> streams \u2705 Complete Inherited <code>properties</code> log_entries \u2705 Complete v5 <code>properties</code>, <code>process_properties</code> measures \u2705 Complete v5 <code>properties</code>, <code>process_properties</code>"},{"location":"query-guide/properties-jsonb-migration/#key-benefits","title":"Key Benefits","text":""},{"location":"query-guide/properties-jsonb-migration/#storage-efficiency","title":"Storage Efficiency","text":"<ul> <li>Dictionary compression eliminates redundant JSONB objects</li> <li>Typical storage reduction of 70-80% for property data</li> <li>Better memory utilization during query processing</li> </ul>"},{"location":"query-guide/properties-jsonb-migration/#performance-improvements","title":"Performance Improvements","text":"<ul> <li>Native JSONB operations via <code>property_get()</code> and <code>properties_length()</code></li> <li>Optimized UDF implementations handle both legacy and JSONB formats</li> <li>Automatic pass-through optimization for JSONB data</li> </ul>"},{"location":"query-guide/properties-jsonb-migration/#operational-benefits","title":"Operational Benefits","text":"<ul> <li>Zero-downtime migration through schema versioning</li> <li>Automatic partition rebuilds triggered by schema hash changes</li> <li>Full backward compatibility with existing SQL queries</li> </ul>"},{"location":"query-guide/properties-jsonb-migration/#technical-details","title":"Technical Details","text":""},{"location":"query-guide/properties-jsonb-migration/#storage-format-comparison","title":"Storage Format Comparison","text":"<p>Legacy Format (Before Migration): <pre><code>-- Inefficient nested structure\nList&lt;Struct&lt;\n    key: Utf8,\n    value: Utf8\n&gt;&gt;\n</code></pre></p> <p>New Format (After Migration): <pre><code>-- Optimized dictionary-encoded JSONB\nDictionary&lt;Int32, Binary&gt;\n</code></pre></p>"},{"location":"query-guide/properties-jsonb-migration/#migration-method","title":"Migration Method","text":"<ol> <li>Read-time Transformation: Existing data converted to JSONB during query processing</li> <li>Schema Versioning: Version increments trigger automatic partition rebuilds</li> <li>Zero Database Changes: PostgreSQL schema remains unchanged</li> <li>Automatic Inheritance: Process/stream views inherit JSONB from blocks table</li> </ol>"},{"location":"query-guide/properties-jsonb-migration/#query-compatibility","title":"Query Compatibility","text":""},{"location":"query-guide/properties-jsonb-migration/#no-changes-required","title":"No Changes Required","text":"<p>All existing queries continue to work unchanged:</p> <pre><code>-- These queries work identically before and after migration\nSELECT property_get(properties, 'service') as service_name\nFROM log_entries\nWHERE properties_length(properties) &gt; 0;\n\nSELECT property_get(process_properties, 'thread-name') as thread_name\nFROM measures\nWHERE property_get(process_properties, 'version') = '1.2.3';\n</code></pre>"},{"location":"query-guide/properties-jsonb-migration/#enhanced-functions","title":"Enhanced Functions","text":"<p>The migration enhances existing functions with automatic format detection:</p>"},{"location":"query-guide/properties-jsonb-migration/#property_getproperties-key","title":"<code>property_get(properties, key)</code>","text":"<ul> <li>Automatically handles all property formats (legacy, JSONB, dictionary-encoded)</li> <li>Returns same results regardless of underlying storage format</li> <li>Performance optimized for JSONB with pass-through operations</li> </ul>"},{"location":"query-guide/properties-jsonb-migration/#properties_lengthproperties","title":"<code>properties_length(properties)</code>","text":"<ul> <li>Works with all property formats transparently</li> <li>Replaces legacy <code>array_length(properties)</code> calls</li> <li>Consistent behavior across all view sets</li> </ul>"},{"location":"query-guide/properties-jsonb-migration/#migration-process","title":"Migration Process","text":""},{"location":"query-guide/properties-jsonb-migration/#automatic-schema-versioning","title":"Automatic Schema Versioning","text":"<p>The migration uses schema versioning to trigger automatic rebuilds:</p> <ol> <li>Schema Hash Changes: Property format changes increment schema version</li> <li>Partition Rebuilds: New schema versions automatically rebuild affected partitions</li> <li>Transparent Process: No manual intervention required</li> <li>Rollback Safety: Previous partitions remain until rebuild completion</li> </ol>"},{"location":"query-guide/properties-jsonb-migration/#view-set-details","title":"View Set Details","text":""},{"location":"query-guide/properties-jsonb-migration/#phase-1-core-infrastructure","title":"Phase 1: Core Infrastructure","text":"<ul> <li>blocks table: Updated to output JSONB via <code>PropertiesColumnReader</code></li> <li>Schema version: v1 \u2192 v2</li> <li>Impact: Foundation for all other view sets</li> </ul>"},{"location":"query-guide/properties-jsonb-migration/#phase-2-inherited-views","title":"Phase 2: Inherited Views","text":"<ul> <li>processes/streams: Automatically inherit JSONB from blocks table</li> <li>No explicit changes: SQL inheritance provides automatic JSONB format</li> <li>Zero effort: Benefits gained without code modifications</li> </ul>"},{"location":"query-guide/properties-jsonb-migration/#phase-3-direct-schema-updates","title":"Phase 3: Direct Schema Updates","text":"<ul> <li>log_entries/measures: Direct schema and builder updates</li> <li>Schema version: v4 \u2192 v5</li> <li>Arrow builders: Updated to use JSONB dictionary builders</li> </ul>"},{"location":"query-guide/properties-jsonb-migration/#best-practices","title":"Best Practices","text":""},{"location":"query-guide/properties-jsonb-migration/#query-optimization","title":"Query Optimization","text":"<p>Use the new format for maximum performance:</p> <pre><code>-- Optimal: Direct property access (post-migration)\nSELECT property_get(properties, 'hostname') as hostname\nFROM log_entries\nWHERE properties_length(properties) &gt; 0;\n\n-- Still works: Legacy compatibility maintained\nSELECT property_get(properties, 'hostname') as hostname\nFROM log_entries\nWHERE property_get(properties, 'env') = 'production';\n</code></pre>"},{"location":"query-guide/properties-jsonb-migration/#property-filtering","title":"Property Filtering","text":"<p>Take advantage of JSONB efficiency:</p> <pre><code>-- Efficient property-based filtering\nSELECT time, level, msg\nFROM log_entries\nWHERE property_get(properties, 'service') = 'web_server'\n  AND property_get(properties, 'env') = 'production'\n  AND time &gt;= NOW() - INTERVAL '1 hour';\n</code></pre>"},{"location":"query-guide/properties-jsonb-migration/#aggregation-patterns","title":"Aggregation Patterns","text":"<p>Properties work efficiently in GROUP BY operations:</p> <pre><code>-- Service-level error analysis\nSELECT\n    property_get(properties, 'service') as service,\n    COUNT(*) as total_logs,\n    COUNT(CASE WHEN level &lt;= 2 THEN 1 END) as error_count\nFROM log_entries\nWHERE time &gt;= NOW() - INTERVAL '1 day'\n  AND properties_length(properties) &gt; 0\nGROUP BY service\nORDER BY error_count DESC;\n</code></pre>"},{"location":"query-guide/properties-jsonb-migration/#monitoring-and-validation","title":"Monitoring and Validation","text":""},{"location":"query-guide/properties-jsonb-migration/#performance-verification","title":"Performance Verification","text":"<p>Monitor query performance improvements:</p> <pre><code>-- Property access patterns\nSELECT\n    COUNT(*) as properties_queries,\n    AVG(properties_length(properties)) as avg_prop_count\nFROM log_entries\nWHERE time &gt;= NOW() - INTERVAL '1 hour'\n  AND properties_length(properties) &gt; 0;\n</code></pre>"},{"location":"query-guide/properties-jsonb-migration/#data-integrity-checks","title":"Data Integrity Checks","text":"<p>Verify migration completion:</p> <pre><code>-- Check property data consistency\nSELECT\n    COUNT(*) as total_rows,\n    COUNT(CASE WHEN properties_length(properties) &gt; 0 THEN 1 END) as with_properties,\n    COUNT(CASE WHEN property_get(properties, 'hostname') IS NOT NULL THEN 1 END) as with_hostname\nFROM processes\nWHERE insert_time &gt;= NOW() - INTERVAL '1 day';\n</code></pre>"},{"location":"query-guide/properties-jsonb-migration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"query-guide/properties-jsonb-migration/#common-issues","title":"Common Issues","text":""},{"location":"query-guide/properties-jsonb-migration/#property-access-returns-null","title":"Property Access Returns NULL","text":"<p>Symptom: <code>property_get()</code> returns NULL for existing properties Solution: Verify property key names - JSONB is case-sensitive</p>"},{"location":"query-guide/properties-jsonb-migration/#performance-not-improved","title":"Performance Not Improved","text":"<p>Symptom: Queries still slow after migration Solution: Ensure using <code>properties_length()</code> instead of <code>array_length()</code></p>"},{"location":"query-guide/properties-jsonb-migration/#rollback-procedures","title":"Rollback Procedures","text":"<p>If issues arise, the migration can be rolled back:</p> <ol> <li>Schema Revert: Restore previous schema versions via code rollback</li> <li>Partition Rebuild: Automatic rebuild occurs with old schema hash</li> <li>Query Compatibility: No SQL changes needed during rollback</li> </ol>"},{"location":"query-guide/properties-jsonb-migration/#support","title":"Support","text":""},{"location":"query-guide/properties-jsonb-migration/#updated-functions","title":"Updated Functions","text":"<p>All property functions maintain backward compatibility:</p> <ul> <li>\u2705 <code>property_get()</code> - Works with all formats</li> <li>\u2705 <code>properties_length()</code> - Replaces <code>array_length()</code></li> <li>\u2705 <code>properties_to_jsonb()</code> - Pass-through for JSONB data</li> <li>\u2705 <code>properties_to_dict()</code> - Dictionary encoding support</li> </ul>"},{"location":"query-guide/properties-jsonb-migration/#documentation","title":"Documentation","text":"<ul> <li>Schema Reference - Updated field types and formats</li> <li>Functions Reference - Property function documentation</li> <li>Query Patterns - Optimized query examples</li> </ul>"},{"location":"query-guide/properties-jsonb-migration/#conclusion","title":"Conclusion","text":"<p>The properties to JSONB migration represents a significant improvement in Micromegas storage efficiency and query performance. With full backward compatibility and automatic deployment, users benefit from:</p> <ul> <li>Immediate storage savings of 70-80% for property data</li> <li>Improved query performance through optimized JSONB operations</li> <li>Zero operational impact via seamless migration process</li> <li>Future-proof architecture for advanced property operations</li> </ul> <p>All existing queries continue to work unchanged while gaining the benefits of the new optimized storage format.</p>"},{"location":"query-guide/python-api-advanced/","title":"Python API Advanced Guide","text":"<p>This guide covers advanced usage patterns, performance optimization techniques, and specialized features for power users of the Micromegas Python client.</p> <p>Prerequisites</p> <p>Before reading this guide, familiarize yourself with the Python API Reference for basic usage patterns.</p>"},{"location":"query-guide/python-api-advanced/#advanced-connection-patterns","title":"Advanced Connection Patterns","text":""},{"location":"query-guide/python-api-advanced/#authentication-and-headers","title":"Authentication and Headers","text":"<p>Configure custom authentication headers for enterprise deployments:</p> <pre><code>from micromegas.flightsql.client import FlightSQLClient\n\n# Token-based authentication\nclient = FlightSQLClient(\n    \"grpc+tls://analytics.company.com:50051\",\n    headers={\n        \"authorization\": \"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\",\n        \"x-tenant-id\": \"production\",\n        \"x-request-id\": \"trace-12345\"\n    }\n)\n\n# API key authentication  \nclient = FlightSQLClient(\n    \"grpc+tls://micromegas.example.com:50051\",\n    headers={\n        \"x-api-key\": \"mm_prod_1234567890abcdef\",\n        \"x-client-version\": \"python-1.0.0\"\n    }\n)\n</code></pre>"},{"location":"query-guide/python-api-advanced/#schema-discovery-and-query-validation","title":"Schema Discovery and Query Validation","text":""},{"location":"query-guide/python-api-advanced/#advanced-schema-introspection","title":"Advanced Schema Introspection","text":"<pre><code>def analyze_query_schema(client, sql):\n    \"\"\"Analyze query result schema without execution.\"\"\"\n    stmt = client.prepare_statement(sql)\n\n    analysis = {\n        'column_count': len(stmt.dataset_schema),\n        'columns': [],\n        'estimated_row_size': 0\n    }\n\n    for field in stmt.dataset_schema:\n        col_info = {\n            'name': field.name,\n            'type': str(field.type),\n            'nullable': field.nullable\n        }\n\n        # Estimate column size for memory planning\n        if 'string' in str(field.type):\n            estimated_size = 50  # Average string length\n        elif 'int64' in str(field.type):\n            estimated_size = 8\n        elif 'int32' in str(field.type):\n            estimated_size = 4\n        elif 'timestamp' in str(field.type):\n            estimated_size = 8\n        else:\n            estimated_size = 16  # Default estimate\n\n        col_info['estimated_bytes'] = estimated_size\n        analysis['columns'].append(col_info)\n        analysis['estimated_row_size'] += estimated_size\n\n    return analysis\n\n# Usage\nsql = \"SELECT time, process_id, level, msg FROM log_entries\"\nschema_info = analyze_query_schema(client, sql)\n\nprint(f\"Query will return {schema_info['column_count']} columns\")\nprint(f\"Estimated row size: {schema_info['estimated_row_size']} bytes\")\n\nfor col in schema_info['columns']:\n    print(f\"  {col['name']}: {col['type']} ({col['estimated_bytes']} bytes)\")\n</code></pre>"},{"location":"query-guide/python-api-advanced/#query-validation-pipeline","title":"Query Validation Pipeline","text":"<pre><code>def validate_query(client, sql, max_columns=50, max_estimated_size=1000):\n    \"\"\"Validate query before execution.\"\"\"\n    try:\n        analysis = analyze_query_schema(client, sql)\n    except Exception as e:\n        return {\n            'valid': False,\n            'error': f\"Schema analysis failed: {e}\",\n            'recommendations': [\"Check SQL syntax and table names\"]\n        }\n\n    recommendations = []\n\n    if analysis['column_count'] &gt; max_columns:\n        recommendations.append(f\"Query returns {analysis['column_count']} columns, consider selecting specific columns\")\n\n    if analysis['estimated_row_size'] &gt; max_estimated_size:\n        recommendations.append(f\"Large estimated row size ({analysis['estimated_row_size']} bytes), consider using query_stream()\")\n\n    # Check for potentially expensive operations\n    sql_upper = sql.upper()\n    if 'ORDER BY' in sql_upper and 'LIMIT' not in sql_upper:\n        recommendations.append(\"ORDER BY without LIMIT may be expensive, consider adding LIMIT\")\n\n    if not any(param in sql_upper for param in ['BEGIN', 'END', 'WHERE TIME']):\n        recommendations.append(\"No time filtering detected, consider adding time range parameters\")\n\n    return {\n        'valid': True,\n        'analysis': analysis,\n        'recommendations': recommendations\n    }\n\n# Usage\nsql = \"\"\"\nSELECT time, process_id, level, target, msg, properties \nFROM log_entries \nORDER BY time DESC\n\"\"\"\n\nvalidation = validate_query(client, sql)\nif validation['valid']:\n    print(\"Query is valid\")\n    for rec in validation['recommendations']:\n        print(f\"\u26a0\ufe0f  {rec}\")\nelse:\n    print(f\"\u274c Query validation failed: {validation['error']}\")\n</code></pre>"},{"location":"query-guide/python-api-advanced/#performance-optimization-patterns","title":"Performance Optimization Patterns","text":""},{"location":"query-guide/python-api-advanced/#intelligent-batching-for-large-datasets","title":"Intelligent Batching for Large Datasets","text":"<pre><code>import time\nfrom datetime import datetime, timedelta, timezone\n\nclass OptimizedQueryExecutor:\n    \"\"\"Execute large queries with intelligent batching and progress tracking.\"\"\"\n\n    def __init__(self, client, batch_size_mb=100):\n        self.client = client\n        self.batch_size_mb = batch_size_mb\n        self.stats = {\n            'batches_processed': 0,\n            'rows_processed': 0,\n            'total_time': 0,\n            'avg_batch_time': 0\n        }\n\n    def execute_large_query(self, sql, begin, end, processor_func=None):\n        \"\"\"Execute query with automatic batching based on result size.\"\"\"\n        start_time = time.time()\n        total_rows = 0\n\n        # First, estimate result size\n        count_sql = f\"SELECT COUNT(*) as row_count FROM ({sql}) as subquery\"\n        try:\n            count_df = self.client.query(count_sql, begin, end)\n            estimated_rows = count_df['row_count'].iloc[0]\n            print(f\"\ud83d\udcca Estimated {estimated_rows:,} rows\")\n        except:\n            estimated_rows = None\n            print(\"\u26a0\ufe0f  Could not estimate row count\")\n\n        # Execute with streaming for large results\n        batch_count = 0\n        for batch in self.client.query_stream(sql, begin, end):\n            batch_start = time.time()\n            df = batch.to_pandas()\n            batch_rows = len(df)\n\n            # Process batch\n            if processor_func:\n                processor_func(df, batch_count)\n\n            # Update statistics\n            batch_count += 1\n            total_rows += batch_rows\n            batch_time = time.time() - batch_start\n\n            self.stats['batches_processed'] = batch_count\n            self.stats['rows_processed'] = total_rows\n            self.stats['avg_batch_time'] = (self.stats['avg_batch_time'] * (batch_count - 1) + batch_time) / batch_count\n\n            # Progress reporting\n            if estimated_rows:\n                progress = (total_rows / estimated_rows) * 100\n                print(f\"\ud83d\udd04 Batch {batch_count}: {batch_rows:,} rows ({progress:.1f}% complete)\")\n            else:\n                print(f\"\ud83d\udd04 Batch {batch_count}: {batch_rows:,} rows ({total_rows:,} total)\")\n\n        self.stats['total_time'] = time.time() - start_time\n\n        print(f\"\u2705 Completed: {total_rows:,} rows in {self.stats['total_time']:.2f}s\")\n        print(f\"   Average: {total_rows/self.stats['total_time']:.0f} rows/sec\")\n\n        return self.stats\n\n# Usage example\ndef process_error_logs(df, batch_num):\n    \"\"\"Process each batch of error logs.\"\"\"\n    errors = df[df['level'] &lt;= 2]  # Error and critical levels\n    if not errors.empty:\n        print(f\"  Found {len(errors)} errors in batch {batch_num}\")\n        # Could save to file, send alerts, etc.\n\nexecutor = OptimizedQueryExecutor(client)\nend = datetime.now(timezone.utc)\nbegin = end - timedelta(days=7)\n\nstats = executor.execute_large_query(\n    \"SELECT time, level, target, msg FROM log_entries WHERE level &lt;= 3\",\n    begin, end,\n    processor_func=process_error_logs\n)\n</code></pre>"},{"location":"query-guide/python-api-advanced/#next-steps","title":"Next Steps","text":"<ul> <li>Schema Reference - Understand table structures and relationships</li> <li>Functions Reference - Available SQL functions and operators  </li> <li>Query Patterns - Common observability query patterns</li> <li>Performance Guide - Query optimization techniques</li> </ul> <p>For complex integration scenarios or custom tooling, consider the patterns in this guide as starting points for your specific use case.</p>"},{"location":"query-guide/python-api/","title":"Python API Reference","text":"<p>The Micromegas Python client provides a simple but powerful interface for querying observability data using SQL. This page covers all client methods, connection options, and advanced features.</p>"},{"location":"query-guide/python-api/#installation","title":"Installation","text":"<p>Install the Micromegas Python client from PyPI:</p> <pre><code>pip install micromegas\n</code></pre>"},{"location":"query-guide/python-api/#basic-usage","title":"Basic Usage","text":""},{"location":"query-guide/python-api/#connection","title":"Connection","text":"<pre><code>import micromegas\n\n# Connect to local Micromegas instance\nclient = micromegas.connect()\n\n# Connect with dictionary encoding preservation (for memory efficiency)\nclient = micromegas.connect(preserve_dictionary=True)\n</code></pre> <p>The <code>connect()</code> function connects to the analytics service at <code>grpc://localhost:50051</code>.</p> <p>Parameters: - <code>preserve_dictionary</code> (bool, optional): Enable dictionary encoding preservation for memory-efficient data transfer. Default: <code>False</code></p>"},{"location":"query-guide/python-api/#simple-queries","title":"Simple Queries","text":"<pre><code>import datetime\n\n# Set up time range\nnow = datetime.datetime.now(datetime.timezone.utc)\nbegin = now - datetime.timedelta(hours=1)\nend = now\n\n# Execute query with time range\nsql = \"SELECT * FROM log_entries LIMIT 10;\"\ndf = client.query(sql, begin, end)\nprint(df)\n</code></pre>"},{"location":"query-guide/python-api/#client-methods","title":"Client Methods","text":""},{"location":"query-guide/python-api/#querysql-beginnone-endnone","title":"<code>query(sql, begin=None, end=None)</code>","text":"<p>Execute a SQL query and return results as a pandas DataFrame.</p> <p>Parameters:</p> <ul> <li><code>sql</code> (str): SQL query string</li> <li><code>begin</code> (datetime or str, optional): \u26a1 Recommended - Start time for partition elimination. Can be a <code>datetime</code> object or RFC3339 string (e.g., <code>\"2024-01-01T00:00:00Z\"</code>)</li> <li><code>end</code> (datetime or str, optional): \u26a1 Recommended - End time for partition elimination. Can be a <code>datetime</code> object or RFC3339 string (e.g., <code>\"2024-01-01T23:59:59Z\"</code>)</li> </ul> <p>Returns:</p> <ul> <li><code>pandas.DataFrame</code>: Query results</li> </ul> <p>Performance Note: Using <code>begin</code> and <code>end</code> parameters instead of SQL time filters allows the analytics server to eliminate entire partitions before query execution, providing significant performance improvements.</p> <p>Example: <pre><code># \u2705 EFFICIENT: API time range enables partition elimination\ndf = client.query(\"\"\"\n    SELECT time, process_id, level, msg\n    FROM log_entries\n    WHERE level &lt;= 3\n    ORDER BY time DESC\n    LIMIT 100;\n\"\"\", begin, end)  # \u2b50 Time range in API parameters\n\n# \u274c INEFFICIENT: SQL time filter scans all partitions\ndf = client.query(\"\"\"\n    SELECT time, process_id, level, msg\n    FROM log_entries\n    WHERE time &gt;= NOW() - INTERVAL '1 hour'  -- Server scans ALL partitions\n      AND level &lt;= 3\n    ORDER BY time DESC\n    LIMIT 100;\n\"\"\")  # Missing API time parameters!\n\n# \u2705 Using RFC3339 strings for time ranges\ndf = client.query(\"\"\"\n    SELECT time, process_id, level, msg\n    FROM log_entries\n    WHERE level &lt;= 3\n    ORDER BY time DESC\n    LIMIT 100;\n\"\"\", \"2024-01-01T00:00:00Z\", \"2024-01-01T23:59:59Z\")  # \u2b50 RFC3339 strings\n\n# \u2705 OK: Query without time range (for metadata queries)\nprocesses = client.query(\"SELECT process_id, exe FROM processes LIMIT 10;\")\n</code></pre></p>"},{"location":"query-guide/python-api/#query_streamsql-beginnone-endnone","title":"<code>query_stream(sql, begin=None, end=None)</code>","text":"<p>Execute a SQL query and return results as a stream of Apache Arrow RecordBatch objects. Use this for large datasets to avoid memory issues.</p> <p>Parameters:</p> <ul> <li><code>sql</code> (str): SQL query string  </li> <li><code>begin</code> (datetime or str, optional): \u26a1 Recommended - Start time for partition elimination. Can be a <code>datetime</code> object or RFC3339 string (e.g., <code>\"2024-01-01T00:00:00Z\"</code>)</li> <li><code>end</code> (datetime or str, optional): \u26a1 Recommended - End time for partition elimination. Can be a <code>datetime</code> object or RFC3339 string (e.g., <code>\"2024-01-01T23:59:59Z\"</code>)</li> </ul> <p>Returns:</p> <ul> <li>Iterator of <code>pyarrow.RecordBatch</code>: Stream of result batches</li> </ul> <p>Example: <pre><code>import pyarrow as pa\n\n# Stream large dataset\nsql = \"\"\"\n    SELECT time, process_id, level, target, msg\n    FROM log_entries\n    WHERE time &gt;= NOW() - INTERVAL '7 days'\n    ORDER BY time DESC;\n\"\"\"\n\nfor record_batch in client.query_stream(sql, begin, end):\n    # record_batch is a pyarrow.RecordBatch\n    print(f\"Batch shape: {record_batch.num_rows} x {record_batch.num_columns}\")\n    print(f\"Schema: {record_batch.schema}\")\n\n    # Convert to pandas for analysis\n    df = record_batch.to_pandas()\n\n    # Process this batch\n    error_logs = df[df['level'] &lt;= 3]\n    if not error_logs.empty:\n        print(f\"Found {len(error_logs)} errors in this batch\")\n        # Process errors...\n\n    # Memory is automatically freed after each batch\n</code></pre></p>"},{"location":"query-guide/python-api/#query_arrowsql-beginnone-endnone","title":"<code>query_arrow(sql, begin=None, end=None)</code>","text":"<p>Execute a SQL query and return results as an Apache Arrow Table. This method preserves dictionary encoding when <code>preserve_dictionary=True</code> is set during connection.</p> <p>Parameters:</p> <ul> <li><code>sql</code> (str): SQL query string</li> <li><code>begin</code> (datetime or str, optional): Start time for partition elimination</li> <li><code>end</code> (datetime or str, optional): End time for partition elimination</li> </ul> <p>Returns:</p> <ul> <li><code>pyarrow.Table</code>: Query results as Arrow Table</li> </ul> <p>Example: <pre><code># Connect with dictionary preservation\ndict_client = micromegas.connect(preserve_dictionary=True)\n\n# Get Arrow table with preserved dictionary encoding\ntable = dict_client.query_arrow(\"\"\"\n    SELECT properties_to_dict(properties) as dict_props\n    FROM measures\n\"\"\", begin, end)\n\n# Check if column uses dictionary encoding\nprint(f\"Dictionary encoded: {pa.types.is_dictionary(table.schema.field('dict_props').type)}\")\nprint(f\"Memory usage: {table.nbytes:,} bytes\")\n</code></pre></p>"},{"location":"query-guide/python-api/#dictionary-encoding-for-memory-efficiency","title":"Dictionary Encoding for Memory Efficiency","text":"<p>When working with large datasets containing repeated values (like properties), dictionary encoding can reduce memory usage by 50-80%. Micromegas provides built-in support for dictionary encoding:</p>"},{"location":"query-guide/python-api/#using-dictionary-encoded-properties","title":"Using Dictionary-Encoded Properties","text":"<pre><code># Connect with dictionary preservation enabled\nclient = micromegas.connect(preserve_dictionary=True)\n\n# Use properties_to_dict UDF for dictionary encoding\nsql = \"\"\"\nSELECT \n    time,\n    process_id,\n    properties_to_dict(properties) as dict_props,\n    properties_length(properties_to_dict(properties)) as prop_count\nFROM measures\nWHERE time &gt;= NOW() - INTERVAL '1 hour'\n\"\"\"\n\n# Option 1: Get as pandas DataFrame (automatic conversion)\ndf = client.query(sql, begin, end)\nprint(f\"DataFrame shape: {df.shape}\")\nprint(f\"Memory usage: {df.memory_usage(deep=True).sum():,} bytes\")\n\n# Option 2: Get as Arrow Table (preserves dictionary encoding)\ntable = client.query_arrow(sql, begin, end)\nprint(f\"Arrow table memory: {table.nbytes:,} bytes\")\n\n# Dictionary encoding typically uses 50-80% less memory\n</code></pre>"},{"location":"query-guide/python-api/#compatibility-with-standard-functions","title":"Compatibility with Standard Functions","text":"<p>Dictionary-encoded data works seamlessly with Micromegas UDFs:</p> <pre><code>sql = \"\"\"\nSELECT \n    -- Direct property access\n    property_get(properties, 'source') as source,\n\n    -- Length calculation (works with both formats)\n    properties_length(properties) as regular_count,\n    properties_length(properties_to_dict(properties)) as dict_count,\n\n    -- Convert back to array when needed\n    array_length(properties_to_array(properties_to_dict(properties))) as array_count\nFROM measures\n\"\"\"\n\ndf = client.query(sql, begin, end)\n</code></pre>"},{"location":"query-guide/python-api/#working-with-results","title":"Working with Results","text":""},{"location":"query-guide/python-api/#pandas-dataframes","title":"pandas DataFrames","text":"<p>All <code>query()</code> results are pandas DataFrames, giving you access to the full pandas ecosystem:</p> <pre><code># Basic DataFrame operations\nresult = client.query(\"SELECT process_id, exe, start_time FROM processes;\")\n\n# Inspect the data\nprint(f\"Shape: {result.shape}\")\nprint(f\"Columns: {result.columns.tolist()}\")\nprint(f\"Data types:\\n{result.dtypes}\")\n\n# Filter and analyze\nrecent = result[result['start_time'] &gt; datetime.datetime.now() - datetime.timedelta(days=1)]\nprint(f\"Recent processes: {len(recent)}\")\n\n# Group and aggregate\nby_exe = result.groupby('exe').size().sort_values(ascending=False)\nprint(\"Processes by executable:\")\nprint(by_exe.head())\n</code></pre>"},{"location":"query-guide/python-api/#pyarrow-recordbatch","title":"pyarrow RecordBatch","text":"<p>Streaming queries return Apache Arrow RecordBatch objects:</p> <pre><code>for batch in client.query_stream(sql, begin, end):\n    # RecordBatch properties\n    print(f\"Rows: {batch.num_rows}\")\n    print(f\"Columns: {batch.num_columns}\")\n    print(f\"Schema: {batch.schema}\")\n\n    # Access individual columns\n    time_column = batch.column('time')\n    level_column = batch.column('level')\n\n    # Convert to pandas (zero-copy operation)\n    df = batch.to_pandas()\n\n    # Convert to other formats\n    table = batch.to_pylist()  # List of dictionaries\n    numpy_dict = batch.to_pydict()  # Dictionary of numpy arrays\n</code></pre>"},{"location":"query-guide/python-api/#connection-configuration","title":"Connection Configuration","text":""},{"location":"query-guide/python-api/#flightsqlclienturi-headersnone","title":"<code>FlightSQLClient(uri, headers=None)</code>","text":"<p>For advanced connection scenarios, use the <code>FlightSQLClient</code> class directly:</p> <pre><code>from micromegas.flightsql.client import FlightSQLClient\n\n# Connect to remote server with authentication\nclient = FlightSQLClient(\n    \"grpc+tls://remote-server:50051\",\n    headers={\"authorization\": \"Bearer your-token\"}\n)\n\n# Connect to local server (equivalent to micromegas.connect())\nclient = FlightSQLClient(\"grpc://localhost:50051\")\n</code></pre> <p>Parameters: - <code>uri</code> (str): FlightSQL server URI. Use <code>grpc://</code> for unencrypted or <code>grpc+tls://</code> for TLS connections - <code>headers</code> (dict, optional): Custom headers for authentication or metadata</p>"},{"location":"query-guide/python-api/#schema-discovery","title":"Schema Discovery","text":""},{"location":"query-guide/python-api/#prepare_statementsql","title":"<code>prepare_statement(sql)</code>","text":"<p>Get query schema information without executing the query:</p> <pre><code># Prepare statement to discover schema\nstmt = client.prepare_statement(\n    \"SELECT time, level, msg FROM log_entries WHERE level &lt;= 3\"\n)\n\n# Inspect the schema\nprint(\"Query result schema:\")\nfor field in stmt.dataset_schema:\n    print(f\"  {field.name}: {field.type}\")\n\n# Output:\n#   time: timestamp[ns]\n#   level: int32  \n#   msg: string\n\n# The query is also available\nprint(f\"Query: {stmt.query}\")\n</code></pre>"},{"location":"query-guide/python-api/#prepared_statement_streamstatement","title":"<code>prepared_statement_stream(statement)</code>","text":"<p>Execute a prepared statement (mainly useful after schema inspection):</p> <pre><code># Execute the prepared statement\nfor batch in client.prepared_statement_stream(stmt):\n    df = batch.to_pandas()\n    print(f\"Received {len(df)} rows\")\n</code></pre> <p>Note: Prepared statements are primarily for schema discovery. Execution offers no performance benefit over <code>query_stream()</code>.</p>"},{"location":"query-guide/python-api/#process-and-stream-discovery","title":"Process and Stream Discovery","text":""},{"location":"query-guide/python-api/#find_processprocess_id","title":"<code>find_process(process_id)</code>","text":"<p>Find detailed information about a specific process:</p> <pre><code># Find process by ID\nprocess_info = client.find_process('550e8400-e29b-41d4-a716-446655440000')\n\nif not process_info.empty:\n    print(f\"Process: {process_info['exe'].iloc[0]}\")\n    print(f\"Started: {process_info['start_time'].iloc[0]}\")\n    print(f\"Computer: {process_info['computer'].iloc[0]}\")\nelse:\n    print(\"Process not found\")\n</code></pre>"},{"location":"query-guide/python-api/#query_streamsbegin-end-limit-process_idnone-tag_filternone","title":"<code>query_streams(begin, end, limit, process_id=None, tag_filter=None)</code>","text":"<p>Query event streams with filtering:</p> <pre><code># Query all streams from the last hour\nend = datetime.datetime.now(datetime.timezone.utc)\nbegin = end - datetime.timedelta(hours=1)\nstreams = client.query_streams(begin, end, limit=100)\n\n# Filter by process\nprocess_streams = client.query_streams(\n    begin, end, \n    limit=50,\n    process_id='550e8400-e29b-41d4-a716-446655440000'\n)\n\n# Filter by stream tag\nlog_streams = client.query_streams(\n    begin, end,\n    limit=20, \n    tag_filter='log'\n)\n\nprint(f\"Found {len(streams)} total streams\")\nprint(f\"Stream types: {streams['stream_type'].value_counts()}\")\n</code></pre>"},{"location":"query-guide/python-api/#query_blocksbegin-end-limit-stream_id","title":"<code>query_blocks(begin, end, limit, stream_id)</code>","text":"<p>Query data blocks within a stream (for low-level inspection):</p> <pre><code># First find a stream\nstreams = client.query_streams(begin, end, limit=1)\nif not streams.empty:\n    stream_id = streams['stream_id'].iloc[0]\n\n    # Query blocks in that stream\n    blocks = client.query_blocks(begin, end, 100, stream_id)\n    print(f\"Found {len(blocks)} blocks\")\n    print(f\"Total events: {blocks['nb_events'].sum()}\")\n    print(f\"Total size: {blocks['payload_size'].sum()} bytes\")\n</code></pre>"},{"location":"query-guide/python-api/#query_spansbegin-end-limit-stream_id","title":"<code>query_spans(begin, end, limit, stream_id)</code>","text":"<p>Query execution spans for performance analysis:</p> <pre><code># Query spans for detailed performance analysis\nspans = client.query_spans(begin, end, 1000, stream_id)\n\n# Find slowest operations\nslow_spans = spans.nlargest(10, 'duration')\nprint(\"Slowest operations:\")\nfor _, span in slow_spans.iterrows():\n    duration_ms = span['duration'] / 1000000  # Convert nanoseconds to milliseconds\n    print(f\"  {span['name']}: {duration_ms:.2f}ms\")\n\n# Analyze span hierarchy\nroot_spans = spans[spans['parent_span_id'].isna()]\nprint(f\"Found {len(root_spans)} root operations\")\n</code></pre>"},{"location":"query-guide/python-api/#data-management","title":"Data Management","text":""},{"location":"query-guide/python-api/#bulk_ingesttable_name-df","title":"<code>bulk_ingest(table_name, df)</code>","text":"<p>Bulk ingest metadata for replication or administrative tasks:</p> <pre><code>import pandas as pd\n\n# Example: Replicate process metadata\nprocesses_df = pd.DataFrame({\n    'process_id': ['550e8400-e29b-41d4-a716-446655440000'],\n    'exe': ['/usr/bin/myapp'],\n    'username': ['user'],\n    'realname': ['User Name'],\n    'computer': ['hostname'],\n    'distro': ['Ubuntu 22.04'],\n    'cpu_brand': ['Intel Core i7'],\n    'tsc_frequency': [2400000000],\n    'start_time': [datetime.datetime.now(datetime.timezone.utc)],\n    'start_ticks': [1234567890],\n    'insert_time': [datetime.datetime.now(datetime.timezone.utc)],\n    'parent_process_id': [''],\n    'properties': [[]]\n})\n\n# Ingest process metadata\nresult = client.bulk_ingest('processes', processes_df)\nif result:\n    print(f\"Ingested {result.record_count} process records\")\n</code></pre> <p>Supported tables: <code>processes</code>, <code>streams</code>, <code>blocks</code>, <code>payloads</code></p> <p>Note: This method is for metadata replication and administrative tasks. Use the telemetry ingestion service HTTP API for normal data ingestion.</p>"},{"location":"query-guide/python-api/#materialize_partitionsview_set_name-begin-end-partition_delta_seconds","title":"<code>materialize_partitions(view_set_name, begin, end, partition_delta_seconds)</code>","text":"<p>Create materialized partitions for performance optimization:</p> <pre><code># Materialize hourly partitions for the last 24 hours\nend = datetime.datetime.now(datetime.timezone.utc)\nbegin = end - datetime.timedelta(days=1)\n\nclient.materialize_partitions(\n    'log_entries',\n    begin,\n    end,\n    3600  # 1-hour partitions\n)\n# Prints progress messages for each materialized partition\n</code></pre>"},{"location":"query-guide/python-api/#retire_partitionsview_set_name-view_instance_id-begin-end","title":"<code>retire_partitions(view_set_name, view_instance_id, begin, end)</code>","text":"<p>Remove materialized partitions to free up storage:</p> <pre><code># Retire old partitions\nclient.retire_partitions(\n    'log_entries',\n    'process-123-456', \n    begin,\n    end\n)\n# Prints status messages as partitions are retired\n</code></pre> <p>Warning: This operation cannot be undone. Retired partitions must be re-materialized if needed.</p>"},{"location":"query-guide/python-api/#administrative-functions","title":"Administrative Functions","text":"<p>The <code>micromegas.admin</code> module provides administrative functions for schema evolution and partition lifecycle management. These functions are intended for system administrators and should be used with caution.</p>"},{"location":"query-guide/python-api/#list_incompatible_partitionsclient-view_set_namenone","title":"<code>list_incompatible_partitions(client, view_set_name=None)</code>","text":"<p>Lists partitions with schemas incompatible with current view set schemas. These partitions cannot be queried correctly alongside current partitions and should be retired to enable schema evolution.</p> <pre><code>import micromegas\nimport micromegas.admin\n\n# Connect to analytics service\nclient = micromegas.connect()\n\n# List all incompatible partitions across all view sets\nincompatible = micromegas.admin.list_incompatible_partitions(client)\nprint(f\"Found {len(incompatible)} groups of incompatible partitions\")\nprint(f\"Total incompatible partitions: {incompatible['partition_count'].sum()}\")\nprint(f\"Total size to be freed: {incompatible['total_size_bytes'].sum() / (1024**3):.2f} GB\")\n\n# List incompatible partitions for specific view set\nlog_incompatible = micromegas.admin.list_incompatible_partitions(client, 'log_entries')\nprint(f\"Log entries incompatible partitions: {log_incompatible['partition_count'].sum()}\")\n</code></pre> <p>Returns: - <code>view_set_name</code>: Name of the view set - <code>view_instance_id</code>: Instance ID (e.g., process_id or 'global') - <code>incompatible_schema_hash</code>: The old schema hash in the partition - <code>current_schema_hash</code>: The current schema hash from ViewFactory - <code>partition_count</code>: Number of incompatible partitions with this schema - <code>total_size_bytes</code>: Total size in bytes of all incompatible partitions - <code>file_paths</code>: Array of file paths for each incompatible partition</p>"},{"location":"query-guide/python-api/#retire_incompatible_partitionsclient-view_set_namenone","title":"<code>retire_incompatible_partitions(client, view_set_name=None)</code>","text":"<p>Retires partitions with schemas incompatible with current view set schemas. This enables safe schema evolution by cleaning up old schema versions.</p> <pre><code>import micromegas\nimport micromegas.admin\n\nclient = micromegas.connect()\n\n# Preview what would be retired (recommended first step)\npreview = micromegas.admin.list_incompatible_partitions(client, 'log_entries')\nprint(f\"Would retire {preview['partition_count'].sum()} partitions\")\nprint(f\"Would free {preview['total_size_bytes'].sum() / (1024**3):.2f} GB\")\n\n# Retire incompatible partitions for specific view set\nif input(\"Proceed with retirement? (yes/no): \") == \"yes\":\n    result = micromegas.admin.retire_incompatible_partitions(client, 'log_entries')\n    print(f\"Retired {result['partitions_retired'].sum()} partitions\")\n    print(f\"Failed {result['partitions_failed'].sum()} partitions\")\n\n    # Check for any failures\n    for _, row in result.iterrows():\n        if row['partitions_failed'] &gt; 0:\n            print(f\"Failures in {row['view_set_name']}/{row['view_instance_id']}:\")\n            for msg in row['retirement_messages']:\n                if msg.startswith(\"ERROR:\"):\n                    print(f\"  {msg}\")\n</code></pre> <p>Returns: - <code>view_set_name</code>: View set that was processed - <code>view_instance_id</code>: Instance ID of partitions retired - <code>partitions_retired</code>: Count of partitions successfully retired - <code>partitions_failed</code>: Count of partitions that failed to retire - <code>storage_freed_bytes</code>: Total bytes freed from storage - <code>retirement_messages</code>: Array of detailed messages for each retirement attempt</p> <p>Safety Features: - Uses file-path-based retirement for precision targeting - Cannot accidentally retire compatible partitions - Provides detailed error reporting for failures - Allows view-specific filtering to prevent bulk operations</p> <p>\u26a0\ufe0f DESTRUCTIVE OPERATION: This operation is irreversible. Retired partitions will be permanently deleted from metadata and their data files removed from object storage. Always preview with <code>list_incompatible_partitions()</code> before calling this function.</p>"},{"location":"query-guide/python-api/#time-utilities","title":"Time Utilities","text":""},{"location":"query-guide/python-api/#format_datetimevalue-and-parse_time_deltauser_string","title":"<code>format_datetime(value)</code> and <code>parse_time_delta(user_string)</code>","text":"<p>Utility functions for time handling:</p> <pre><code>from micromegas.time import format_datetime, parse_time_delta\n\n# Format datetime for queries\ndt = datetime.datetime.now(datetime.timezone.utc)\nformatted = format_datetime(dt)\nprint(formatted)  # \"2024-01-01T12:00:00+00:00\"\n\n# Parse human-readable time deltas\none_hour = parse_time_delta('1h')\nthirty_minutes = parse_time_delta('30m') \nseven_days = parse_time_delta('7d')\n\n# Use in calculations\nrecent_time = datetime.datetime.now(datetime.timezone.utc) - parse_time_delta('2h')\n</code></pre> <p>Supported units: <code>m</code> (minutes), <code>h</code> (hours), <code>d</code> (days)</p>"},{"location":"query-guide/python-api/#advanced-features","title":"Advanced Features","text":""},{"location":"query-guide/python-api/#query-streaming-benefits","title":"Query Streaming Benefits","text":"<p>Use <code>query_stream()</code> for large datasets to:</p> <ul> <li>Reduce memory usage: Process data in chunks instead of loading everything</li> <li>Improve responsiveness: Start processing before the query completes</li> <li>Handle large results: Query datasets larger than available RAM</li> </ul> <pre><code># Example: Process week of data in batches\ntotal_errors = 0\ntotal_rows = 0\n\nfor batch in client.query_stream(\"\"\"\n    SELECT level, msg FROM log_entries \n    WHERE time &gt;= NOW() - INTERVAL '7 days'\n\"\"\", begin, end):\n    df = batch.to_pandas()\n    errors_in_batch = len(df[df['level'] &lt;= 2])\n\n    total_errors += errors_in_batch\n    total_rows += len(df)\n\n    print(f\"Batch: {len(df)} rows, {errors_in_batch} errors\")\n\nprint(f\"Total: {total_rows} rows, {total_errors} errors\")\n</code></pre>"},{"location":"query-guide/python-api/#flightsql-protocol-benefits","title":"FlightSQL Protocol Benefits","text":"<p>Micromegas uses Apache Arrow FlightSQL for optimal performance:</p> <ul> <li>Columnar data transfer: Orders of magnitude faster than JSON</li> <li>Binary protocol: No serialization/deserialization overhead  </li> <li>Native compression: Efficient network utilization</li> <li>Vectorized operations: Optimized for analytical workloads</li> <li>Zero-copy operations: Direct memory mapping from network buffers</li> </ul>"},{"location":"query-guide/python-api/#connection-configuration_1","title":"Connection Configuration","text":"<pre><code># Connect to local server (default)\nclient = micromegas.connect()\n\n# Connect to a custom endpoint using FlightSQLClient directly\nfrom micromegas.flightsql.client import FlightSQLClient\nclient = FlightSQLClient(\"grpc://remote-server:50051\")\n</code></pre>"},{"location":"query-guide/python-api/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    df = client.query(\"SELECT * FROM log_entries;\", begin, end)\nexcept Exception as e:\n    print(f\"Query failed: {e}\")\n\n# Check for empty results\nif df.empty:\n    print(\"No data found for this time range\")\nelse:\n    print(f\"Found {len(df)} rows\")\n</code></pre>"},{"location":"query-guide/python-api/#performance-tips","title":"Performance Tips","text":""},{"location":"query-guide/python-api/#use-time-ranges","title":"Use Time Ranges","text":"<p>Always specify time ranges for better performance:</p> <pre><code># \u2705 Good - efficient\ndf = client.query(sql, begin, end)\n\n# \u274c Avoid - can be slow\ndf = client.query(sql)\n</code></pre>"},{"location":"query-guide/python-api/#streaming-for-large-results","title":"Streaming for Large Results","text":"<p>Use streaming for queries that might return large datasets:</p> <pre><code># If you expect &gt; 100MB of results, use streaming\nif expected_result_size_mb &gt; 100:\n    for batch in client.query_stream(sql, begin, end):\n        process_batch(batch.to_pandas())\nelse:\n    df = client.query(sql, begin, end)\n    process_dataframe(df)\n</code></pre>"},{"location":"query-guide/python-api/#limit-result-size","title":"Limit Result Size","text":"<p>Add LIMIT clauses for exploratory queries:</p> <pre><code># Good for exploration\ndf = client.query(\"SELECT * FROM log_entries LIMIT 1000;\", begin, end)\n\n# Then remove limit for production queries\ndf = client.query(\"SELECT * FROM log_entries WHERE level &lt;= 2;\", begin, end)\n</code></pre>"},{"location":"query-guide/python-api/#integration-examples","title":"Integration Examples","text":""},{"location":"query-guide/python-api/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Query data\ndf = client.query(\"\"\"\n    SELECT time, name, value \n    FROM measures \n    WHERE name = 'cpu_usage'\n\"\"\", begin, end)\n\n# Plot time series\nplt.figure(figsize=(12, 6))\nplt.plot(df['time'], df['value'])\nplt.title('CPU Usage Over Time')\nplt.xlabel('Time')\nplt.ylabel('CPU Usage %')\nplt.show()\n</code></pre>"},{"location":"query-guide/python-api/#data-pipeline","title":"Data Pipeline","text":"<pre><code>import pandas as pd\n\ndef extract_metrics(process_id, hours=24):\n    \"\"\"Extract metrics for a specific process.\"\"\"\n    end = datetime.datetime.now(datetime.timezone.utc)\n    begin = end - datetime.timedelta(hours=hours)\n\n    sql = f\"\"\"\n        SELECT time, name, value, unit\n        FROM view_instance('measures', '{process_id}')\n        ORDER BY time;\n    \"\"\"\n\n    return client.query(sql, begin, end)\n\ndef analyze_performance(df):\n    \"\"\"Analyze performance metrics.\"\"\"\n    metrics = {}\n    for name in df['name'].unique():\n        data = df[df['name'] == name]['value']\n        metrics[name] = {\n            'mean': data.mean(),\n            'max': data.max(),\n            'min': data.min(),\n            'std': data.std()\n        }\n    return metrics\n\n# Use in pipeline\nprocess_metrics = extract_metrics('my-service-123')\nperformance_summary = analyze_performance(process_metrics)\nprint(performance_summary)\n</code></pre>"},{"location":"query-guide/python-api/#next-steps","title":"Next Steps","text":"<ul> <li>Python API Advanced - Advanced patterns, performance optimization, and specialized tooling</li> <li>Schema Reference - Understand available views and fields</li> <li>Functions Reference - Learn about SQL functions</li> <li>Query Patterns - Common observability query patterns</li> <li>Performance Guide - Optimize your queries</li> </ul>"},{"location":"query-guide/query-patterns/","title":"Query Patterns","text":"<p>Common patterns and examples for querying observability data with Micromegas SQL.</p>"},{"location":"query-guide/query-patterns/#error-tracking-and-debugging","title":"Error Tracking and Debugging","text":""},{"location":"query-guide/query-patterns/#recent-errors","title":"Recent Errors","text":"<pre><code>-- Get all errors from the last hour\nSELECT time, process_id, target, msg\nFROM log_entries\nWHERE level &lt;= 2  -- Fatal and Error\n  AND time &gt;= NOW() - INTERVAL '1 hour'\nORDER BY time DESC;\n</code></pre>"},{"location":"query-guide/query-patterns/#error-trends","title":"Error Trends","text":"<pre><code>-- Hourly error counts for trend analysis\nSELECT\n    date_trunc('hour', time) as hour,\n    COUNT(*) as error_count\nFROM log_entries\nWHERE level &lt;= 2\n  AND time &gt;= NOW() - INTERVAL '24 hours'\nGROUP BY date_trunc('hour', time)\nORDER BY hour;\n</code></pre>"},{"location":"query-guide/query-patterns/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"query-guide/query-patterns/#slow-operations","title":"Slow Operations","text":"<pre><code>-- Find slowest function calls\nSELECT\n    name,\n    AVG(duration) / 1000000.0 as avg_ms,\n    MAX(duration) / 1000000.0 as max_ms,\n    COUNT(*) as call_count\nFROM view_instance('thread_spans', 'my_process')\nWHERE duration &gt; 10000000  -- &gt; 10ms\nGROUP BY name\nORDER BY avg_ms DESC\nLIMIT 10;\n</code></pre>"},{"location":"query-guide/query-patterns/#resource-usage","title":"Resource Usage","text":"<pre><code>-- CPU and memory trends\nSELECT\n    date_trunc('minute', time) as minute,\n    name,\n    AVG(value) as avg_value,\n    unit\nFROM measures\nWHERE name IN ('cpu_usage', 'memory_usage')\n  AND time &gt;= NOW() - INTERVAL '1 hour'\nGROUP BY minute, name, unit\nORDER BY minute, name;\n</code></pre>"},{"location":"query-guide/query-patterns/#async-performance-analysis","title":"Async Performance Analysis","text":""},{"location":"query-guide/query-patterns/#top-level-async-operations","title":"Top-Level Async Operations","text":"<pre><code>-- Find slowest top-level async operations\nSELECT\n    name,\n    AVG(duration_ms) as avg_duration,\n    MAX(duration_ms) as max_duration,\n    COUNT(*) as operation_count\nFROM (\n    SELECT\n        begin_events.name,\n        CAST((end_events.time - begin_events.time) AS BIGINT) / 1000000 as duration_ms\n    FROM\n        (SELECT * FROM view_instance('async_events', 'my_process') WHERE event_type = 'begin' AND depth = 0) begin_events\n    LEFT JOIN\n        (SELECT * FROM view_instance('async_events', 'my_process') WHERE event_type = 'end') end_events\n        ON begin_events.span_id = end_events.span_id\n    WHERE end_events.span_id IS NOT NULL\n)\nGROUP BY name\nORDER BY avg_duration DESC\nLIMIT 10;\n</code></pre>"},{"location":"query-guide/query-patterns/#nested-async-operations","title":"Nested Async Operations","text":"<pre><code>-- Find operations that spawn many async children\nSELECT\n    parent_name,\n    parent_depth,\n    COUNT(*) as child_count,\n    AVG(child_duration_ms) as avg_child_duration\nFROM (\n    SELECT\n        parent.name as parent_name,\n        parent.depth as parent_depth,\n        CAST((child_end.time - child_begin.time) AS BIGINT) / 1000000 as child_duration_ms\n    FROM view_instance('async_events', 'my_process') parent\n    JOIN view_instance('async_events', 'my_process') child_begin\n         ON parent.span_id = child_begin.parent_span_id AND child_begin.event_type = 'begin'\n    JOIN view_instance('async_events', 'my_process') child_end\n         ON child_begin.span_id = child_end.span_id AND child_end.event_type = 'end'\n    WHERE parent.event_type = 'begin'\n)\nGROUP BY parent_name, parent_depth\nHAVING COUNT(*) &gt; 5  -- Operations with many children\nORDER BY child_count DESC;\n</code></pre>"},{"location":"query-guide/query-patterns/#async-operation-timeline","title":"Async Operation Timeline","text":"<pre><code>-- Timeline view of async operations with depth hierarchy\nSELECT\n    time,\n    event_type,\n    name,\n    depth,\n    span_id,\n    parent_span_id,\n    REPEAT('  ', depth) || name as indented_name  -- Visual hierarchy\nFROM view_instance('async_events', 'my_process')\nWHERE time &gt;= NOW() - INTERVAL '10 minutes'\nORDER BY time;\n</code></pre>"},{"location":"query-guide/quick-start/","title":"Quick Start","text":"<p>Get up and running with Micromegas SQL queries in minutes. This guide shows you the essential patterns for querying your observability data.</p>"},{"location":"query-guide/quick-start/#basic-connection","title":"Basic Connection","text":"<p>All Micromegas queries start by connecting to the analytics service:</p> <pre><code>import datetime\nimport micromegas\n\n# Connect to Micromegas analytics service\nclient = micromegas.connect()\n</code></pre> <p>The <code>connect()</code> function connects to the analytics service at <code>grpc://localhost:50051</code>.</p>"},{"location":"query-guide/quick-start/#your-first-query","title":"Your First Query","text":"<p>Let's query recent log entries to see what data is available:</p> <pre><code># Set up time range for queries\nnow = datetime.datetime.now(datetime.timezone.utc)\nbegin = now - datetime.timedelta(hours=1)\nend = now\n\n# Query recent log entries\nsql = \"\"\"\n    SELECT time, process_id, level, target, msg\n    FROM log_entries\n    WHERE level &lt;= 4\n    ORDER BY time DESC\n    LIMIT 10;\n\"\"\"\n\n# Execute the query\nlogs = client.query(sql, begin, end)\nprint(logs)\nprint(f\"Result type: {type(logs)}\")  # pandas.DataFrame\n</code></pre> <p>Key points:</p> <ul> <li>\u26a1 Important: Always specify time range via API parameters (<code>begin</code>, <code>end</code>) for best performance</li> <li>Results are returned as pandas DataFrames</li> <li><code>level &lt;= 4</code> filters to show errors and warnings (see log levels)</li> <li>Use API time parameters instead of SQL time filters for partition elimination</li> </ul>"},{"location":"query-guide/quick-start/#understanding-return-types","title":"Understanding Return Types","text":"<p>All queries return pandas DataFrames:</p> <pre><code># Query returns a pandas DataFrame\nresult = client.query(\"SELECT process_id, exe FROM processes LIMIT 5;\")\n\n# Access DataFrame properties\nprint(f\"Shape: {result.shape}\")\nprint(f\"Columns: {result.columns.tolist()}\")\nprint(f\"Data types:\\n{result.dtypes}\")\n\n# Use pandas operations\nfiltered = result[result['exe'].str.contains('analytics')]\nprint(filtered.head())\n</code></pre> <p>This makes it easy to work with results using the entire pandas ecosystem for analysis, visualization, and data processing.</p>"},{"location":"query-guide/quick-start/#essential-query-patterns","title":"Essential Query Patterns","text":""},{"location":"query-guide/quick-start/#1-process-information","title":"1. Process Information","text":"<p>Get an overview of processes sending telemetry:</p> <pre><code>processes = client.query(\"\"\"\n    SELECT process_id, exe, computer, start_time\n    FROM processes\n    ORDER BY start_time DESC\n    LIMIT 10;\n\"\"\")\nprint(processes)\n</code></pre>"},{"location":"query-guide/quick-start/#2-recent-log-entries","title":"2. Recent Log Entries","text":"<p>Query logs with error filtering:</p> <pre><code>error_logs = client.query(\"\"\"\n    SELECT time, process_id, level, target, msg\n    FROM log_entries\n    WHERE level &lt;= 3  -- Fatal, Error, Warn\n    ORDER BY time DESC\n    LIMIT 50;\n\"\"\", begin, end)\nprint(error_logs)\n</code></pre>"},{"location":"query-guide/quick-start/#3-performance-metrics","title":"3. Performance Metrics","text":"<p>Query numeric measurements:</p> <pre><code>metrics = client.query(\"\"\"\n    SELECT time, process_id, name, value, unit\n    FROM measures\n    WHERE name LIKE '%cpu%'\n    ORDER BY time DESC\n    LIMIT 20;\n\"\"\", begin, end)\nprint(metrics)\n</code></pre>"},{"location":"query-guide/quick-start/#4-process-specific-data","title":"4. Process-Specific Data","text":"<p>Use view instances for better performance when focusing on specific processes:</p> <pre><code>process_id = \"your_process_id_here\"  # Replace with actual process ID\n\nprocess_logs = client.query(f\"\"\"\n    SELECT time, level, target, msg\n    FROM view_instance('log_entries', '{process_id}')\n    WHERE level &lt;= 3\n    ORDER BY time DESC\n    LIMIT 20;\n\"\"\", begin, end)\nprint(process_logs)\n</code></pre>"},{"location":"query-guide/quick-start/#log-levels","title":"Log Levels","text":"<p>Micromegas uses numeric log levels for efficient filtering:</p> Level Name Description 1 Fatal Critical errors that cause application termination 2 Error Errors that don't stop execution but need attention 3 Warn Warning conditions that might cause problems 4 Info Informational messages about normal operation 5 Debug Detailed information for debugging 6 Trace Very detailed tracing information <p>Common filters:</p> <ul> <li><code>level &lt;= 2</code> - Only fatal and error messages</li> <li><code>level &lt;= 3</code> - Fatal, error, and warning messages</li> <li><code>level &lt;= 4</code> - All messages except debug and trace</li> </ul>"},{"location":"query-guide/quick-start/#time-range-best-practices","title":"Time Range Best Practices","text":""},{"location":"query-guide/quick-start/#always-use-time-ranges","title":"Always Use Time Ranges","text":"<pre><code># \u2705 Good - efficient and memory-safe\ndf = client.query(sql, begin_time, end_time)\n\n# \u274c Avoid - can be slow and memory-intensive\ndf = client.query(sql)  # Queries ALL data\n</code></pre>"},{"location":"query-guide/quick-start/#common-time-ranges","title":"Common Time Ranges","text":"<pre><code>now = datetime.datetime.now(datetime.timezone.utc)\n\n# Last hour\nbegin = now - datetime.timedelta(hours=1)\n\n# Last day\nbegin = now - datetime.timedelta(days=1)\n\n# Last week\nbegin = now - datetime.timedelta(weeks=1)\n\n# Custom range\nbegin = datetime.datetime(2024, 1, 1, tzinfo=datetime.timezone.utc)\nend = datetime.datetime(2024, 1, 2, tzinfo=datetime.timezone.utc)\n</code></pre>"},{"location":"query-guide/quick-start/#safe-queries-without-time-ranges","title":"Safe Queries Without Time Ranges","text":"<p>Some queries are safe to run without time ranges because they operate on small metadata tables:</p> <pre><code># Process information (typically small dataset)\nprocesses = client.query(\"SELECT process_id, exe FROM processes LIMIT 10;\")\n\n# Stream metadata\nstreams = client.query(\"SELECT stream_id, process_id FROM streams LIMIT 10;\")\n\n# Count queries (use with caution on large datasets)\ncount = client.query(\"SELECT COUNT(*) FROM log_entries;\")\n</code></pre> <p>Performance Impact</p> <p>Avoid querying <code>log_entries</code>, <code>measures</code>, <code>thread_spans</code>, or <code>async_events</code> without time ranges on production systems with large datasets.</p>"},{"location":"query-guide/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you can run basic queries:</p> <ol> <li>Explore the Python API - Learn about streaming and advanced features</li> <li>Review the Schema - Understand all available fields and data types</li> <li>Try Query Patterns - Common observability query patterns</li> <li>Optimize Performance - Learn to write efficient queries</li> </ol>"},{"location":"query-guide/quick-start/#quick-reference","title":"Quick Reference","text":""},{"location":"query-guide/quick-start/#essential-views","title":"Essential Views","text":"<ul> <li><code>processes</code> - Process metadata</li> <li><code>log_entries</code> - Application logs</li> <li><code>measures</code> - Numeric metrics</li> <li><code>thread_spans</code> - Execution timing</li> <li><code>async_events</code> - Async operation tracking</li> </ul>"},{"location":"query-guide/quick-start/#key-functions","title":"Key Functions","text":"<ul> <li><code>view_instance('view_name', 'process_id')</code> - Process-scoped views</li> <li><code>property_get(properties, 'key')</code> - Extract property values</li> <li><code>make_histogram(start, end, bins, values)</code> - Create histograms with specified range</li> </ul>"},{"location":"query-guide/quick-start/#time-functions","title":"Time Functions","text":"<ul> <li><code>NOW()</code> - Current timestamp</li> <li><code>INTERVAL '1 hour'</code> - Time duration</li> <li><code>date_trunc('hour', time)</code> - Truncate to time boundary</li> </ul>"},{"location":"query-guide/schema-reference/","title":"Schema Reference","text":"<p>This page provides a complete reference to all views, data types, and field definitions available in Micromegas SQL queries.</p>"},{"location":"query-guide/schema-reference/#views-overview","title":"Views Overview","text":"<p>Micromegas organizes telemetry data into several views that can be queried using SQL:</p> View Description Use Cases <code>processes</code> Process metadata and system information System overview, process tracking <code>streams</code> Data stream information within processes Stream debugging, data flow analysis <code>blocks</code> Core telemetry block metadata Low-level data inspection <code>log_entries</code> Application log messages with levels Error tracking, debugging, monitoring <code>log_stats</code> Aggregated log statistics by process, level, and target Log volume analysis, monitoring trends <code>measures</code> Numeric metrics and performance data Performance monitoring, alerting <code>thread_spans</code> Synchronous execution spans and timing Performance profiling, call tracing <code>async_events</code> Asynchronous event lifecycle tracking Async operation monitoring"},{"location":"query-guide/schema-reference/#core-views","title":"Core Views","text":""},{"location":"query-guide/schema-reference/#processes","title":"<code>processes</code>","text":"<p>Contains metadata about processes that have sent telemetry data.</p> Field Type Description <code>process_id</code> <code>Dictionary(Int16, Utf8)</code> Unique identifier for the process <code>exe</code> <code>Dictionary(Int16, Utf8)</code> Executable name <code>username</code> <code>Dictionary(Int16, Utf8)</code> User who ran the process <code>realname</code> <code>Dictionary(Int16, Utf8)</code> Real name of the user <code>computer</code> <code>Dictionary(Int16, Utf8)</code> Computer/hostname <code>distro</code> <code>Dictionary(Int16, Utf8)</code> Operating system distribution <code>cpu_brand</code> <code>Dictionary(Int16, Utf8)</code> CPU brand information <code>tsc_frequency</code> <code>UInt64</code> Time stamp counter frequency <code>start_time</code> <code>Timestamp(Nanosecond)</code> Process start time <code>start_ticks</code> <code>UInt64</code> Process start time in ticks <code>insert_time</code> <code>Timestamp(Nanosecond)</code> When the process data was first inserted <code>parent_process_id</code> <code>Dictionary(Int16, Utf8)</code> Parent process identifier <code>properties</code> <code>Dictionary(Int32, Binary)</code> Additional process metadata (JSONB format) <code>last_update_time</code> <code>Timestamp(Nanosecond)</code> When the process data was last updated <code>last_block_end_ticks</code> <code>Int64</code> Tick count when the last block ended <code>last_block_end_time</code> <code>Timestamp(Nanosecond)</code> Timestamp when the last block ended <p>Example Queries: <pre><code>-- Get all processes from the last day\nSELECT process_id, exe, computer, start_time\nFROM processes\nWHERE start_time &gt;= NOW() - INTERVAL '1 day'\nORDER BY start_time DESC;\n\n-- Find processes by executable name\nSELECT process_id, exe, username, computer\nFROM processes\nWHERE exe LIKE '%analytics%';\n</code></pre></p>"},{"location":"query-guide/schema-reference/#streams","title":"<code>streams</code>","text":"<p>Contains information about data streams within processes.</p> Field Type Description <code>stream_id</code> <code>Dictionary(Int16, Utf8)</code> Unique identifier for the stream <code>process_id</code> <code>Dictionary(Int16, Utf8)</code> Reference to the parent process <code>dependencies_metadata</code> <code>Binary</code> Stream dependency metadata <code>objects_metadata</code> <code>Binary</code> Stream object metadata <code>tags</code> <code>List&lt;Utf8&gt;</code> Stream tags <code>properties</code> <code>Dictionary(Int32, Binary)</code> Stream properties (JSONB format) <code>insert_time</code> <code>Timestamp(Nanosecond)</code> When the stream data was first inserted <code>last_update_time</code> <code>Timestamp(Nanosecond)</code> When the stream data was last updated <p>Example Queries: <pre><code>-- Get streams for a specific process\nSELECT stream_id, tags, properties\nFROM streams\nWHERE process_id = 'my_process_123';\n\n-- Join streams with process information\nSELECT s.stream_id, s.tags, p.exe, p.computer\nFROM streams s\nJOIN processes p ON s.process_id = p.process_id;\n</code></pre></p>"},{"location":"query-guide/schema-reference/#blocks","title":"<code>blocks</code>","text":"<p>Core table containing telemetry block metadata with joined process and stream information.</p> Field Type Description <code>block_id</code> <code>Utf8</code> Unique identifier for the block <code>stream_id</code> <code>Utf8</code> Stream identifier <code>process_id</code> <code>Utf8</code> Process identifier <code>begin_time</code> <code>Timestamp(Nanosecond)</code> Block start time <code>begin_ticks</code> <code>Int64</code> Block start time in ticks <code>end_time</code> <code>Timestamp(Nanosecond)</code> Block end time <code>end_ticks</code> <code>Int64</code> Block end time in ticks <code>nb_objects</code> <code>Int32</code> Number of objects in block <code>object_offset</code> <code>Int64</code> Offset to objects in storage <code>payload_size</code> <code>Int64</code> Size of block payload <code>insert_time</code> <code>Timestamp(Nanosecond)</code> When block was inserted <p>Joined Stream Fields:</p> Field Type Description <code>streams.dependencies_metadata</code> <code>Binary</code> Stream dependency metadata <code>streams.objects_metadata</code> <code>Binary</code> Stream object metadata <code>streams.tags</code> <code>List&lt;Utf8&gt;</code> Stream tags <code>streams.properties</code> <code>Dictionary(Int32, Binary)</code> Stream properties (JSONB format) <code>streams.insert_time</code> <code>Timestamp(Nanosecond)</code> When stream was inserted <p>Joined Process Fields:</p> Field Type Description <code>processes.start_time</code> <code>Timestamp(Nanosecond)</code> Process start time <code>processes.start_ticks</code> <code>Int64</code> Process start ticks <code>processes.tsc_frequency</code> <code>Int64</code> Time stamp counter frequency <code>processes.exe</code> <code>Utf8</code> Executable name <code>processes.username</code> <code>Utf8</code> User who ran the process <code>processes.realname</code> <code>Utf8</code> Real name of the user <code>processes.computer</code> <code>Utf8</code> Computer/hostname <code>processes.distro</code> <code>Utf8</code> Operating system distribution <code>processes.cpu_brand</code> <code>Utf8</code> CPU brand information <code>processes.insert_time</code> <code>Timestamp(Nanosecond)</code> When process was inserted <code>processes.parent_process_id</code> <code>Utf8</code> Parent process identifier <code>processes.properties</code> <code>Dictionary(Int32, Binary)</code> Process properties (JSONB format) <p>Example Queries: <pre><code>-- Analyze block sizes and object counts\nSELECT\n    process_id,\n    AVG(payload_size) as avg_block_size,\n    AVG(nb_objects) as avg_objects_per_block,\n    COUNT(*) as total_blocks\nFROM blocks\nWHERE insert_time &gt;= NOW() - INTERVAL '1 hour'\nGROUP BY process_id;\n</code></pre></p>"},{"location":"query-guide/schema-reference/#observability-data-views","title":"Observability Data Views","text":""},{"location":"query-guide/schema-reference/#log_entries","title":"<code>log_entries</code>","text":"<p>Text-based log entries with levels and structured data.</p> Field Type Description <code>process_id</code> <code>Dictionary(Int16, Utf8)</code> Process identifier <code>stream_id</code> <code>Dictionary(Int16, Utf8)</code> Stream identifier <code>block_id</code> <code>Dictionary(Int16, Utf8)</code> Block identifier <code>insert_time</code> <code>Timestamp(Nanosecond)</code> Block insertion time <code>exe</code> <code>Dictionary(Int16, Utf8)</code> Executable name <code>username</code> <code>Dictionary(Int16, Utf8)</code> User who ran the process <code>computer</code> <code>Dictionary(Int16, Utf8)</code> Computer/hostname <code>time</code> <code>Timestamp(Nanosecond)</code> Log entry timestamp <code>target</code> <code>Dictionary(Int16, Utf8)</code> Module/target <code>level</code> <code>Int32</code> Log level (see Log Levels) <code>msg</code> <code>Utf8</code> Log message <code>properties</code> <code>Dictionary(Int32, Binary)</code> Log-specific properties (JSONB format) <code>process_properties</code> <code>Dictionary(Int32, Binary)</code> Process-specific properties (JSONB format)"},{"location":"query-guide/schema-reference/#log-levels","title":"Log Levels","text":"<p>Micromegas uses numeric log levels for efficient filtering:</p> Level Name Description 1 Fatal Critical errors that cause application termination 2 Error Errors that don't stop execution but need attention 3 Warn Warning conditions that might cause problems 4 Info Informational messages about normal operation 5 Debug Detailed information for debugging 6 Trace Very detailed tracing information <p>Example Queries: <pre><code>-- Get recent error and warning logs\nSELECT time, process_id, level, target, msg\nFROM log_entries\nWHERE level &lt;= 3  -- Fatal, Error, Warn\n  AND time &gt;= NOW() - INTERVAL '1 hour'\nORDER BY time DESC;\n\n-- Count logs by level for a specific process\nSELECT level, COUNT(*) as count\nFROM view_instance('log_entries', 'my_process_123')\nWHERE time &gt;= NOW() - INTERVAL '1 day'\nGROUP BY level\nORDER BY level;\n</code></pre></p>"},{"location":"query-guide/schema-reference/#log_stats","title":"<code>log_stats</code>","text":"<p>Materialized view providing aggregated log statistics by process, minute, level, and target. This view is optimized for analyzing log volume trends and patterns over time.</p> Field Type Description <code>time_bin</code> <code>Timestamp(Nanosecond)</code> 1-minute time bucket for aggregation <code>process_id</code> <code>Dictionary(Int16, Utf8)</code> Process identifier <code>level</code> <code>Int32</code> Log level (see Log Levels) <code>target</code> <code>Dictionary(Int16, Utf8)</code> Module/target that generated the logs <code>count</code> <code>Int64</code> Number of log entries in this aggregation <p>Key Features: - Pre-aggregated by 1-minute intervals for efficient time-series queries - Materialized for fast query performance - Automatically updated as new log data arrives - Daily partitioning for efficient storage and querying</p> <p>Example Queries: <pre><code>-- Analyze log volume trends over the last hour\nSELECT \n    time_bin,\n    SUM(count) as total_logs,\n    SUM(CASE WHEN level &lt;= 2 THEN count ELSE 0 END) as error_count\nFROM log_stats\nWHERE time_bin &gt;= NOW() - INTERVAL '1 hour'\nGROUP BY time_bin\nORDER BY time_bin;\n\n-- Find noisiest modules by log volume\nSELECT \n    target,\n    SUM(count) as total_logs,\n    COUNT(DISTINCT time_bin) as active_minutes\nFROM log_stats\nWHERE time_bin &gt;= NOW() - INTERVAL '1 day'\nGROUP BY target\nORDER BY total_logs DESC\nLIMIT 20;\n\n-- Monitor error rate by process\nSELECT \n    process_id,\n    time_bin,\n    SUM(CASE WHEN level &lt;= 2 THEN count ELSE 0 END) * 100.0 / SUM(count) as error_percentage\nFROM log_stats\nWHERE time_bin &gt;= NOW() - INTERVAL '6 hours'\nGROUP BY process_id, time_bin\nHAVING SUM(count) &gt; 100  -- Filter out low-volume periods\nORDER BY time_bin, error_percentage DESC;\n\n-- Compare log levels distribution\nSELECT \n    level,\n    SUM(count) as total_count,\n    SUM(count) * 100.0 / (SELECT SUM(count) FROM log_stats WHERE time_bin &gt;= NOW() - INTERVAL '1 day') as percentage\nFROM log_stats\nWHERE time_bin &gt;= NOW() - INTERVAL '1 day'\nGROUP BY level\nORDER BY level;\n</code></pre></p>"},{"location":"query-guide/schema-reference/#measures","title":"<code>measures</code>","text":"<p>Numerical measurements and counters.</p> Field Type Description <code>process_id</code> <code>Dictionary(Int16, Utf8)</code> Process identifier <code>stream_id</code> <code>Dictionary(Int16, Utf8)</code> Stream identifier <code>block_id</code> <code>Dictionary(Int16, Utf8)</code> Block identifier <code>insert_time</code> <code>Timestamp(Nanosecond)</code> Block insertion time <code>exe</code> <code>Dictionary(Int16, Utf8)</code> Executable name <code>username</code> <code>Dictionary(Int16, Utf8)</code> User who ran the process <code>computer</code> <code>Dictionary(Int16, Utf8)</code> Computer/hostname <code>time</code> <code>Timestamp(Nanosecond)</code> Measurement timestamp <code>target</code> <code>Dictionary(Int16, Utf8)</code> Module/target <code>name</code> <code>Dictionary(Int16, Utf8)</code> Metric name <code>unit</code> <code>Dictionary(Int16, Utf8)</code> Measurement unit <code>value</code> <code>Float64</code> Metric value <code>properties</code> <code>Dictionary(Int32, Binary)</code> Metric-specific properties (JSONB format) <code>process_properties</code> <code>Dictionary(Int32, Binary)</code> Process-specific properties (JSONB format) <p>Example Queries: <pre><code>-- Get CPU metrics over time\nSELECT time, value, unit\nFROM measures\nWHERE name = 'cpu_usage'\n  AND time &gt;= NOW() - INTERVAL '1 hour'\nORDER BY time;\n\n-- Aggregate memory usage by process\nSELECT\n    process_id,\n    AVG(value) as avg_memory,\n    MAX(value) as peak_memory,\n    unit\nFROM measures\nWHERE name LIKE '%memory%'\n  AND time &gt;= NOW() - INTERVAL '1 hour'\nGROUP BY process_id, unit;\n</code></pre></p>"},{"location":"query-guide/schema-reference/#thread_spans","title":"<code>thread_spans</code>","text":"<p>Derived view for analyzing span durations and hierarchies. Access via <code>view_instance('thread_spans', stream_id)</code>.</p> Field Type Description <code>id</code> <code>Int64</code> Span identifier <code>parent</code> <code>Int64</code> Parent span identifier <code>depth</code> <code>UInt32</code> Nesting depth in call tree <code>hash</code> <code>UInt32</code> Span hash for deduplication <code>begin</code> <code>Timestamp(Nanosecond)</code> Span start time <code>end</code> <code>Timestamp(Nanosecond)</code> Span end time <code>duration</code> <code>Int64</code> Span duration in nanoseconds <code>name</code> <code>Dictionary(Int16, Utf8)</code> Span name (function) <code>target</code> <code>Dictionary(Int16, Utf8)</code> Module/target <code>filename</code> <code>Dictionary(Int16, Utf8)</code> Source file <code>line</code> <code>UInt32</code> Line number <p>Example Queries: <pre><code>-- Get slowest functions in a stream\nSELECT name, AVG(duration) as avg_duration_ns, COUNT(*) as call_count\nFROM view_instance('thread_spans', 'stream_123')\nWHERE duration &gt; 1000000  -- &gt; 1ms\nGROUP BY name\nORDER BY avg_duration_ns DESC\nLIMIT 10;\n\n-- Analyze call hierarchy\nSELECT depth, name, duration\nFROM view_instance('thread_spans', 'stream_123')\nWHERE parent = 42  -- specific parent span\nORDER BY begin;\n</code></pre></p>"},{"location":"query-guide/schema-reference/#async_events","title":"<code>async_events</code>","text":"<p>Asynchronous span events for tracking async operations with call hierarchy depth information.</p> Field Type Description <code>stream_id</code> <code>Dictionary(Int16, Utf8)</code> Thread stream identifier <code>block_id</code> <code>Dictionary(Int16, Utf8)</code> Block identifier <code>time</code> <code>Timestamp(Nanosecond)</code> Event timestamp <code>event_type</code> <code>Dictionary(Int16, Utf8)</code> \"begin\" or \"end\" <code>span_id</code> <code>Int64</code> Async span identifier <code>parent_span_id</code> <code>Int64</code> Parent span identifier <code>depth</code> <code>UInt32</code> Nesting depth in async call hierarchy <code>name</code> <code>Dictionary(Int16, Utf8)</code> Span name (function) <code>filename</code> <code>Dictionary(Int16, Utf8)</code> Source file <code>target</code> <code>Dictionary(Int16, Utf8)</code> Module/target <code>line</code> <code>UInt32</code> Line number <p>Example Queries:</p> <pre><code>-- Find top-level async operations (depth = 0) with performance metrics\nSELECT\n    name,\n    depth,\n    AVG(duration_ms) as avg_duration,\n    COUNT(*) as operation_count\nFROM (\n    SELECT\n        begin_events.name,\n        begin_events.depth,\n        CAST((end_events.time - begin_events.time) AS BIGINT) / 1000000 as duration_ms\n    FROM\n        (SELECT * FROM view_instance('async_events', 'my_process_123') WHERE event_type = 'begin') begin_events\n    LEFT JOIN\n        (SELECT * FROM view_instance('async_events', 'my_process_123') WHERE event_type = 'end') end_events\n        ON begin_events.span_id = end_events.span_id\n    WHERE end_events.span_id IS NOT NULL AND begin_events.depth = 0\n)\nGROUP BY name, depth\nORDER BY avg_duration DESC;\n\n-- Compare performance by call depth\nSELECT\n    depth,\n    COUNT(*) as span_count,\n    AVG(duration_ms) as avg_duration,\n    MIN(duration_ms) as min_duration,\n    MAX(duration_ms) as max_duration\nFROM (\n    SELECT\n        begin_events.depth,\n        CAST((end_events.time - begin_events.time) AS BIGINT) / 1000000 as duration_ms\n    FROM\n        (SELECT * FROM view_instance('async_events', 'my_process_123') WHERE event_type = 'begin') begin_events\n    LEFT JOIN\n        (SELECT * FROM view_instance('async_events', 'my_process_123') WHERE event_type = 'end') end_events\n        ON begin_events.span_id = end_events.span_id\n    WHERE end_events.span_id IS NOT NULL\n)\nGROUP BY depth\nORDER BY depth;\n\n-- Find operations that spawn many nested async calls\nSELECT\n    name,\n    depth,\n    COUNT(*) as nested_count\nFROM view_instance('async_events', 'my_process_123')\nWHERE depth &gt; 0 AND event_type = 'begin'\nGROUP BY name, depth\nHAVING COUNT(*) &gt; 5  -- Functions that create multiple nested async operations\nORDER BY nested_count DESC, depth DESC;\n\n-- Analyze async call hierarchy and parent-child relationships\nSELECT\n    parent.name as parent_operation,\n    parent.depth as parent_depth,\n    child.name as child_operation,\n    child.depth as child_depth,\n    COUNT(*) as relationship_count\nFROM view_instance('async_events', 'my_process_123') parent\nJOIN view_instance('async_events', 'my_process_123') child\n     ON parent.span_id = child.parent_span_id\nWHERE parent.event_type = 'begin' AND child.event_type = 'begin'\nGROUP BY parent.name, parent.depth, child.name, child.depth\nORDER BY relationship_count DESC;\n\n-- Filter async operations by depth level for focused analysis\n-- Shallow operations only (depth &lt;= 2)\nSELECT name, event_type, time, depth, span_id\nFROM view_instance('async_events', 'my_process_123')\nWHERE depth &lt;= 2\nORDER BY time;\n\n-- Deep nested operations only (depth &gt;= 3)\nSELECT name, depth, COUNT(*) as deep_operation_count\nFROM view_instance('async_events', 'my_process_123')\nWHERE depth &gt;= 3 AND event_type = 'begin'\nGROUP BY name, depth\nORDER BY depth DESC, deep_operation_count DESC;\n\n-- Track async operation lifecycle with depth context\nSELECT time, event_type, name, span_id, parent_span_id, depth\nFROM view_instance('async_events', 'my_process_123')\nWHERE span_id = 12345\nORDER BY time;\n</code></pre>"},{"location":"query-guide/schema-reference/#data-types","title":"Data Types","text":""},{"location":"query-guide/schema-reference/#properties","title":"Properties","text":"<p>Key-value pairs stored as dictionary-encoded JSONB with the following structure:</p> <pre><code>-- Properties structure (optimized JSONB format)\nDictionary(Int32, Binary)\n</code></pre> <p>This format provides: - Dictionary compression - Repeated property sets stored once and referenced by index - JSONB efficiency - Native binary JSON format for fast property access - Storage optimization - Significant memory and storage savings over legacy formats</p> <p>Common properties fields:</p> <ul> <li><code>properties</code> - Event-specific metadata (log properties, metric properties)</li> <li><code>process_properties</code> - Process-wide metadata shared across all events from a process</li> </ul> <p>Querying properties: <pre><code>-- Access property values using property_get function (works with all formats)\nSELECT property_get(process_properties, 'thread-name') as thread_name\nFROM log_entries\nWHERE property_get(process_properties, 'thread-name') IS NOT NULL;\n\n-- Count properties using properties_length\nSELECT properties_length(properties) as prop_count\nFROM log_entries\nWHERE properties_length(properties) &gt; 0;\n</code></pre></p> <p>Legacy Support: Micromegas maintains full backward compatibility. Existing queries using <code>property_get()</code> and <code>properties_length()</code> work unchanged with the new JSONB format.</p>"},{"location":"query-guide/schema-reference/#dictionary-compression","title":"Dictionary Compression","text":"<p>Most string fields use dictionary compression (<code>Dictionary(Int16, Utf8)</code>) for storage efficiency:</p> <ul> <li>Reduces storage space for repeated values</li> <li>Improves query performance</li> <li>Transparent to SQL queries - use as normal strings</li> </ul>"},{"location":"query-guide/schema-reference/#timestamps","title":"Timestamps","text":"<p>All time fields use <code>Timestamp(Nanosecond)</code> precision:</p> <ul> <li>Nanosecond resolution for high-precision timing</li> <li>UTC timezone assumed</li> <li>Compatible with standard SQL time functions</li> </ul>"},{"location":"query-guide/schema-reference/#view-relationships","title":"View Relationships","text":"<p>Views can be joined to combine information:</p> <pre><code>-- Join log entries with process information\nSELECT l.time, l.level, l.msg, p.exe, p.computer\nFROM log_entries l\nJOIN processes p ON l.process_id = p.process_id\nWHERE l.level &lt;= 2;  -- Fatal and Error only\n\n-- Join measures with stream information\nSELECT m.time, m.name, m.value, s.tags\nFROM measures m\nJOIN streams s ON m.stream_id = s.stream_id\nWHERE m.name = 'cpu_usage';\n</code></pre>"},{"location":"query-guide/schema-reference/#performance-considerations","title":"Performance Considerations","text":""},{"location":"query-guide/schema-reference/#dictionary-fields","title":"Dictionary Fields","text":"<p>Dictionary-compressed fields are optimized for:</p> <ul> <li>Equality comparisons (<code>field = 'value'</code>)</li> <li>IN clauses (<code>field IN ('val1', 'val2')</code>)</li> <li>LIKE patterns on repeated values</li> </ul>"},{"location":"query-guide/schema-reference/#time-based-queries","title":"Time-based Queries","text":"<p>Always use time ranges for optimal performance: <pre><code>-- Good - uses time index\nWHERE time &gt;= NOW() - INTERVAL '1 hour'\n\n-- Avoid - full table scan\nWHERE level &lt;= 3\n</code></pre></p>"},{"location":"query-guide/schema-reference/#view-instances","title":"View Instances","text":"<p>Use <code>view_instance()</code> for process-specific queries: <pre><code>-- Better performance for single process\nSELECT * FROM view_instance('log_entries', 'process_123')\n\n-- Less efficient for single process\nSELECT * FROM log_entries WHERE process_id = 'process_123'\n</code></pre></p>"},{"location":"query-guide/schema-reference/#next-steps","title":"Next Steps","text":"<ul> <li>Functions Reference - SQL functions available for queries</li> <li>Query Patterns - Common observability query patterns</li> <li>Performance Guide - Optimize your queries for best performance</li> </ul>"},{"location":"unreal/","title":"Unreal Engine Integration","text":"<p>Micromegas provides high-performance observability for Unreal Engine applications through a native integration that captures logs, metrics, and traces with minimal overhead.</p>"},{"location":"unreal/#overview","title":"Overview","text":"<p>The Unreal Engine integration consists of:</p> <ul> <li>MicromegasTracing: Extension to Unreal's Core module providing logging, metrics, and span tracking</li> <li>MicromegasTelemetrySink: Plugin adding HTTP transport for sending telemetry to the ingestion service</li> </ul>"},{"location":"unreal/#key-features","title":"Key Features","text":"<ul> <li>Low Overhead: ~20ns per event, matching the Rust implementation's performance</li> <li>Seamless Integration: Automatically captures existing UE_LOG statements</li> <li>Simple Setup: One header file to include: <code>#include \"MicromegasTracing/Macros.h\"</code></li> <li>Comprehensive Telemetry: Logs, metrics, spans, and crash reporting in a unified system</li> <li>Thread-Safe: Asynchronous delivery without blocking the game thread</li> <li>Context Propagation: Global properties automatically attached to all telemetry</li> </ul>"},{"location":"unreal/#quick-start","title":"Quick Start","text":""},{"location":"unreal/#1-install-the-plugin","title":"1. Install the Plugin","text":"<p>Copy the Unreal modules to your project: - <code>unreal/MicromegasTracing</code> \u2192 Your project's Source folder - <code>unreal/MicromegasTelemetrySink</code> \u2192 Your project's Plugins folder</p>"},{"location":"unreal/#2-initialize-telemetry","title":"2. Initialize Telemetry","text":"<p>In your GameInstance or GameMode:</p> <pre><code>#include \"MicromegasTelemetrySink/MicromegasTelemetrySinkModule.h\"\n\nvoid AMyGameMode::BeginPlay()\n{\n    Super::BeginPlay();\n\n    // Initialize telemetry\n    IMicromegasTelemetrySinkModule::LoadModuleChecked().InitTelemetry(\n        \"https://your-telemetry-server:9000\",  // Your ingestion server\n        MyAuthProvider                         // Your authentication handler\n    );\n}\n</code></pre>"},{"location":"unreal/#3-add-instrumentation","title":"3. Add Instrumentation","text":"<pre><code>#include \"MicromegasTracing/Macros.h\"\n\nvoid AMyActor::Tick(float DeltaTime)\n{\n    // Trace function execution\n    MICROMEGAS_SPAN_FUNCTION(\"Game\");\n\n    // Log an event\n    MICROMEGAS_LOG(\"Game\", MicromegasTracing::LogLevel::Info, \n                   TEXT(\"Actor ticking\"));\n\n    // Record a metric\n    MICROMEGAS_FMETRIC(\"Game\", MicromegasTracing::Verbosity::Med, \n                       TEXT(\"TickTime\"), TEXT(\"ms\"), DeltaTime * 1000);\n}\n</code></pre>"},{"location":"unreal/#4-view-your-data","title":"4. View Your Data","text":"<p>Once your game is running and generating telemetry:</p> <ul> <li>Query your data: Follow the Query Guide to learn SQL querying and Python API usage</li> <li>Visualize traces: Generate Perfetto traces for detailed performance analysis</li> <li>Build dashboards: Create custom analytics and monitoring dashboards</li> </ul>"},{"location":"unreal/#what-gets-captured","title":"What Gets Captured","text":""},{"location":"unreal/#automatic-telemetry","title":"Automatic Telemetry","text":"<ul> <li>UE_LOG statements: All existing Unreal logs are automatically captured</li> <li>Frame metrics: Delta time, frame rate (when MetricPublisher is active)</li> <li>Memory metrics: Physical and virtual memory usage</li> <li>Map changes: Current level/world tracked in context</li> <li>Crashes: Stack traces and context on Windows (requires debug symbols)</li> </ul>"},{"location":"unreal/#manual-instrumentation","title":"Manual Instrumentation","text":"<ul> <li>Custom spans: Track specific operations and their duration</li> <li>Business metrics: Player counts, game state, performance indicators</li> <li>Custom logs: Direct telemetry logging with structured properties</li> <li>Context properties: Session IDs, user IDs, build versions</li> </ul>"},{"location":"unreal/#architecture","title":"Architecture","text":"<pre><code>Game Code\n    \u2193\n[MicromegasTracing Module]\n    \u251c\u2500 Logging API\n    \u251c\u2500 Metrics API\n    \u251c\u2500 Spans API\n    \u2514\u2500 Default Context\n         \u2193\n[MicromegasTelemetrySink Plugin]\n    \u251c\u2500 HTTP Event Sink\n    \u251c\u2500 Flush Monitor\n    \u251c\u2500 Sampling Controller\n    \u2514\u2500 Crash Reporter\n         \u2193\n[Telemetry Ingestion Server]\n    \u251c\u2500 PostgreSQL (metadata)\n    \u2514\u2500 Object Storage (payloads)\n</code></pre>"},{"location":"unreal/#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Detailed setup instructions</li> <li>Instrumentation API - Complete API reference</li> <li>Examples - Common instrumentation patterns</li> </ul>"},{"location":"unreal/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Spans are disabled by default (enable with <code>telemetry.spans.enable 1</code>)</li> <li>Events are buffered in thread-local storage before async delivery</li> <li>The HTTP sink runs on a dedicated thread</li> <li>Use sampling for high-frequency spans to manage data volume</li> <li>Default context operations are expensive - use for infrequent changes</li> </ul>"},{"location":"unreal/#platform-support","title":"Platform Support","text":"<ul> <li>Windows: Full support including crash reporting</li> <li>Linux: Full support</li> <li>Mac: Full support</li> <li>Consoles: Requires network configuration</li> <li>Mobile: Consider battery and bandwidth optimization</li> </ul>"},{"location":"unreal/examples/","title":"Examples","text":"<p>Practical examples of instrumenting common Unreal Engine scenarios with Micromegas.</p>"},{"location":"unreal/examples/#basic-setup","title":"Basic Setup","text":""},{"location":"unreal/examples/#game-instance-initialization","title":"Game Instance Initialization","text":"<p>Complete setup with authentication and default context:</p> <pre><code>// MyGameInstance.h\nUCLASS()\nclass MYGAME_API UMyGameInstance : public UGameInstance\n{\n    GENERATED_BODY()\n\npublic:\n    virtual void Init() override;\n    virtual void Shutdown() override;\n\nprivate:\n    void SetupTelemetryContext();\n};\n\n// MyGameInstance.cpp\n#include \"MyGameInstance.h\"\n#include \"MicromegasTelemetrySink/MicromegasTelemetrySinkModule.h\"\n#include \"MicromegasTracing/Macros.h\"\n#include \"MicromegasTracing/Dispatch.h\"\n#include \"MicromegasTracing/DefaultContext.h\"\n\nvoid UMyGameInstance::Init()\n{\n    Super::Init();\n\n    // Create auth provider\n    class FSimpleAuthProvider : public ITelemetryAuthenticator\n    {\n    public:\n        virtual ~FSimpleAuthProvider() = default;\n\n        virtual void Init(const MicromegasTracing::EventSinkPtr&amp; InSink) override {}\n\n        virtual bool IsReady() override { return true; }\n\n        virtual bool Sign(IHttpRequest&amp; Request) override\n        {\n            FString ApiKey = GetDefault&lt;UGameSettings&gt;()-&gt;TelemetryApiKey;\n            Request.SetHeader(TEXT(\"Authorization\"), TEXT(\"Bearer \") + ApiKey);\n            return true;\n        }\n    };\n\n    // Initialize telemetry\n    auto Auth = MakeShared&lt;FSimpleAuthProvider&gt;();\n    IMicromegasTelemetrySinkModule::LoadModuleChecked().InitTelemetry(\n        TEXT(\"https://telemetry.example.com:9000\"),\n        Auth\n    );\n\n    SetupTelemetryContext();\n\n    MICROMEGAS_LOG(\"Game\", MicromegasTracing::LogLevel::Info, \n                   TEXT(\"Game instance initialized\"));\n}\n\nvoid UMyGameInstance::SetupTelemetryContext()\n{\n    if (auto* Ctx = MicromegasTracing::Dispatch::GetDefaultContext())\n    {\n        // Session info\n        Ctx-&gt;Set(FName(\"session_id\"), FName(*FGuid::NewGuid().ToString()));\n        Ctx-&gt;Set(FName(\"timestamp\"), FName(*FDateTime::UtcNow().ToString()));\n\n        // Build info\n        Ctx-&gt;Set(FName(\"build_version\"), FName(TEXT(GAME_VERSION)));\n        Ctx-&gt;Set(FName(\"build_config\"), FName(TEXT(STRINGIFY(UE_BUILD_CONFIGURATION))));\n\n        // Platform info\n        Ctx-&gt;Set(FName(\"platform\"), FName(*UGameplayStatics::GetPlatformName()));\n        Ctx-&gt;Set(FName(\"cpu\"), FName(*FPlatformMisc::GetCPUBrand()));\n        Ctx-&gt;Set(FName(\"gpu\"), FName(*GRHIAdapterName));\n\n        // Player info (if available)\n        if (ULocalPlayer* LocalPlayer = GetFirstGamePlayer())\n        {\n            Ctx-&gt;Set(FName(\"player_id\"), FName(*LocalPlayer-&gt;GetPreferredUniqueNetId().ToString()));\n        }\n    }\n}\n\nvoid UMyGameInstance::Shutdown()\n{\n    MICROMEGAS_LOG(\"Game\", MicromegasTracing::LogLevel::Info, \n                   TEXT(\"Game instance shutting down\"));\n\n    // Force flush before shutdown\n    MicromegasTracing::Dispatch::FlushLogStream();\n    MicromegasTracing::Dispatch::FlushMetricStream();\n    MicromegasTracing::Dispatch::FlushCurrentThreadStream();\n\n    Super::Shutdown();\n}\n</code></pre>"},{"location":"unreal/examples/#game-loop-instrumentation","title":"Game Loop Instrumentation","text":""},{"location":"unreal/examples/#game-mode-with-performance-metrics","title":"Game Mode with Performance Metrics","text":"<pre><code>// MyGameMode.cpp\nvoid AMyGameMode::Tick(float DeltaSeconds)\n{\n    MICROMEGAS_SPAN_FUNCTION(\"Game.GameMode\");\n\n    Super::Tick(DeltaSeconds);\n\n    // Frame metrics\n    MICROMEGAS_FMETRIC(\"Performance\", MicromegasTracing::Verbosity::Med,\n                       TEXT(\"FrameTime\"), TEXT(\"ms\"), DeltaSeconds * 1000.0f);\n\n    MICROMEGAS_FMETRIC(\"Performance\", MicromegasTracing::Verbosity::Med,\n                       TEXT(\"FPS\"), TEXT(\"fps\"), 1.0f / DeltaSeconds);\n\n    // Game state metrics\n    MICROMEGAS_IMETRIC(\"Game\", MicromegasTracing::Verbosity::Low,\n                       TEXT(\"PlayerCount\"), TEXT(\"count\"), \n                       GetNumPlayers());\n\n    MICROMEGAS_IMETRIC(\"Game\", MicromegasTracing::Verbosity::Low,\n                       TEXT(\"AICount\"), TEXT(\"count\"),\n                       GetWorld()-&gt;GetNumPawns() - GetNumPlayers());\n\n    // Memory metrics (every 60 frames)\n    static int32 FrameCounter = 0;\n    if (++FrameCounter % 60 == 0)\n    {\n        FPlatformMemoryStats MemStats = FPlatformMemory::GetStats();\n        MICROMEGAS_IMETRIC(\"Memory\", MicromegasTracing::Verbosity::Low,\n                           TEXT(\"WorkingSetSize\"), TEXT(\"bytes\"),\n                           MemStats.UsedPhysical);\n    }\n}\n\nvoid AMyGameMode::HandleMatchIsWaitingToStart()\n{\n    MICROMEGAS_LOG(\"Game.Match\", MicromegasTracing::LogLevel::Info,\n                   TEXT(\"Match waiting to start\"));\n    Super::HandleMatchIsWaitingToStart();\n}\n\nvoid AMyGameMode::HandleMatchHasStarted()\n{\n    MICROMEGAS_SPAN_FUNCTION(\"Game.Match\");\n    MICROMEGAS_LOG(\"Game.Match\", MicromegasTracing::LogLevel::Info,\n                   FString::Printf(TEXT(\"Match started on map: %s\"), \n                   *GetWorld()-&gt;GetMapName()));\n\n    // Update context with match info\n    if (auto* Ctx = MicromegasTracing::Dispatch::GetDefaultContext())\n    {\n        Ctx-&gt;Set(FName(\"match_id\"), FName(*FGuid::NewGuid().ToString()));\n        Ctx-&gt;Set(FName(\"map\"), FName(*GetWorld()-&gt;GetMapName()));\n        Ctx-&gt;Set(FName(\"game_mode\"), FName(*GetClass()-&gt;GetName()));\n    }\n\n    Super::HandleMatchHasStarted();\n}\n</code></pre>"},{"location":"unreal/examples/#actor-and-component-lifecycle","title":"Actor and Component Lifecycle","text":""},{"location":"unreal/examples/#instrumented-actor","title":"Instrumented Actor","text":"<pre><code>// MyActor.cpp\n#include \"MicromegasTracing/Macros.h\"\n\nvoid AMyActor::BeginPlay()\n{\n    MICROMEGAS_SPAN_UOBJECT(\"Actor.Lifecycle\", this);\n\n    Super::BeginPlay();\n\n    MICROMEGAS_LOG(\"Actor\", MicromegasTracing::LogLevel::Debug,\n                   FString::Printf(TEXT(\"%s spawned at %s\"), \n                   *GetName(), *GetActorLocation().ToString()));\n\n    // Track actor spawns by class\n    MICROMEGAS_IMETRIC(\"Actor\", MicromegasTracing::Verbosity::Med,\n                       *FString::Printf(TEXT(\"Spawned.%s\"), *GetClass()-&gt;GetName()),\n                       TEXT(\"count\"), 1);\n}\n\nvoid AMyActor::Tick(float DeltaTime)\n{\n    // Only trace tick for important actors\n    if (bIsImportant)\n    {\n        MICROMEGAS_SPAN_UOBJECT(\"Actor.Tick\", this);\n        Super::Tick(DeltaTime);\n\n        // Actor-specific logic...\n    }\n    else\n    {\n        Super::Tick(DeltaTime);\n    }\n}\n\nvoid AMyActor::EndPlay(const EEndPlayReason::Type EndPlayReason)\n{\n    MICROMEGAS_LOG(\"Actor\", MicromegasTracing::LogLevel::Debug,\n                   FString::Printf(TEXT(\"%s destroyed: %s\"), \n                   *GetName(), \n                   *UEnum::GetValueAsString(EndPlayReason)));\n\n    Super::EndPlay(EndPlayReason);\n}\n\nfloat AMyActor::TakeDamage(float DamageAmount, FDamageEvent const&amp; DamageEvent,\n                           AController* EventInstigator, AActor* DamageCauser)\n{\n    float ActualDamage = Super::TakeDamage(DamageAmount, DamageEvent, \n                                           EventInstigator, DamageCauser);\n\n    MICROMEGAS_LOG(\"Combat\", MicromegasTracing::LogLevel::Info,\n                   FString::Printf(TEXT(\"%s took %.1f damage from %s\"), \n                   *GetName(), ActualDamage,\n                   DamageCauser ? *DamageCauser-&gt;GetName() : TEXT(\"Unknown\")));\n\n    MICROMEGAS_FMETRIC(\"Combat\", MicromegasTracing::Verbosity::High,\n                       TEXT(\"DamageDealt\"), TEXT(\"points\"), ActualDamage);\n\n    return ActualDamage;\n}\n</code></pre>"},{"location":"unreal/examples/#player-controller","title":"Player Controller","text":""},{"location":"unreal/examples/#player-actions-and-input","title":"Player Actions and Input","text":"<pre><code>// MyPlayerController.cpp\nvoid AMyPlayerController::BeginPlay()\n{\n    Super::BeginPlay();\n\n    if (auto* Ctx = MicromegasTracing::Dispatch::GetDefaultContext())\n    {\n        // Set player-specific context\n        Ctx-&gt;Set(FName(\"player_name\"), FName(*PlayerState-&gt;GetPlayerName()));\n        Ctx-&gt;Set(FName(\"player_id\"), FName(*GetUniqueID().ToString()));\n    }\n\n    MICROMEGAS_LOG(\"Player\", MicromegasTracing::LogLevel::Info,\n                   FString::Printf(TEXT(\"Player %s joined\"), \n                   *PlayerState-&gt;GetPlayerName()));\n}\n\nvoid AMyPlayerController::SetupInputComponent()\n{\n    Super::SetupInputComponent();\n\n    InputComponent-&gt;BindAction(\"Fire\", IE_Pressed, this, &amp;AMyPlayerController::OnFire);\n    InputComponent-&gt;BindAction(\"Jump\", IE_Pressed, this, &amp;AMyPlayerController::OnJump);\n    InputComponent-&gt;BindAction(\"Interact\", IE_Pressed, this, &amp;AMyPlayerController::OnInteract);\n}\n\nvoid AMyPlayerController::OnFire()\n{\n    MICROMEGAS_LOG(\"Player.Input\", MicromegasTracing::LogLevel::Trace,\n                   TEXT(\"Fire action\"));\n    MICROMEGAS_IMETRIC(\"Player.Actions\", MicromegasTracing::Verbosity::High,\n                       TEXT(\"Fire\"), TEXT(\"count\"), 1);\n\n    // Fire logic...\n}\n\nvoid AMyPlayerController::OnJump()\n{\n    MICROMEGAS_IMETRIC(\"Player.Actions\", MicromegasTracing::Verbosity::High,\n                       TEXT(\"Jump\"), TEXT(\"count\"), 1);\n    // Jump logic...\n}\n\nvoid AMyPlayerController::OnInteract()\n{\n    MICROMEGAS_SPAN_SCOPE(\"Player.Interaction\", \"Interact\");\n\n    FHitResult HitResult;\n    if (GetHitResultUnderCursor(ECC_Pawn, false, HitResult))\n    {\n        if (AActor* HitActor = HitResult.GetActor())\n        {\n            MICROMEGAS_LOG(\"Player.Interaction\", MicromegasTracing::LogLevel::Info,\n                           FString::Printf(TEXT(\"Interacting with %s\"), \n                           *HitActor-&gt;GetName()));\n\n            // Interaction logic...\n        }\n    }\n}\n</code></pre>"},{"location":"unreal/examples/#network-replication","title":"Network Replication","text":""},{"location":"unreal/examples/#network-metrics-and-events","title":"Network Metrics and Events","text":"<pre><code>// MyGameState.cpp\nvoid AMyGameState::Tick(float DeltaSeconds)\n{\n    Super::Tick(DeltaSeconds);\n\n    // Network metrics (every second)\n    TimeSinceLastNetworkUpdate += DeltaSeconds;\n    if (TimeSinceLastNetworkUpdate &gt;= 1.0f)\n    {\n        TimeSinceLastNetworkUpdate = 0.0f;\n\n        if (UNetDriver* NetDriver = GetWorld()-&gt;GetNetDriver())\n        {\n            MICROMEGAS_IMETRIC(\"Network\", MicromegasTracing::Verbosity::Med,\n                               TEXT(\"ClientConnections\"), TEXT(\"count\"),\n                               NetDriver-&gt;ClientConnections.Num());\n\n            MICROMEGAS_IMETRIC(\"Network\", MicromegasTracing::Verbosity::Med,\n                               TEXT(\"TotalNetObjects\"), TEXT(\"count\"),\n                               NetDriver-&gt;GetNetworkObjectList().GetObjects().Num());\n\n            // Bandwidth metrics\n            MICROMEGAS_IMETRIC(\"Network\", MicromegasTracing::Verbosity::Med,\n                               TEXT(\"InBytes\"), TEXT(\"bytes\"),\n                               NetDriver-&gt;InBytes);\n\n            MICROMEGAS_IMETRIC(\"Network\", MicromegasTracing::Verbosity::Med,\n                               TEXT(\"OutBytes\"), TEXT(\"bytes\"),\n                               NetDriver-&gt;OutBytes);\n        }\n    }\n}\n\n// RPC tracking\nvoid AMyGameState::ServerRPC_Implementation(const FString&amp; Data)\n{\n    MICROMEGAS_SPAN_SCOPE(\"Network.RPC\", \"ServerRPC\");\n    MICROMEGAS_IMETRIC(\"Network.RPC\", MicromegasTracing::Verbosity::High,\n                       TEXT(\"ServerCalls\"), TEXT(\"count\"), 1);\n\n    // Process RPC...\n}\n\nvoid AMyGameState::ClientRPC_Implementation(const FString&amp; Data)\n{\n    MICROMEGAS_IMETRIC(\"Network.RPC\", MicromegasTracing::Verbosity::High,\n                       TEXT(\"ClientCalls\"), TEXT(\"count\"), 1);\n\n    // Process RPC...\n}\n\nvoid AMyGameState::OnRep_ReplicatedProperty()\n{\n    MICROMEGAS_IMETRIC(\"Network.Replication\", MicromegasTracing::Verbosity::High,\n                       TEXT(\"PropertyUpdates\"), TEXT(\"count\"), 1);\n}\n</code></pre>"},{"location":"unreal/examples/#asset-loading-and-streaming","title":"Asset Loading and Streaming","text":""},{"location":"unreal/examples/#content-loading-instrumentation","title":"Content Loading Instrumentation","text":"<pre><code>// MyAssetManager.cpp\nvoid UMyAssetManager::LoadAssetAsync(const FString&amp; AssetPath)\n{\n    MICROMEGAS_SPAN_NAME(\"Content.AsyncLoad\", *AssetPath);\n\n    MICROMEGAS_LOG(\"Content\", MicromegasTracing::LogLevel::Debug,\n                   FString::Printf(TEXT(\"Loading asset: %s\"), *AssetPath));\n\n    FStreamableManager&amp; Streamable = UAssetManager::GetStreamableManager();\n    TSharedPtr&lt;FStreamableHandle&gt; Handle = Streamable.RequestAsyncLoad(\n        FSoftObjectPath(AssetPath),\n        FStreamableDelegate::CreateLambda([AssetPath]()\n        {\n            MICROMEGAS_LOG(\"Content\", MicromegasTracing::LogLevel::Debug,\n                           FString::Printf(TEXT(\"Asset loaded: %s\"), *AssetPath));\n\n            MICROMEGAS_IMETRIC(\"Content\", MicromegasTracing::Verbosity::Med,\n                               TEXT(\"AssetsLoaded\"), TEXT(\"count\"), 1);\n        })\n    );\n}\n\nvoid UMyAssetManager::OnLevelStreamingComplete(ULevelStreaming* StreamedLevel)\n{\n    if (StreamedLevel &amp;&amp; StreamedLevel-&gt;GetLoadedLevel())\n    {\n        int64 SizeBytes = StreamedLevel-&gt;GetLoadedLevel()-&gt;GetOutermost()-&gt;GetFileSize();\n\n        MICROMEGAS_LOG(\"Content.Streaming\", MicromegasTracing::LogLevel::Info,\n                       FString::Printf(TEXT(\"Level streamed: %s (%.2f MB)\"), \n                       *StreamedLevel-&gt;GetWorldAssetPackageFName().ToString(),\n                       SizeBytes / (1024.0f * 1024.0f)));\n\n        MICROMEGAS_IMETRIC(\"Content.Streaming\", MicromegasTracing::Verbosity::Low,\n                           TEXT(\"LevelSize\"), TEXT(\"bytes\"), SizeBytes);\n    }\n}\n</code></pre>"},{"location":"unreal/examples/#ai-and-behavior-trees","title":"AI and Behavior Trees","text":""},{"location":"unreal/examples/#ai-controller-instrumentation","title":"AI Controller Instrumentation","text":"<pre><code>// MyAIController.cpp\nvoid AMyAIController::RunBehaviorTree(UBehaviorTree* BTAsset)\n{\n    MICROMEGAS_SPAN_SCOPE(\"AI.BehaviorTree\", \"RunTree\");\n\n    MICROMEGAS_LOG(\"AI\", MicromegasTracing::LogLevel::Debug,\n                   FString::Printf(TEXT(\"Starting behavior tree: %s\"), \n                   *BTAsset-&gt;GetName()));\n\n    return Super::RunBehaviorTree(BTAsset);\n}\n\nvoid AMyAIController::OnMoveCompleted(FAIRequestID RequestID, EPathFollowingResult::Type Result)\n{\n    MICROMEGAS_LOG(\"AI.Movement\", MicromegasTracing::LogLevel::Trace,\n                   FString::Printf(TEXT(\"AI move completed: %s\"), \n                   *UEnum::GetValueAsString(Result)));\n\n    MICROMEGAS_IMETRIC(\"AI.Movement\", MicromegasTracing::Verbosity::High,\n                       TEXT(\"MovesCompleted\"), TEXT(\"count\"), 1);\n\n    Super::OnMoveCompleted(RequestID, Result);\n}\n\n// BTTask instrumentation\nEBTNodeResult::Type UMyBTTask::ExecuteTask(UBehaviorTreeComponent&amp; OwnerComp, uint8* NodeMemory)\n{\n    MICROMEGAS_SPAN_SCOPE(\"AI.BTTask\", GetNodeName());\n\n    EBTNodeResult::Type Result = Super::ExecuteTask(OwnerComp, NodeMemory);\n\n    MICROMEGAS_LOG(\"AI.BehaviorTree\", MicromegasTracing::LogLevel::Trace,\n                   FString::Printf(TEXT(\"Task %s: %s\"), \n                   *GetNodeName(),\n                   *UEnum::GetValueAsString(Result)));\n\n    return Result;\n}\n</code></pre>"},{"location":"unreal/examples/#profiling-critical-paths","title":"Profiling Critical Paths","text":""},{"location":"unreal/examples/#render-thread-instrumentation","title":"Render Thread Instrumentation","text":"<pre><code>// MySceneProxy.cpp\nvoid FMySceneProxy::GetDynamicMeshElements(...)\n{\n    MICROMEGAS_SPAN_SCOPE(\"Render.SceneProxy\", \"GetDynamicMeshElements\");\n\n    // Expensive rendering operations\n    MICROMEGAS_IMETRIC(\"Render\", MicromegasTracing::Verbosity::High,\n                       TEXT(\"DynamicElements\"), TEXT(\"count\"), Elements.Num());\n}\n</code></pre>"},{"location":"unreal/examples/#physics-simulation","title":"Physics Simulation","text":"<pre><code>// MyPhysicsActor.cpp\nvoid AMyPhysicsActor::SimulatePhysics(float DeltaTime)\n{\n    MICROMEGAS_SPAN_FUNCTION(\"Physics.Simulation\");\n\n    double StartTime = FPlatformTime::Seconds();\n\n    // Run physics simulation\n    RunComplexPhysicsSimulation(DeltaTime);\n\n    double SimTime = (FPlatformTime::Seconds() - StartTime) * 1000.0;\n    MICROMEGAS_FMETRIC(\"Physics\", MicromegasTracing::Verbosity::Med,\n                       TEXT(\"SimulationTime\"), TEXT(\"ms\"), SimTime);\n\n    if (SimTime &gt; 16.0) // Longer than a frame\n    {\n        MICROMEGAS_LOG(\"Physics\", MicromegasTracing::LogLevel::Warn,\n                       FString::Printf(TEXT(\"Physics simulation took %.2fms\"), SimTime));\n    }\n}\n</code></pre>"},{"location":"unreal/examples/#error-handling-and-debugging","title":"Error Handling and Debugging","text":""},{"location":"unreal/examples/#comprehensive-error-logging","title":"Comprehensive Error Logging","text":"<pre><code>void UMyGameSubsystem::HandleError(const FString&amp; ErrorContext, const FString&amp; ErrorMessage)\n{\n    // Log the error\n    MICROMEGAS_LOG(\"Error\", MicromegasTracing::LogLevel::Error,\n                   FString::Printf(TEXT(\"[%s] %s\"), *ErrorContext, *ErrorMessage));\n\n    // Track error metrics\n    MICROMEGAS_IMETRIC(\"Errors\", MicromegasTracing::Verbosity::Low,\n                       *FString::Printf(TEXT(\"Error.%s\"), *ErrorContext),\n                       TEXT(\"count\"), 1);\n\n    // Add error to context for correlation\n    if (auto* Ctx = MicromegasTracing::Dispatch::GetDefaultContext())\n    {\n        Ctx-&gt;Set(FName(\"last_error\"), FName(*ErrorMessage));\n        Ctx-&gt;Set(FName(\"error_time\"), FName(*FDateTime::UtcNow().ToString()));\n    }\n\n    // Force flush for critical errors\n    if (IsCriticalError(ErrorContext))\n    {\n        MicromegasTracing::Dispatch::FlushLogStream();\n        MicromegasTracing::Dispatch::FlushMetricStream();\n        MicromegasTracing::Dispatch::FlushCurrentThreadStream();\n    }\n}\n\n// Assertion handler\nvoid CheckGameState(bool bCondition, const FString&amp; Message)\n{\n    if (!bCondition)\n    {\n        MICROMEGAS_LOG(\"Assert\", MicromegasTracing::LogLevel::Fatal,\n                       FString::Printf(TEXT(\"Assertion failed: %s\"), *Message));\n\n        // Flush before potential crash\n        MicromegasTracing::Dispatch::FlushLogStream();\n        MicromegasTracing::Dispatch::FlushMetricStream();\n        MicromegasTracing::Dispatch::FlushCurrentThreadStream();\n\n        check(false);\n    }\n}\n</code></pre>"},{"location":"unreal/installation/","title":"Installation Guide","text":"<p>This guide walks through installing and configuring the Micromegas Unreal Engine integration.</p>"},{"location":"unreal/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Unreal Engine 4.27+ or 5.0+</li> <li>Visual Studio 2019 or 2022 (Windows)</li> <li>Xcode (Mac)</li> <li>A running Micromegas ingestion server</li> </ul>"},{"location":"unreal/installation/#standard-installation","title":"Standard Installation","text":""},{"location":"unreal/installation/#step-1-copy-the-modules","title":"Step 1: Copy the Modules","text":"<ol> <li> <p>Copy the Core module extension to Unreal's Core module:    <pre><code>micromegas/unreal/MicromegasTracing/Public/MicromegasTracing \u2192 \nYourUnrealEngine/Engine/Source/Runtime/Core/Public/MicromegasTracing\n\nmicromegas/unreal/MicromegasTracing/Private \u2192\nYourUnrealEngine/Engine/Source/Runtime/Core/Private/MicromegasTracing\n</code></pre></p> </li> <li> <p>Copy the plugin to your project:    <pre><code>micromegas/unreal/MicromegasTelemetrySink \u2192 YourProject/Plugins/MicromegasTelemetrySink\n</code></pre></p> </li> </ol>"},{"location":"unreal/installation/#step-2-configure-build-dependencies","title":"Step 2: Configure Build Dependencies","text":"<p>Since MicromegasTracing is now part of the Core module, you only need to add the plugin:</p> <pre><code>// YourGame.Build.cs\npublic class YourGame : ModuleRules\n{\n    public YourGame(ReadOnlyTargetRules Target) : base(Target)\n    {\n        PublicDependencyModuleNames.AddRange(new string[] { \n            \"Core\",  // MicromegasTracing is now included in Core\n            \"CoreUObject\", \n            \"Engine\"\n        });\n\n        PrivateDependencyModuleNames.AddRange(new string[] {\n            \"MicromegasTelemetrySink\"  // Add this plugin\n        });\n    }\n}\n</code></pre>"},{"location":"unreal/installation/#step-3-enable-the-plugin","title":"Step 3: Enable the Plugin","text":"<p>Either:</p> <ul> <li>Via Editor: Go to Edit \u2192 Plugins \u2192 Search for \"MicromegasTelemetrySink\" \u2192 Enable</li> <li>Via .uproject: Add to the Plugins section:   <pre><code>{\n  \"Name\": \"MicromegasTelemetrySink\",\n  \"Enabled\": true\n}\n</code></pre></li> </ul>"},{"location":"unreal/installation/#step-4-build-the-project","title":"Step 4: Build the Project","text":"<p>Regenerate project files and build:</p> <ol> <li>Right-click your <code>.uproject</code> file \u2192 \"Generate Visual Studio project files\"</li> <li>Open the solution in Visual Studio/Xcode</li> <li>Build the project</li> </ol>"},{"location":"unreal/installation/#development-setup-windows","title":"Development Setup (Windows)","text":"<p>For active development on Micromegas while testing in Unreal, use hard links to avoid copying files:</p>"},{"location":"unreal/installation/#step-1-set-environment-variables","title":"Step 1: Set Environment Variables","text":"<pre><code>set MICROMEGAS_UNREAL_ROOT_DIR=C:\\Program Files\\Epic Games\\UE_5.3\nset MICROMEGAS_UNREAL_TELEMETRY_MODULE_DIR=C:\\YourProject\\Plugins\n</code></pre>"},{"location":"unreal/installation/#step-2-run-the-hard-link-script","title":"Step 2: Run the Hard Link Script","text":"<pre><code>cd micromegas/build\npython unreal_hard_link_windows.py\n</code></pre> <p>This creates hard links that:</p> <ul> <li>Link MicromegasTracing into Unreal Engine's Core module</li> <li>Link MicromegasTelemetrySink plugin to your project  </li> <li>Allow you to edit Micromegas source and see changes immediately in Unreal</li> </ul>"},{"location":"unreal/installation/#initial-configuration","title":"Initial Configuration","text":""},{"location":"unreal/installation/#basic-setup","title":"Basic Setup","text":"<p>In your <code>GameInstance</code> or <code>GameMode</code> class:</p> <pre><code>// YourGameInstance.h\n#pragma once\n#include \"Engine/GameInstance.h\"\n#include \"YourGameInstance.generated.h\"\n\nUCLASS()\nclass YOURGAME_API UYourGameInstance : public UGameInstance\n{\n    GENERATED_BODY()\n\npublic:\n    virtual void Init() override;\n};\n\n// YourGameInstance.cpp\n#include \"YourGameInstance.h\"\n#include \"MicromegasTelemetrySink/MicromegasTelemetrySinkModule.h\"\n#include \"MicromegasTracing/Dispatch.h\"\n#include \"MicromegasTracing/DefaultContext.h\"\n\nvoid UYourGameInstance::Init()\n{\n    Super::Init();\n\n    // Initialize telemetry\n    FString ServerUrl = TEXT(\"https://telemetry.yourcompany.com:9000\");\n\n    // Create authentication provider (implement your auth logic)\n    auto AuthProvider = MakeShared&lt;FMyTelemetryAuthenticator&gt;();\n\n    // Initialize the sink\n    IMicromegasTelemetrySinkModule::LoadModuleChecked().InitTelemetry(\n        ServerUrl, \n        AuthProvider\n    );\n\n    // Set default context properties\n    if (auto* Ctx = MicromegasTracing::Dispatch::GetDefaultContext())\n    {\n        Ctx-&gt;Set(FName(\"build_version\"), FName(TEXT(\"1.0.0\")));\n        Ctx-&gt;Set(FName(\"platform\"), FName(*UGameplayStatics::GetPlatformName()));\n        Ctx-&gt;Set(FName(\"session_id\"), FName(*FGuid::NewGuid().ToString()));\n    }\n\n    UE_LOG(LogTemp, Log, TEXT(\"Telemetry initialized\"));\n}\n</code></pre>"},{"location":"unreal/installation/#authentication-provider","title":"Authentication Provider","text":"<p>Implement the authentication interface:</p> <pre><code>// MyTelemetryAuthenticator.h\n#pragma once\n#include \"MicromegasTelemetrySink/TelemetryAuthenticator.h\"\n#include \"Interfaces/IHttpRequest.h\"\n\nclass FMyTelemetryAuthenticator : public ITelemetryAuthenticator\n{\npublic:\n    virtual ~FMyTelemetryAuthenticator() = default;\n\n    virtual void Init(const MicromegasTracing::EventSinkPtr&amp; InSink) override\n    {\n        // Initialize authenticator if needed\n    }\n\n    virtual bool IsReady() override\n    {\n        // Return true when authentication is ready\n        return true;\n    }\n\n    virtual bool Sign(IHttpRequest&amp; Request) override\n    {\n        // Add authentication to the HTTP request\n        Request.SetHeader(TEXT(\"Authorization\"), TEXT(\"Bearer your-api-key-here\"));\n        return true;\n    }\n};\n</code></pre>"},{"location":"unreal/installation/#configuration-options","title":"Configuration Options","text":""},{"location":"unreal/installation/#compile-time-settings","title":"Compile-Time Settings","text":"<p>In <code>MicromegasTelemetrySinkModule.cpp</code>:</p> <pre><code>// Enable telemetry on startup (default: 1)\n#define MICROMEGAS_ENABLE_TELEMETRY_ON_START 1\n\n// Enable crash reporting on Windows (default: 1 on Windows, 0 elsewhere)\n#define MICROMEGAS_CRASH_REPORTING 1\n</code></pre>"},{"location":"unreal/installation/#runtime-console-commands","title":"Runtime Console Commands","text":"<p>Available console commands for runtime control:</p> <ul> <li><code>telemetry.enable</code> - Initialize the telemetry system</li> <li><code>telemetry.flush</code> - Force flush all pending events</li> <li><code>telemetry.spans.enable 1</code> - Enable span recording (disabled by default)</li> <li><code>telemetry.spans.enable 0</code> - Disable span recording</li> <li><code>telemetry.spans.all 1</code> - Record all spans without sampling</li> </ul>"},{"location":"unreal/installation/#verifying-installation","title":"Verifying Installation","text":""},{"location":"unreal/installation/#test-basic-logging","title":"Test Basic Logging","text":"<p>Add to any Actor or GameMode:</p> <pre><code>#include \"MicromegasTracing/Macros.h\"\n\nvoid ATestActor::BeginPlay()\n{\n    Super::BeginPlay();\n\n    // This should appear in your telemetry\n    MICROMEGAS_LOG(\"Test\", MicromegasTracing::LogLevel::Info, \n                   TEXT(\"Micromegas telemetry is working!\"));\n\n    // This metric should be recorded\n    MICROMEGAS_IMETRIC(\"Test\", MicromegasTracing::Verbosity::Med, \n                       TEXT(\"TestCounter\"), TEXT(\"count\"), 1);\n}\n</code></pre>"},{"location":"unreal/installation/#check-server-connection","title":"Check Server Connection","text":"<ol> <li>Run your game in the editor</li> <li>Open the console (` key)</li> <li>Type: <code>telemetry.flush</code></li> <li>Check your ingestion server logs for incoming requests</li> <li>Query your data using the Python client or CLI tools</li> </ol>"},{"location":"unreal/installation/#platform-specific-notes","title":"Platform-Specific Notes","text":""},{"location":"unreal/installation/#windows","title":"Windows","text":"<ul> <li>Crash reporting is enabled by default</li> <li>Requires debug symbols (<code>.pdb</code> files) for meaningful stack traces</li> <li>Windows Defender may flag the first network connection - add an exception if needed</li> </ul>"},{"location":"unreal/installation/#linux","title":"Linux","text":"<ul> <li>Ensure your ingestion server is accessible from the game server</li> <li>Check firewall rules for port 9000 (or your configured port)</li> </ul>"},{"location":"unreal/installation/#mac","title":"Mac","text":"<ul> <li>Code signing may be required for shipping builds</li> <li>Network permissions needed for telemetry upload</li> </ul>"},{"location":"unreal/installation/#consoles","title":"Consoles","text":"<ul> <li>Special network configuration required</li> <li>Contact your platform representative for network policy compliance</li> </ul>"},{"location":"unreal/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Instrumentation API - Learn about logging, metrics, and spans</li> <li>Examples - See common usage patterns</li> </ul>"},{"location":"unreal/instrumentation-api/","title":"Instrumentation API Reference","text":"<p>Complete reference for the Micromegas Unreal Engine instrumentation API.</p>"},{"location":"unreal/instrumentation-api/#header-file","title":"Header File","text":"<p>All instrumentation macros are available by including:</p> <pre><code>#include \"MicromegasTracing/Macros.h\"\n</code></pre>"},{"location":"unreal/instrumentation-api/#logging-api","title":"Logging API","text":""},{"location":"unreal/instrumentation-api/#micromegas_log","title":"MICROMEGAS_LOG","text":"<p>Records a log entry with dynamic string content.</p> <pre><code>MICROMEGAS_LOG(target, level, message)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target</code> (const char*): Log target/category (e.g., \"Game\", \"Network\", \"AI\")</li> <li><code>level</code> (MicromegasTracing::LogLevel): Severity level</li> <li><code>message</code> (FString): The log message</li> </ul> <p>Log Levels:</p> <ul> <li><code>MicromegasTracing::LogLevel::Fatal</code> - Critical errors causing shutdown</li> <li><code>MicromegasTracing::LogLevel::Error</code> - Errors requiring attention</li> <li><code>MicromegasTracing::LogLevel::Warn</code> - Warning conditions</li> <li><code>MicromegasTracing::LogLevel::Info</code> - Informational messages</li> <li><code>MicromegasTracing::LogLevel::Debug</code> - Debug information</li> <li><code>MicromegasTracing::LogLevel::Trace</code> - Detailed trace information</li> </ul> <p>Example: <pre><code>MICROMEGAS_LOG(\"Game\", MicromegasTracing::LogLevel::Info, \n               TEXT(\"Player connected\"));\n\nMICROMEGAS_LOG(\"Network\", MicromegasTracing::LogLevel::Error,\n               FString::Printf(TEXT(\"Connection failed: %s\"), *ErrorMessage));\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#micromegas_log_properties","title":"MICROMEGAS_LOG_PROPERTIES","text":"<p>Records a log entry with additional structured properties.</p> <pre><code>MICROMEGAS_LOG_PROPERTIES(target, level, properties, message)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target</code> (const char*): Log target/category</li> <li><code>level</code> (MicromegasTracing::LogLevel): Severity level</li> <li><code>properties</code> (PropertySet*): Additional key-value properties</li> <li><code>message</code> (FString): The log message</li> </ul> <p>Example: <pre><code>PropertySet* Props = CreatePropertySet();\nProps-&gt;Add(\"player_id\", \"12345\");\nProps-&gt;Add(\"action\", \"login\");\n\nMICROMEGAS_LOG_PROPERTIES(\"Game\", MicromegasTracing::LogLevel::Info, \n                         Props, TEXT(\"Player action recorded\"));\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#ue_log-integration","title":"UE_LOG Integration","text":"<p>All existing <code>UE_LOG</code> statements are automatically captured by Micromegas when the log interop is initialized. No code changes required.</p> <pre><code>// These are automatically sent to telemetry\nUE_LOG(LogTemp, Warning, TEXT(\"This is captured by Micromegas\"));\nUE_LOG(LogGameMode, Error, TEXT(\"So is this\"));\n</code></pre>"},{"location":"unreal/instrumentation-api/#metrics-api","title":"Metrics API","text":""},{"location":"unreal/instrumentation-api/#micromegas_imetric","title":"MICROMEGAS_IMETRIC","text":"<p>Records an integer metric value.</p> <pre><code>MICROMEGAS_IMETRIC(target, level, name, unit, expression)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target</code> (const char*): Metric target/category</li> <li><code>level</code> (MicromegasTracing::Verbosity): Verbosity level</li> <li><code>name</code> (const TCHAR*): Metric name</li> <li><code>unit</code> (const TCHAR*): Unit of measurement</li> <li><code>expression</code> (int64): Value or expression to record</li> </ul> <p>Verbosity Levels:</p> <ul> <li><code>MicromegasTracing::Verbosity::Low</code> - Critical metrics only</li> <li><code>MicromegasTracing::Verbosity::Med</code> - Standard metrics</li> <li><code>MicromegasTracing::Verbosity::High</code> - Detailed metrics</li> </ul> <p>Common Units:</p> <ul> <li><code>TEXT(\"count\")</code> - Simple counter</li> <li><code>TEXT(\"bytes\")</code> - Memory/data size</li> <li><code>TEXT(\"ms\")</code> - Milliseconds</li> <li><code>TEXT(\"percent\")</code> - Percentage (0-100)</li> <li><code>TEXT(\"ticks\")</code> - Will be automatically converted into nanoseconds</li> </ul> <p>Example: <pre><code>MICROMEGAS_IMETRIC(\"Game\", MicromegasTracing::Verbosity::Med,\n                   TEXT(\"PlayerCount\"), TEXT(\"count\"), \n                   GetWorld()-&gt;GetNumPlayerControllers());\n\nMICROMEGAS_IMETRIC(\"Memory\", MicromegasTracing::Verbosity::Low,\n                   TEXT(\"TextureMemory\"), TEXT(\"bytes\"),\n                   GetTextureMemoryUsage());\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#micromegas_fmetric","title":"MICROMEGAS_FMETRIC","text":"<p>Records a floating-point metric value.</p> <pre><code>MICROMEGAS_FMETRIC(target, level, name, unit, expression)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target</code> (const char*): Metric target/category</li> <li><code>level</code> (MicromegasTracing::Verbosity): Verbosity level</li> <li><code>name</code> (const TCHAR*): Metric name</li> <li><code>unit</code> (const TCHAR*): Unit of measurement</li> <li><code>expression</code> (double): Value or expression to record</li> </ul> <p>Example: <pre><code>MICROMEGAS_FMETRIC(\"Performance\", MicromegasTracing::Verbosity::Med,\n                   TEXT(\"FrameTime\"), TEXT(\"ms\"), \n                   DeltaTime * 1000.0);\n\nMICROMEGAS_FMETRIC(\"Game\", MicromegasTracing::Verbosity::High,\n                   TEXT(\"HealthPercent\"), TEXT(\"percent\"),\n                   (Health / MaxHealth) * 100.0);\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#spanstracing-api","title":"Spans/Tracing API","text":"<p>Important: Spans are disabled by default. Enable with console command: <code>telemetry.spans.enable 1</code>. Use reasonable sampling strategy for high-frequency spans.</p>"},{"location":"unreal/instrumentation-api/#micromegas_span_function","title":"MICROMEGAS_SPAN_FUNCTION","text":"<p>Traces the current function's execution time using the function name as the span name.</p> <pre><code>MICROMEGAS_SPAN_FUNCTION(target)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target</code> (const char*): Span target/category</li> </ul> <p>Example: <pre><code>void AMyActor::ComplexCalculation()\n{\n    MICROMEGAS_SPAN_FUNCTION(\"Game.Physics\");\n    // Function is automatically traced\n    // ... complex physics calculations ...\n}\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#micromegas_span_scope","title":"MICROMEGAS_SPAN_SCOPE","text":"<p>Creates a named scope span with a static name.</p> <pre><code>MICROMEGAS_SPAN_SCOPE(target, name)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target</code> (const char*): Span target/category</li> <li><code>name</code> (const char*): Static span name</li> </ul> <p>Example: <pre><code>void ProcessAI()\n{\n    {\n        MICROMEGAS_SPAN_SCOPE(\"AI\", \"Pathfinding\");\n        // ... pathfinding code ...\n    }\n\n    {\n        MICROMEGAS_SPAN_SCOPE(\"AI\", \"DecisionTree\");\n        // ... decision tree evaluation ...\n    }\n}\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#micromegas_span_name","title":"MICROMEGAS_SPAN_NAME","text":"<p>Creates a span with a dynamic name (must be statically allocated).</p> <pre><code>MICROMEGAS_SPAN_NAME(target, name_expression)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target</code> (const char*): Span target/category</li> <li><code>name_expression</code>: Expression returning a statically allocated string (e.g., FName)</li> </ul> <p>Example: <pre><code>void ProcessAsset(const FString&amp; AssetPath)\n{\n    FName AssetName(*AssetPath);\n    MICROMEGAS_SPAN_NAME(\"Content\", AssetName);\n    // ... process asset ...\n}\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#micromegas_span_uobject","title":"MICROMEGAS_SPAN_UOBJECT","text":"<p>Creates a span named after a UObject.</p> <pre><code>MICROMEGAS_SPAN_UOBJECT(target, object)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target</code> (const char*): Span target/category</li> <li><code>object</code> (UObject*): The UObject whose name to use</li> </ul> <p>Example: <pre><code>void AMyActor::Tick(float DeltaTime)\n{\n    MICROMEGAS_SPAN_UOBJECT(\"Game.Actors\", this);\n    Super::Tick(DeltaTime);\n    // ... tick logic ...\n}\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#micromegas_span_name_conditional","title":"MICROMEGAS_SPAN_NAME_CONDITIONAL","text":"<p>Creates a span conditionally.</p> <pre><code>MICROMEGAS_SPAN_NAME_CONDITIONAL(target, condition, name)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target</code> (const char*): Span target/category</li> <li><code>condition</code> (bool): Whether to create the span</li> <li><code>name</code>: Span name if condition is true</li> </ul> <p>Example: <pre><code>void RenderFrame(bool bDetailedProfiling)\n{\n    MICROMEGAS_SPAN_NAME_CONDITIONAL(\"Render\", bDetailedProfiling, \n                                      TEXT(\"DetailedFrame\"));\n    // ... rendering code ...\n}\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#default-context-api","title":"Default Context API","text":"<p>The Default Context allows setting global properties that are automatically attached to all telemetry.</p>"},{"location":"unreal/instrumentation-api/#accessing-the-default-context","title":"Accessing the Default Context","text":"<pre><code>MicromegasTracing::DefaultContext* Ctx = \n    MicromegasTracing::Dispatch::GetDefaultContext();\n</code></pre>"},{"location":"unreal/instrumentation-api/#set","title":"Set","text":"<p>Adds or updates a context property.</p> <pre><code>void Set(FName Key, FName Value)\n</code></pre> <p>Example: <pre><code>if (auto* Ctx = MicromegasTracing::Dispatch::GetDefaultContext())\n{\n    Ctx-&gt;Set(FName(\"user_id\"), FName(*UserId));\n    Ctx-&gt;Set(FName(\"session_id\"), FName(*SessionId));\n    Ctx-&gt;Set(FName(\"map\"), FName(*GetWorld()-&gt;GetMapName()));\n}\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#unset","title":"Unset","text":"<p>Removes a context property.</p> <pre><code>void Unset(FName Key)\n</code></pre> <p>Example: <pre><code>Ctx-&gt;Unset(FName(\"temp_flag\"));\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#clear","title":"Clear","text":"<p>Removes all context properties.</p> <pre><code>void Clear()\n</code></pre> <p>Example: <pre><code>// Clear context on logout\nCtx-&gt;Clear();\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#copy","title":"Copy","text":"<p>Copies current context to a map.</p> <pre><code>void Copy(TMap&lt;FName, FName&gt;&amp; Out) const\n</code></pre> <p>Example: <pre><code>TMap&lt;FName, FName&gt; CurrentContext;\nCtx-&gt;Copy(CurrentContext);\n// Examine or log current context\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#console-commands","title":"Console Commands","text":"<p>Runtime control commands available in the Unreal console:</p>"},{"location":"unreal/instrumentation-api/#telemetryenable","title":"telemetry.enable","text":"<p>Initializes the telemetry system if not already enabled.</p> <pre><code>telemetry.enable\n</code></pre>"},{"location":"unreal/instrumentation-api/#telemetryflush","title":"telemetry.flush","text":"<p>Forces immediate flush of all pending telemetry events.</p> <pre><code>telemetry.flush\n</code></pre>"},{"location":"unreal/instrumentation-api/#telemetryspansenable","title":"telemetry.spans.enable","text":"<p>Enables or disables span recording.</p> <pre><code>telemetry.spans.enable 1  // Enable spans\ntelemetry.spans.enable 0  // Disable spans\n</code></pre>"},{"location":"unreal/instrumentation-api/#telemetryspansall","title":"telemetry.spans.all","text":"<p>Enables recording of all spans without sampling.</p> <pre><code>telemetry.spans.all 1  // Record all spans\ntelemetry.spans.all 0  // Use sampling\n</code></pre>"},{"location":"unreal/instrumentation-api/#best-practices","title":"Best Practices","text":""},{"location":"unreal/instrumentation-api/#performance","title":"Performance","text":"<ol> <li>Use sampling for high-frequency spans - It's OK to keep spans enabled with reasonable sampling</li> <li>Use appropriate verbosity - Lower verbosity for high-frequency metrics</li> <li>Batch operations - Let the system batch; avoid frequent flushes</li> <li>Static strings - Use TEXT() macro for string literals</li> <li>Limit context cardinality - Context keys/values are never freed</li> </ol>"},{"location":"unreal/instrumentation-api/#error-handling","title":"Error Handling","text":"<p>Always check for null pointers when using the context API:</p> <pre><code>if (auto* Ctx = MicromegasTracing::Dispatch::GetDefaultContext())\n{\n    // Safe to use Ctx\n    Ctx-&gt;Set(FName(\"key\"), FName(\"value\"));\n}\n</code></pre>"},{"location":"unreal/instrumentation-api/#thread-safety","title":"Thread Safety","text":"<p>All Micromegas APIs are thread-safe and can be called from any thread:</p> <pre><code>// Safe from game thread\nMICROMEGAS_LOG(\"Game\", MicromegasTracing::LogLevel::Info, TEXT(\"Game thread\"));\n\n// Safe from render thread\nMICROMEGAS_LOG(\"Render\", MicromegasTracing::LogLevel::Info, TEXT(\"Render thread\"));\n\n// Safe from worker threads\nParallelFor(NumItems, [](int32 Index)\n{\n    MICROMEGAS_IMETRIC(\"Worker\", MicromegasTracing::Verbosity::High,\n                       TEXT(\"ItemProcessed\"), TEXT(\"count\"), 1);\n});\n</code></pre>"},{"location":"unreal/instrumentation-api/#integration-examples","title":"Integration Examples","text":""},{"location":"unreal/instrumentation-api/#with-gameplay-abilities","title":"With Gameplay Abilities","text":"<pre><code>void UMyGameplayAbility::ActivateAbility(...)\n{\n    MICROMEGAS_SPAN_NAME(\"Abilities\", GetFName());\n    MICROMEGAS_LOG(\"Abilities\", MicromegasTracing::LogLevel::Info,\n                   FString::Printf(TEXT(\"Ability %s activated\"), *GetName()));\n\n    Super::ActivateAbility(...);\n}\n</code></pre>"},{"location":"unreal/instrumentation-api/#with-animation","title":"With Animation","text":"<pre><code>void UAnimInstance::NativeUpdateAnimation(float DeltaSeconds)\n{\n    MICROMEGAS_SPAN_FUNCTION(\"Animation\");\n    MICROMEGAS_FMETRIC(\"Animation\", MicromegasTracing::Verbosity::High,\n                       TEXT(\"UpdateTime\"), TEXT(\"ms\"), DeltaSeconds * 1000);\n\n    Super::NativeUpdateAnimation(DeltaSeconds);\n}\n</code></pre>"},{"location":"unreal/instrumentation-api/#with-networking","title":"With Networking","text":"<pre><code>void AMyPlayerController::ClientRPC_Implementation()\n{\n    MICROMEGAS_LOG(\"Network\", MicromegasTracing::LogLevel::Debug,\n                   TEXT(\"RPC received\"));\n    MICROMEGAS_IMETRIC(\"Network\", MicromegasTracing::Verbosity::Med,\n                       TEXT(\"RPCCount\"), TEXT(\"count\"), 1);\n}\n</code></pre>"}]}